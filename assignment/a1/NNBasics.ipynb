{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Neural Network Basics\n",
        "\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/datasci-w266/2025-fall-main/blob/master/assignment/a1/NNBasics.ipynb)\n",
        "\n",
        "### Brief Review of Machine Learning\n",
        "\n",
        "In supervised learning, parametric models are those where the model is a function of a fixed form with a number of untrained parameters. Together with a loss function and a training set, an optimizer can adjust these parameters\n",
        "\n",
        "## Part A:  Linear & Logistic Regression\n",
        "\n",
        "You've likely seen linear regression before.  In linear regression, we fit a line (technically, hyperplane) that predicts a target variable, $y$, based on some features $x$.  The form of this model is affine (even if we call it \"linear\"):  \n",
        "\n",
        "$$y_{hat} = xW + b$$\n",
        "\n",
        "where $W$ and $b$ are weights and an offset, respectively, and are the parameters of this parametric model.  The loss function that the optimizer uses to fit these parameters is the squared error ($||\\cdots||_2$) between the prediction and the ground truth in the training set.\n",
        "\n",
        "You've also likely seen logistic regression, which is tightly related to linear regression.  Logistic regression also fits a line - this time separating the positive and negative examples of a binary classifier.  The form of this model is similar: \n",
        "\n",
        "$$y_{hat} = \\sigma(xW + b)$$\n",
        "\n",
        "where again $W$ and $b$ are the parameters of this model, and $\\sigma$ is the [sigmoid function](https://en.wikipedia.org/wiki/Sigmoid_function) which maps un-normalized scores (\"logits\") to values $\\hat{y} \\in [0,1]$ that represent probabilities. The loss function that the optimizer uses to fit these parameters is the [cross entropy](../../materials/lesson_notebook/lesson_1_NN_Review.ipynb) between the prediction and the ground truth in the training set.\n",
        "\n",
        "This pattern of an affine transform, $xW + b$, occurs over and over in machine learning.\n",
        "\n",
        "**We'll use logistic regression as our running example for the rest of this part.**\n",
        "\n",
        "\n",
        "### Short Answer Questions\n",
        "\n",
        "Imagine you want to implement logistic regression:\n",
        "\n",
        "* `z = xW + b`\n",
        "* `y_hat = sigmoid(z)`\n",
        "\n",
        "Where:\n",
        "1.  `x` is an 11-dimensional feature vector\n",
        "2.  `W` is the weight vector\n",
        "3.  `b` is the bias term\n",
        "\n",
        "What are the dimensions of `W` and `b`?  Recall that in logistic regression, `z` is just a scalar (commonly referred to as the \"logit\").\n",
        "\n",
        "Sketch a picture of the whole equation using rectangles to illustrate the dimensions of `x`, `W`, and `b`.  See examples below for inspiration (though please label each dimension).  We don't ask you to submit this, but make sure you can do it!  It's the \"print\" debugging statement of neural networks!  It's also useful for reading papers... if you can't draw the shapes of all the tensors, you don't (yet) know what's going on!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part B: Batching\n",
        "\n",
        "Let's say we want to perform inference using your model (parameters `W` and `b`) above on multiple examples instead of just one. On modern hardware (especially GPUs), we can do this efficiently by *batching*.\n",
        "\n",
        "To do this, we stack up the feature vectors in x like in the diagram below.  Note that changing the number of examples you run on (i.e. your batch size) *does not* affect the number of parameters in your model.  You're just running the same thing in parallel (instead of running the above one feature vector at a time at a time).\n",
        "\n",
        "![](batchaffine.png)\n",
        "\n",
        "The red (# features) and blue (batch size) lines represent dimensions that are the same.\n",
        "\n",
        "### Short Answer Questions\n",
        "\n",
        "If we have 11 features and running the model in parallel with 30 examples, what are the dimensions of:\n",
        "\n",
        "1. `W` ?\n",
        "2. `b` ?\n",
        "3. `x` ?\n",
        "4. `z` ?\n",
        "\n",
        "_Hint:_ remember that your model parameters stay fixed!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1. Weight vector $W$\n",
        "\n",
        "* Same as before: one weight per feature.\n",
        "* Shape: **(11, 1)** ✅\n",
        "\n",
        "---\n",
        "\n",
        "### 2. Bias $b$\n",
        "\n",
        "* Bias is **always a scalar parameter** (shared across examples).\n",
        "* Shape: **(1, )**\n",
        "* When applied to a batch, it broadcasts to all 30 examples → effectively adds a vector of shape (30, 1).\n",
        "\n",
        "---\n",
        "\n",
        "### 3. Input $X$\n",
        "\n",
        "* 30 examples × 11 features each.\n",
        "* Shape: **(30, 11)** ✅\n",
        "\n",
        "---\n",
        "\n",
        "### 4. Output $z$\n",
        "\n",
        "* Multiply:\n",
        "\n",
        "  $$\n",
        "  (30, 11) \\times (11, 1) = (30, 1)\n",
        "  $$\n",
        "* Then add $b$ (broadcasts to all rows).\n",
        "* Shape: **(30, 1)** ✅\n",
        "\n",
        "---\n",
        "\n",
        "### Correct Answer:\n",
        "\n",
        "* $W: (11, 1)$\n",
        "* $b: (1, )$\n",
        "* $X: (30, 11)$\n",
        "* $z: (30, 1)$\n",
        "\n",
        "---\n",
        "\n",
        "The key point: **parameters $W$ and $b$ do not depend on batch size**. Only the input/output ($X, z$) scale with the batch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part C: Logistic Regression - NumPy Implementation\n",
        "\n",
        "In this section, we'll implement logistic regression by hand and compute a few values to make sure we understand what's going on!\n",
        "\n",
        "Let's say your model has the following parameters:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "W = np.array([45,6,3,25,-1])\n",
        "b = 5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If you want to run the model on the following three examples:\n",
        "\n",
        "* [1, 2, 3, 4, 5]\n",
        "* [0, 0, 0, 0, 5]\n",
        "* [-3, -4, -12, -1, 1]\n",
        "\n",
        "Construct the x matrix **such that you compute the answer all in one big batch** and compute the probability of the positive class for each."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The probabilities of the second example are: 0.5\n",
            "The cross-entropy loss of the second example is: 1.0\n"
          ]
        }
      ],
      "source": [
        "# Import sigmoid.\n",
        "from scipy.special import expit as sigmoid\n",
        "\n",
        "x = np.array(\n",
        "    [\n",
        "        [1, 2, 3, 4, 5],\n",
        "        [0, 0, 0, 0, 5],\n",
        "        [-3, -4, -12, -1, 1],\n",
        "    ]\n",
        ")\n",
        "\n",
        "def linear_model(x, W, b):\n",
        "    return sigmoid(np.dot(W, x) + b)\n",
        "\n",
        "z = linear_model(x.T, W, b)\n",
        "\n",
        "print(f'The probabilities of the second example are: {z[1]}')\n",
        "\n",
        "# Calculate cross-entropy loss of the second example.\n",
        "y = 1  # positive class\n",
        "loss = - (y * np.log2(z[1]) + (1 - y) * np.log2(1 - z[1]))\n",
        "print(f'The cross-entropy loss of the second example is: {loss}') "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Short Answer Questions\n",
        "\n",
        "1. What is the probability of the positive class for the second (middle) example?\n",
        "    - 0.5\n",
        "2. What is the cross-entropy loss in Base 2 of the second example if its label is positive?\n",
        "    - 1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part D: NumPy Feed Forward Neural Network\n",
        "\n",
        "Let's do the same procedure for a simple feed-forward neural network.\n",
        "\n",
        "Imagine you have a 3 layer network (hint: # of affines = # of layers. The affine is the W + b part of a layer).  Each hidden layer is size 10.  Just like before, you've already trained your model and you just want to run it forward.  For this exercise, let's say that each weight matrix is np.ones(...) and each bias term is [-1, -2, -3, ..., -n] if the bias term is $n$ long.  Compute the probability of the positive class for the three examples above, again in a single batch.\n",
        "\n",
        "**Hint:  Draw the shapes of the matrices at each layer out on a piece of paper!  Include it with any questions you post to Ed Discussion.**\n",
        "\n",
        "Assume your model uses a sigmoid as the nonlinearity for all layers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The probabilities of the third example are: 0.36915049986717824\n",
            "The cross-entropy loss of the third example is: 0.6646322280463387\n"
          ]
        }
      ],
      "source": [
        "# Layer 1\n",
        "W1 = np.ones((5, 10))     # (5 input features → 10 hidden units)\n",
        "b1 = -np.arange(1, 11)    # [-1, -2, ..., -10]\n",
        "\n",
        "z1 = x @ W1 + b1          # (3, 5) × (5, 10) + (10,) → (3, 10)\n",
        "a1 = sigmoid(z1)          # apply sigmoid elementwise\n",
        "\n",
        "# Layer 2\n",
        "W2 = np.ones((10, 10))\n",
        "b2 = -np.arange(1, 11)  \n",
        "z2 = a1 @ W2 + b2\n",
        "a2 = sigmoid(z2)\n",
        "\n",
        "# Layer 3\n",
        "W3 = np.ones((10, 1))\n",
        "b3 = np.array([-1])\n",
        "z3 = a2 @ W3 + b3\n",
        "a3 = sigmoid(z3)\n",
        "\n",
        "print(f'The probabilities of the third example are: {a3[2, 0]}')\n",
        "\n",
        "# Calculate cross-entropy loss of the third example.\n",
        "y = 0  # negative class\n",
        "loss = - (y * np.log2(a3[2,0]) + (1 - y ) * np.log2(1 - a3[2,0]))\n",
        "\n",
        "print(f'The cross-entropy loss of the third example is: {loss}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Short Answer Questions\n",
        "\n",
        "1.  What is the probability of the third example?\n",
        "    - 0.37\n",
        "2.  What is the cross-entropy loss if its label is negative?\n",
        "    - 0.66"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part E: Softmax\n",
        "\n",
        "Recall that softmax(z) is a vector with the same length as z, and whose components are:  $softmax(z)_i = \\frac{e^{z_i}}{\\Sigma_j e^{z_j}}$.\n",
        "\n",
        "### Short Answer Questions\n",
        "\n",
        "1. If the logits coming from the main body of the network are [4, 6, 8], what is the probability of the middle class?\n",
        "     - 0.12\n",
        "\n",
        "2. What is the cross-entropy loss if the correct class is the last one? (i.e. corresponding to logit=8)?\n",
        "     - 0.21\n",
        "\n",
        "3. If you had such a three-class classification problem, what would the dimensions of W and b be for the last layer of the feed forward neural network above? \n",
        "     - W3 will be 10x3, b3 will be 1x3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The probability of the middle class is 0.11731042782619837\n",
            "The cross-entropy loss of the correct (last) class is 0.2062067516229886\n"
          ]
        }
      ],
      "source": [
        "# Softmax on a logits\n",
        "logits = np.array([4, 6, 8])\n",
        "\n",
        "# Compute softmax\n",
        "softmax = np.exp(logits)/sum(np.exp(logits))\n",
        "print(f'The probability of the middle class is {softmax[1]}')\n",
        "\n",
        "# CE if the correct class is the last one\n",
        "y = 2  # correct class index\n",
        "\n",
        "# Calculate cross-entropy loss\n",
        "loss = -( 0 * np.log2(softmax[0]) + 0 * np.log2(softmax[1]) + 1 * np.log2(softmax[2]))\n",
        "print(f'The cross-entropy loss of the correct (last) class is {loss}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "2025-fall-main (3.10.2)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
