{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n7BzBd-N9mS1"
      },
      "source": [
        "# Assignment 2: Text Classification with Deep Averaged Networks\n",
        "\n",
        "**Description:** This assignment covers various neural network architectures and components, largely used in the context of classification. You will compare Deep Averaging Networks, Deep Weighted Averaging Networks using Attention, and BERT-based models. In part one, you should be able to develop an intuition for:\n",
        "\n",
        "\n",
        "*   The effects of fine-tuning word vectors or starting with random word vectors\n",
        "*   How various networks behave when the training set size changes\n",
        "*   The effect of shuffling your training data\n",
        "*   The effect of fine tuning your embeddings\n",
        "\n",
        "\n",
        "\n",
        "The assignment notebook closely follows the lesson notebooks. We will use the IMDB dataset and will leverage some of the models, or part of the code, for our current investigation.\n",
        "\n",
        "This notebook uses the Keras 3 functional API.  Make sure the correct versions get loaded.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YCYA5Gvoae4x"
      },
      "source": [
        "## IMPORTANT NOTE:\n",
        "Because of the environment built in to Colab, the software we're importing causes an error to be thrown the first time you run the cells in the Setup section.  As soon as you hit the error, you have to go to the Runtime menu and select 'Restart Session'.  Once the session is restarted you must rerun the cells in Setup and you will not run in to the error."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j0NIBbj_afe8"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/datasci-w266/2025-fall-main/blob/master/assignment/a2/Text_classification_DAN.ipynb)\n",
        "\n",
        "The overall assignment structure is as follows:\n",
        "\n",
        "\n",
        "0. Setup\n",
        "  \n",
        "  0.1 Libraries, Embeddings,  & Helper Functions\n",
        "\n",
        "  0.2 Data Acquisition\n",
        "\n",
        "  0.3. Data Preparation\n",
        "\n",
        "    0.3.1 Training/Test Sets using Word2Vec\n",
        "\n",
        "    0.3.2 Training/Test Sets for BERT-based models\n",
        "\n",
        "\n",
        "1. Classification with various Word2Vec-based Models\n",
        "\n",
        "  1.1 The Role of Shuffling of the Training Set\n",
        "\n",
        "  1.2 DAN\n",
        "    \n",
        "  1.3 Approaches for Training of Embeddings\n",
        "\n",
        "\n",
        "\n",
        "**INSTRUCTIONS:**:\n",
        "\n",
        "* Questions are always indicated as **QUESTION**, so you can search for this string to make sure you answered all of the questions. You are expected to fill out, run, and submit this notebook, as well as to answer the questions in the **answers** file as you did in a1.  Please do **not** remove the output from your notebooks when you submit them as we'll look at the output as well as your code for grading purposes.  We cannot award points if the output cells are empty.\n",
        "\n",
        "* **### YOUR CODE HERE** indicates that you are supposed to write code.\n",
        "\n",
        "* If you want to, you can run all of the cells in section 0 in bulk. This is setup work and no questions are in there. At the end of section 0 we will state all of the relevant variables that were defined and created in section 1.\n",
        "\n",
        "* Finally, unless otherwise indicated your validation accuracy will be 0.65 or higher if you have correctly implemented the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "so-yur1S9mS4"
      },
      "source": [
        "## 0. Setup\n",
        "\n",
        "### 0.1. Libraries and Helper Functions\n",
        "\n",
        "This notebook requires the TensorFlow dataset and other prerequisites that you must download.  THis model uses the Keras 3 interface."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "8uQnMctL9mS5"
      },
      "outputs": [],
      "source": [
        "#@title Installs\n",
        "!pip install pydot --quiet\n",
        "!pip install gensim --quiet\n",
        "!pip install tensorflow-datasets --quiet\n",
        "!pip install tensorflow-text --quiet\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZFFBvPMR9mS8"
      },
      "source": [
        "Now we are ready to do the imports."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Q8b9aykE9mS8"
      },
      "outputs": [],
      "source": [
        "#@title Imports\n",
        "import os\n",
        "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "\n",
        "from keras.layers import Embedding, Input, Dense, Lambda\n",
        "from keras.models import Model\n",
        "\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow_text as tf_text\n",
        "\n",
        "\n",
        "import sklearn as sk\n",
        "\n",
        "import nltk\n",
        "from nltk.data import find\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import re\n",
        "\n",
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.models import KeyedVectors\n",
        "from gensim.test.utils import datapath"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "E7YOY6-nH9va"
      },
      "outputs": [],
      "source": [
        "def print_version(library_name):\n",
        "    try:\n",
        "        lib = __import__(library_name)\n",
        "        version = getattr(lib, '__version__', 'Version number not found')\n",
        "        print(f\"{library_name} version: {version}\")\n",
        "    except ImportError:\n",
        "        print(f\"{library_name} not installed.\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "42ofo0Dxsnbx",
        "outputId": "b8378af6-b14b-4e23-fa02-33641eecc734"
      },
      "outputs": [],
      "source": [
        "#confirm versions\n",
        "print_version('numpy')\n",
        "print_version('tensorflow')\n",
        "print_version('keras')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ESElm33U9mS9"
      },
      "source": [
        "Make sure you are using a Keras version > 3.0 and a tensorflow version > 2.16.0 for this notebook.\n",
        "\n",
        "Below is a helper function to plot histories."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "YKWj6pPM9mS-"
      },
      "outputs": [],
      "source": [
        "#@title Plotting Function\n",
        "\n",
        "# 4-window plot. Small modification from matplotlib examples.\n",
        "\n",
        "def make_plot(axs,\n",
        "              model_history1,\n",
        "              model_history2,\n",
        "              model_1_name='model 1',\n",
        "              model_2_name='model 2',\n",
        "              ):\n",
        "    box = dict(facecolor='yellow', pad=5, alpha=0.2)\n",
        "\n",
        "    for i, metric in enumerate(['loss', 'accuracy']):\n",
        "        # small adjustment to account for the 2 accuracy measures in the Weighted Averging Model with Attention\n",
        "        if 'classification_%s' % metric in model_history2.history:\n",
        "            metric2 = 'classification_%s' % metric\n",
        "        else:\n",
        "            metric2 = metric\n",
        "\n",
        "        y_lim_lower1 = np.min(model_history1.history[metric])\n",
        "        y_lim_lower2 = np.min(model_history2.history[metric2])\n",
        "        y_lim_lower = min(y_lim_lower1, y_lim_lower2) * 0.9\n",
        "\n",
        "        y_lim_upper1 = np.max(model_history1.history[metric])\n",
        "        y_lim_upper2 = np.max(model_history2.history[metric2])\n",
        "        y_lim_upper = max(y_lim_upper1, y_lim_upper2) * 1.1\n",
        "\n",
        "        for j, model_history in enumerate([model_history1, model_history2]):\n",
        "            model_name = [model_1_name, model_2_name][j]\n",
        "            model_metric = [metric, metric2][j]\n",
        "            ax1 = axs[i, j]\n",
        "            ax1.plot(model_history.history[model_metric])\n",
        "            ax1.plot(model_history.history['val_%s' % model_metric])\n",
        "            ax1.set_title('%s - %s' % (metric, model_name))\n",
        "            ax1.set_ylabel(metric, bbox=box)\n",
        "            ax1.set_ylim(y_lim_lower, y_lim_upper)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1QDi-Kg49mS-"
      },
      "source": [
        "Next, we get the word2vec model from NLTK to use as our embeddings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "49X1T6an9mS_",
        "outputId": "3b7abe4f-fc33-4ba7-9231-20ed86957b85"
      },
      "outputs": [],
      "source": [
        "#@title NLTK & Word2Vec\n",
        "\n",
        "nltk.download('word2vec_sample')\n",
        "\n",
        "word2vec_sample = str(find('models/word2vec_sample/pruned.word2vec.txt'))\n",
        "\n",
        "wvmodel = KeyedVectors.load_word2vec_format(datapath(word2vec_sample), binary=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y_rdVE3z9mTA"
      },
      "source": [
        "Now here we have the embedding **model** defined, let's see how many words are in the vocabulary:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uoL6l_q89mTA",
        "outputId": "273df485-b75b-42de-9e51-638781c0fdca"
      },
      "outputs": [],
      "source": [
        "len(wvmodel)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r3Q0zOkJ9mTB"
      },
      "source": [
        "What do the word vectors look like? As expected:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZyAGMYGK9mTB",
        "outputId": "ec0683cf-078c-4dec-bdbb-4d5920036cfb"
      },
      "outputs": [],
      "source": [
        "wvmodel['great'][:20]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2BMraFZS9mTB"
      },
      "source": [
        "We can now build the embedding matrix and a vocabulary dictionary:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "lOTIN3G39mTB"
      },
      "outputs": [],
      "source": [
        "EMBEDDING_DIM = len(wvmodel['university'])      # we know... it's 300\n",
        "\n",
        "# initialize embedding matrix and word-to-id map:\n",
        "embedding_matrix = np.zeros((len(wvmodel) + 1, EMBEDDING_DIM))\n",
        "vocab_dict = {}\n",
        "\n",
        "# build the embedding matrix and the word-to-id map:\n",
        "for i, word in enumerate(wvmodel.index_to_key):\n",
        "    embedding_vector = wvmodel[word]\n",
        "\n",
        "    if embedding_vector is not None:\n",
        "        # words not found in embedding index will be all-zeros.\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "        vocab_dict[word] = i\n",
        "\n",
        "# we can use the last index at the end of the vocab for unknown tokens\n",
        "vocab_dict['[UNK]'] = len(vocab_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5KlSpLnP6VqA",
        "outputId": "86813d56-0e3c-4152-884c-3ac29153c103"
      },
      "outputs": [],
      "source": [
        "embedding_matrix.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LGYcZu0N9mTC",
        "outputId": "db6589b0-2aa0-4b11-a7aa-d3e4b7fc5182"
      },
      "outputs": [],
      "source": [
        "embedding_matrix[:5, :5]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YIL1eUtV9mTC"
      },
      "source": [
        "The last row consists of all zeros. We will use that for the UNK token, the placeholder token for unknown words.\n",
        "\n",
        "### 0.2 Data Acquisition\n",
        "\n",
        "\n",
        "We will use the IMDB dataset delivered as part of the TensorFlow-datasets library, and split into training and test sets. For expedience, we will limit ourselves in terms of train and test examples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 155,
          "referenced_widgets": [
            "bc21adf6926249678f9593b75c098d93",
            "878a9307d3f54027b679dea4f957cc39",
            "33c6c5e197d04b908bb5405a8700432c",
            "e7e76e279b1f4481937a7a9ddd3e7ead",
            "5d5d5b09de504697b9b3dfb589a15f37",
            "1ab8c4eb0dbc4d1bac413a31972c2df2",
            "178d37a90e8e4dcd9ad861ba42785973",
            "a731d95c6523499884c08d32fadb96cb",
            "01e2eea263ab46c79bc0e12b1883a2f4",
            "35a3425659cb4215ad54c1c6d28a8216",
            "8971c6e1b6ae421896d00c98c37bfbdb",
            "481301c7148a4b5c89cd894956d25847",
            "393b7d9f497644c894e22cb97b7f012f",
            "51ca4aab7b434577b0b833c1f43845a0",
            "8f9dd2f526a24e539f6a9fea81d38e46",
            "d9ed388d6022481bb4a901572e3cf3cc",
            "50b795f0f90c46498e617c34fcf92e7f",
            "9ddbbf8cf0884f248d55a96109df4faa",
            "0effd344e5974c5cb82c9493026f2872",
            "17ba42c6c319469296201582fdbc9fbb",
            "dcaa8b1a719a4a7ba12319fc29987fbd",
            "dd9c8cdda45b48f8be8d02559683b05c",
            "dfb9e53620654dd7a57d8212639f259a",
            "ef69c379863245a6b2e4f04815983a47",
            "4f72bab193c844c88bdeead66f4b2112",
            "d21722b8e7af4050a5c573d7ad54c869",
            "e8449b1f5f304b4b900c870e2ffa7977",
            "ef0108ff262545cf96f7cc2a767439ec",
            "bfb437bc39814b609e7e71058319f768",
            "0eed1149356a4e11b1b6ca5330701bd6",
            "7e73f230d6ea45ecbac216e2425d3951",
            "c83cfe72de1543338e4222a720d7a104",
            "3eceb16da39c44cda78b70eba84e1fef",
            "adab986a498a425982026f24c3f6f995",
            "f2e8edece9a94be290754670f8c776de",
            "fc6f2bcedc014be1907baf4a2f9020b8",
            "c0c12d80830c4e09b599e371e2f81893",
            "4001becb2d4b4806814cdaf90b37ebd1",
            "7c15483a68b8491ba8eaccb3f0e924f2",
            "0a4fedecf7034a9b9d807fff84f46bca",
            "8f46d4a8526c45fe9dde2a3634274395",
            "43292a09983340df81454bc1a776009c",
            "0a13da24b84d44c4b3dcf80fc486fe35",
            "2da346420f9e4978a0a698cadb64262b",
            "9c389fa9662e46d09882289e1f19e4cf",
            "df6d825c059f4f20b8c23c1aceb3f8dd",
            "69936bbaf3e14f958b65e6c82fdf98c8",
            "cf995a97c1a84e218df336c51a34a56b",
            "fb534458948e48ab843094f2804a0e1b",
            "d08cfdd31a5845b1a399744bd033e742",
            "f63b740bc9674d82902a197e9f46a8dc",
            "c20a8fbb57594feabd32791f8d605c43",
            "2e334ac437824e00ae994faeadea9465",
            "f2982987deb1489ebf7ef8679cec0f99",
            "c7b8b368659d490296ddfe5509c1266c",
            "f01c64e59b6a484fbb98e343e7c0f9ce",
            "e5da6f0e229d49d9a12ba30f6d3309fa",
            "048010c525c24d52af45d9c53aea2989",
            "29832727a2e14f2a8b16cfa769101c3d",
            "1e9afa40717d4529ad17803331c9455c",
            "2af83f61a83d408eadf2bd54bf540a02",
            "1aa32408ce6f4ef5ac8de7fe8477e7c6",
            "b0f285d7c37e41cb950625df82837684",
            "deb2d1a50f1c4a4ab3cabd8b683ec089",
            "aba8a673a8534a95b301d20365ed21e3",
            "e9947a387b4942a6bbe3491acf59b9bf",
            "531a7e879d04493191c9ddecc2e6b1be",
            "0ec62f338d064d3a8f1ce963fe26ca8f",
            "d6e10ca1304741c98d3a01ef03cf9060",
            "824ab87f815f46e68f12ff3d2d666eed",
            "1bb20650090a4ada9c3faef5fd1742cb",
            "8682ddd52be74d848ee267a9209dbba6",
            "45f0bd9c7dc6415ea0fa7e77a0ab69d7",
            "598845723f3a4b9c807c001ea2e51796",
            "b09691d136874bd5a53f6feb6adfc802",
            "5b55bbe697ca43bdb348183ba564eacf",
            "af47b0e6c52146fc99766a6ea858645a",
            "58adeab34f5347faac4f1b18adfee69b",
            "063bd937baca453cb114ce158caa42e4",
            "3852b95047b8447188d2e6a19a522c88",
            "9e39d8982f0947a789016ede28435a2f",
            "87aa8b1f418b40f9aa959ad6a2fd0f0f",
            "79f9b2bce4db44d09abd79373b8241d9",
            "bfc7a93467e54cddaa9ac57f0f43e5bd",
            "e13ef8d576f94c5a92ecb08a2233cf47",
            "33d2163afa7b46ef834597277cb40b03",
            "da21d5ca3e3541628a259bbf96c4559b",
            "1f0d1f427f7f423d85613a917980b4ad",
            "83ce95ca6ea34629a44efab330420f00",
            "2ac34e0b13274db7aed92f59e75a4088",
            "b296dd045eba4636b56353eea90990d1",
            "3565522df0fb4c6f933eaa7b050e2fc2",
            "68840dfa263a4ef2b62053ae81165e79",
            "8147160444254efe8ca6d31c76bd1f30",
            "b625cde200534a01adf57081c3ac9ce4",
            "b405ac911dbb456ca666bd139d7ff211",
            "64fb814877a04d0cb8b44e1028ac81e8",
            "1247c769117b4d4a9039b9bf5044587a",
            "7ad98c6b853045eba65fd74c06269d19"
          ]
        },
        "id": "uwOF0qYb9mTC",
        "outputId": "d2131098-96a4-4fdb-d9b7-09eae9f61165"
      },
      "outputs": [],
      "source": [
        "train_data, test_data = tfds.load(\n",
        "    name=\"imdb_reviews\",\n",
        "    split=('train[:80%]', 'test[80%:]'),\n",
        "    as_supervised=True)\n",
        "\n",
        "train_examples, train_labels = next(iter(train_data.batch(20000)))\n",
        "test_examples, test_labels = next(iter(test_data.batch(5000)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mPHFtgGkHNOQ"
      },
      "source": [
        "It is always highly recommended to look at the data. What do the records look like? Are they clean or do they contain a lot of cruft (potential noise)?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wvmWKdVQ9mTC",
        "outputId": "39204698-1c37-4d37-f077-741883ea34ed"
      },
      "outputs": [],
      "source": [
        "train_examples[:4]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BzEnCspD9mTD",
        "outputId": "91b2beb6-0644-4857-b7d4-191095bcdcad"
      },
      "outputs": [],
      "source": [
        "train_labels[:4]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CplHsqSDMKCa"
      },
      "source": [
        "For convenience, in this assignment we will define a sequence length and truncate all records at that length. For records that are shorter than our defined sequence length we will add padding characters to insure that our input shapes are consistent across all records."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "Zxu9U3qXMKTW"
      },
      "outputs": [],
      "source": [
        "MAX_SEQUENCE_LENGTH = 100"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1bHwj4vu9mTD"
      },
      "source": [
        "## 0.3. Data Preparation\n",
        "\n",
        "### 0.3.1. Training/Test Sets for Word2Vec-based Models\n",
        "\n",
        "First, we tokenize the data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "ToVTmC8V9mTD"
      },
      "outputs": [],
      "source": [
        "tokenizer = tf_text.WhitespaceTokenizer()\n",
        "train_tokens = tokenizer.tokenize(train_examples)\n",
        "test_tokens = tokenizer.tokenize(test_examples)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qXauPwil9mTD"
      },
      "source": [
        "Let's look at some tokens.  Do they look acceptable?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QZ22GGb-9mTD",
        "outputId": "d34f91fb-d66b-4074-93b3-71e77ea88cb6"
      },
      "outputs": [],
      "source": [
        "train_tokens[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K9D9nqdg9mTE"
      },
      "source": [
        "Yup... looks right. Of course we will need to take care of the encoding later."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XiqFULXx9mTE"
      },
      "source": [
        "Next, we define a simple function that converts the tokens above into the appropriate word2vec index values so we can retrieve the embedding vector associated with the word.   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "ytUsu3kmuM3n"
      },
      "outputs": [],
      "source": [
        "def docs_to_vocab_ids(tokenized_texts_list):\n",
        "    \"\"\"\n",
        "    converting a list of strings to a list of lists of word ids\n",
        "    \"\"\"\n",
        "    texts_vocab_ids = []\n",
        "    text_labels = []\n",
        "    valid_example_list = []\n",
        "    for i, token_list in enumerate(tokenized_texts_list):\n",
        "\n",
        "        # Get the vocab id for each token in this doc ([UNK] if not in vocab)\n",
        "        vocab_ids = []\n",
        "        for token in list(token_list.numpy()):\n",
        "            decoded = token.decode('utf-8', errors='ignore')\n",
        "            if decoded in vocab_dict:\n",
        "                vocab_ids.append(vocab_dict[decoded])\n",
        "            else:\n",
        "                vocab_ids.append(vocab_dict['[UNK]'])\n",
        "\n",
        "        # Truncate text to max length, add padding up to max length\n",
        "        vocab_ids = vocab_ids[:MAX_SEQUENCE_LENGTH]\n",
        "        n_padding = (MAX_SEQUENCE_LENGTH - len(vocab_ids))\n",
        "        # For simplicity in this model, we'll just pad with unknown tokens\n",
        "        vocab_ids += [vocab_dict['[UNK]']] * n_padding\n",
        "        # Add this example to the list of converted docs\n",
        "        texts_vocab_ids.append(vocab_ids)\n",
        "\n",
        "        if i % 5000 == 0:\n",
        "            print('Examples processed: ', i)\n",
        "\n",
        "    print('Total examples: ', i)\n",
        "    return np.array(texts_vocab_ids)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gv_elC2m9mTE"
      },
      "source": [
        "Now we can create training and test data that can be fed into the models of interest.  We need to convert all of the tokens in to their respective input ids."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XpUJLBRkCbtE",
        "outputId": "a658b24e-5f35-4e06-f35a-2a5b8d380ae5"
      },
      "outputs": [],
      "source": [
        "train_input_ids = docs_to_vocab_ids(train_tokens)\n",
        "test_input_ids = docs_to_vocab_ids(test_tokens)\n",
        "\n",
        "train_input_labels = np.array(train_labels)\n",
        "test_input_labels = np.array(test_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-dP2KY7U9mTF"
      },
      "source": [
        "Let's convince ourselves that the data looks correct:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FtU56wVR9mTF",
        "outputId": "de78026c-fe9b-4571-e43f-19a7c71e8df5"
      },
      "outputs": [],
      "source": [
        "train_input_ids[:2]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "myNK4ZhQDQBL"
      },
      "source": [
        "Overall, here are the key variables and sets that we encoded for word2vec and BERT and that may be used moving forward. If the variable naming does not make it obvious, we also state the purpose:\n",
        "\n",
        "#### Parameters:\n",
        "\n",
        "* MAX_SEQUENCE_LENGTH (100)\n",
        "\n",
        "\n",
        "#### Word2vec-based models:\n",
        "\n",
        "* train(/test)_input_ids: input ids for the training(/test) sets for word2vec models\n",
        "* train(/test)_input_labels: the corresponding labels\n",
        "\n",
        "\n",
        "**NOTE:** We recommend you inspect these variables if you have not gone through the code."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w20farC9Exg9"
      },
      "source": [
        "### 1  Keras Functional API warm up\n",
        "\n",
        "Shown below is the output of a call to model summary.  It shows a network with specific named layers.  You are to reproduce the model that generated this summary.\n",
        "\n",
        "**QUESTION:**\n",
        "\n",
        "1.a Create a model using the Keras functional API so that the model.summary() call of your model identically reproduces the model summary shown here:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GYS7nyKLJD6F"
      },
      "source": [
        "**Model Summary Output To Reproduce**\n",
        "```\n",
        "Model: \"a2_question1\"\n",
        "_________________________________________________________________\n",
        " Layer (type)                Output Shape              Param #   \n",
        "_________________________________________________________________\n",
        " input_words (InputLayer)    [(None, 100)]             0         \n",
        "                                                                 \n",
        " embedding (Embedding)       (None, 100, 300)          13194600  \n",
        "                                                                 \n",
        " lambda (Lambda)             (None, 300)               0         \n",
        "                                                                 \n",
        " hidden1 (Dense)             (None, 300)               90300     \n",
        "                                                                 \n",
        " hidden2 (Dense)             (None, 200)               60200     \n",
        "                                                                 \n",
        " output (Dense)              (None, 5)                 1005      \n",
        "                                                                 \n",
        "__________________________________________________________________\n",
        "Total params: 13346105 (50.91 MB)\n",
        "Trainable params: 151505 (591.82 KB)\n",
        "Non-trainable params: 13194600 (50.33 MB)\n",
        "_________________________________________________________________\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "f7yra8DyExg9"
      },
      "outputs": [],
      "source": [
        "input_x = Input(shape = (MAX_SEQUENCE_LENGTH,), name=\"input_words\")\n",
        "\n",
        "### YOUR CODE HERE\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### END YOUR CODE\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics = ['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 331
        },
        "id": "vzEM_W0sExg9",
        "outputId": "921e3638-3d23-4a3b-bc85-3d5f837cc7c1"
      },
      "outputs": [],
      "source": [
        "#Run this cell to generate your summary to match the summary output dabove\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vzbPHBf3GP2O"
      },
      "source": [
        "## 1.1 Classification with various Word2Vec-based Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H7yp2gI-AtCl"
      },
      "source": [
        "**QUESTION:**\n",
        "\n",
        "1.1.a. Revisit the dataset. Is it balanced? Find the percentage of positive examples in the training set. (Copy and paste the decimal value for your calculation, e.g. a number like 0.5678 or 0.8765)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x6EAE6cjA9jM",
        "outputId": "4eec2388-2850-4c91-88dd-611fb074f062"
      },
      "outputs": [],
      "source": [
        "### YOUR CODE HERE\n",
        "### END YOUR CODE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mGkEVpmu6Bs2"
      },
      "source": [
        "**QUESTION:**\n",
        "\n",
        "1.1.b. Now find the percentage of positive examples in the test set.  (Copy and paste the decimal value for your calculation, e.g. a number like 0.5678 or 0.8765)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2K_8eBTHArme",
        "outputId": "3da14404-5428-49a6-dc2c-550e06ae904a"
      },
      "outputs": [],
      "source": [
        "### YOUR CODE HERE\n",
        "### END YOUR CODE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JY6X0wL3BQKD"
      },
      "source": [
        "### 1.2 The Role of Shuffling of the Training Set\n",
        "\n",
        "\n",
        "We will first revisit the DAN model.\n",
        "\n",
        "Reuse the code from the class notebook to build a DAN network with one hidden layer of dimension 100. The optimizer should be Adam. Wrap the model creation in a function according to this API:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "vk-4mCgyBO9S"
      },
      "outputs": [],
      "source": [
        "def create_dan_model(retrain_embeddings=False,\n",
        "                     max_sequence_length=MAX_SEQUENCE_LENGTH,\n",
        "                     hidden_dim=100,\n",
        "                     dropout=0.3,\n",
        "                     embedding_initializer='word2vec',\n",
        "                     learning_rate=0.001):\n",
        "  \"\"\"\n",
        "  Construct the DAN model including the compilation and return it. Parametrize it using the arguments.\n",
        "  :param retrain_embeddings: boolean, indicating whether  the word embeddings are trainable\n",
        "  :param hidden_dim: dimension of the hidden layer\n",
        "  :param dropout: dropout applied to the hidden layer\n",
        "\n",
        "  :returns: the compiled model\n",
        "  \"\"\"\n",
        "\n",
        "  if embedding_initializer == 'word2vec':\n",
        "    embeddings_initializer=keras.initializers.Constant(embedding_matrix)\n",
        "  else:\n",
        "    embeddings_initializer='uniform'\n",
        "\n",
        "\n",
        "  ### YOUR CODE HERE\n",
        "\n",
        "  # start by creating the dan_embedding_layer. Use the embeddings_initializer. variable defined above.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  ### END YOUR CODE\n",
        "  return dan_model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hb4LOJkFlYwF"
      },
      "source": [
        "Let us create a sorted version of the training dataset to run some simulations and see what happens if we feed the model sorted data.  What do you think will happen if we train on sorted data?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "ZX2hWslCflw1"
      },
      "outputs": [],
      "source": [
        "sorted_train_input_data = [(x, y) for (x, y) in zip(list(train_input_ids), list(train_input_labels))]\n",
        "sorted_train_input_data.sort(key = lambda x: x[1])\n",
        "sorted_training_input_ids = np.array([x[0] for x in sorted_train_input_data])\n",
        "sorted_training_labels = np.array([x[1] for x in sorted_train_input_data])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "riQ59wcQmtzs"
      },
      "source": [
        "Next, create your DAN model using the default parameters and train it by:\n",
        "\n",
        "1.  Using the sorted dataset\n",
        "2.  Using 'shuffle=False' as one of the model.fit parameters.\n",
        "3.  Train for 10 epochs with a batch size of 32\n",
        "\n",
        "Make sure you store the history (name it 'dan_sorted_history') as we did in the lesson notebooks.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SIgwDUfpi7nu",
        "outputId": "c44a3faa-90ff-4888-f20e-f24bfc4c3258"
      },
      "outputs": [],
      "source": [
        "### YOUR CODE HERE\n",
        "\n",
        "dan_model_sorted = create_dan_model()\n",
        "\n",
        "#use dan_sorted_history = ... below\n",
        "### END YOUR CODE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4zFifGHMS1S"
      },
      "source": [
        "**QUESTION:**\n",
        "\n",
        "1.2.a What is the final validation accuracy that you observed after you completed the 10 epochs? (Copy and paste the decimal value for the final validation accuracy, e.g. a number like 0.5678 or 0.8765)\n",
        "\n",
        "Hint: You should have an accuracy number above 0.30.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RUNYdZ8rnaNX"
      },
      "source": [
        "Next, recreate the same model and train it with **'shuffle=True'**. (Note that this is also the default.). Use 'dan_shuffled_history' for the history."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fEsrjV2QkCo_",
        "outputId": "874f2209-f07a-411b-e28f-b19724d43d23"
      },
      "outputs": [],
      "source": [
        "### YOUR CODE HERE\n",
        "\n",
        "dan_model_shuffled = create_dan_model()\n",
        "\n",
        "#use dan_shuffled_history = ... below\n",
        "\n",
        "### END YOUR CODE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sXs6UX44ko7P"
      },
      "source": [
        "**QUESTION:**\n",
        "\n",
        "1.2.b What is the final validation accuracy that you observed for the shuffled run after completing 10 epochs? (Copy and paste the decimal value for the final validation accuracy, e.g. a number like 0.5678 or 0.8765)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BYCwHBzyoY0_"
      },
      "source": [
        "Compare the 2 histories in a plot."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 812
        },
        "id": "ZAlGkoidkun-",
        "outputId": "9896fb5b-43a4-4bb9-b868-648e20a6ca8e"
      },
      "outputs": [],
      "source": [
        "fig, axs = plt.subplots(2, 2)\n",
        "fig.subplots_adjust(left=0.2, wspace=0.6)\n",
        "make_plot(axs,\n",
        "          dan_sorted_history,\n",
        "          dan_shuffled_history,\n",
        "          model_1_name='sorted',\n",
        "         model_2_name='shuffled')\n",
        "\n",
        "fig.align_ylabels(axs[:, 1])\n",
        "fig.set_size_inches(18.5, 10.5)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5IYOH-QfSj22"
      },
      "source": [
        "### 1.3 Approaches for Training of Embeddings\n",
        "\n",
        "Rerun the DAN Model in 3 separate configurations:\n",
        "\n",
        "\n",
        "a.   embedding_initializer = 'word2vec' and retrain_embeddings=False\n",
        "\n",
        "b.   embedding_initializer = 'word2vec' and retrain_embeddings=True\n",
        "\n",
        "c.   embedding_initializer = 'uniform' and retrain_embeddings=True\n",
        "\n",
        "\n",
        "**NOTE:** Train the model with static embeddings for 10 epochs and the ones with trainable embeddings for 3 epochs each. Make sure you are using the original training data we created and not the sorted data.\n",
        "\n",
        "What do you observe about the effects of initializing and retraining the embedding matrix?\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a0jwQ6ailUm4"
      },
      "source": [
        "**QUESTION:**\n",
        "\n",
        "1.3.a First, what is the final validation accuracy that you just observed for the static model initialized with the word2vec after 10 epochs?  (Copy and paste the decimal value for the final validation accuracy, e.g. a number like 0.5678 or 0.8765)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h6Pxm-2xU1aw",
        "outputId": "8c6e98b2-7faf-4c3e-bea9-7b7d0bb776c5"
      },
      "outputs": [],
      "source": [
        "#1.3.a\n",
        "### YOUR CODE HERE\n",
        "\n",
        "\n",
        "### END YOUR CODE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sZXr9UY7lfHE"
      },
      "source": [
        "**QUESTION:**\n",
        "\n",
        "\n",
        "1.3.b What is the final validation accuracy that you observed for the model where you initialized with word2vec vectors but allow them to retrain for 3 epochs? (Copy and paste the decimal value for the final validation accuracy, e.g. a number like 0.5678 or 0.8765)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qwlDqMTxVwbQ",
        "outputId": "2cf490ec-5bbd-427a-8a1e-e67838bf2fcc"
      },
      "outputs": [],
      "source": [
        "#1.3.b\n",
        "### YOUR CODE HERE\n",
        "\n",
        "\n",
        "### END YOUR CODE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hO791d-oYOgg"
      },
      "source": [
        "**QUESTION:**\n",
        "\n",
        "1.3.c What is the final validation accuracy that you observed for the model where you initialized randomly and then trained?  (Copy and paste the decimal value for the final validation accuracy, e.g. a number like 0.5678 or 0.8765)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H0rMPTAOVw70",
        "outputId": "27af3743-a9c1-44d9-8110-4c86d1858234"
      },
      "outputs": [],
      "source": [
        "#1.3.c\n",
        "### YOUR CODE HERE\n",
        "\n",
        "\n",
        "### END YOUR CODE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "VGdBx9wAyxzo"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "state": {},
        "version_major": 2,
        "version_minor": 0
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
