{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n7BzBd-N9mS1"
      },
      "source": [
        "# Assignment 2: Text Classification with BERT\n",
        "\n",
        "**Description:** This assignment notebook builds on the material from the\n",
        "[lesson 4 notebook](https://github.com/datasci-w266/2025-fall-main/blob/master/materials/lesson_notebooks/lesson_4_BERT.ipynb), in which we fine-tuned a BERT model for the IMDB movie reviews sentiment classification task. In that notebook, we used the bert-base-cased model and applied traditional fine-tuning, with a brief class exercise at the end to try unfreezing different numbers of layers. In this assignment, we'll start with that exercise, and ask you to explore unfreezing more specific layers yourself. Then you'll search for and try different pre-trained BERT-style models.\n",
        "\n",
        "This notebook should be run on a Google Colab leveraging a GPU. By default, when you open the notebook in Colab it will try to use a GPU. Please note that you the GPU is reuqired for Section 3 but not for Sections 1 and 2.\n",
        "Since colab is providing free access to a GPU they place constraints on that access.  Therefore you might want to turn off the GPU access (Edit -> Notebook Settings) until you get to section 3.  Total runtime of the entire notebook (with solutions and a Colab GPU) should be about 1h with the majority of that time being in Section 3. If Colab tells you that you have reached your GPU limit, wait up to 24 hours and you should be able to access a GPU again.\n",
        "\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/datasci-w266/2025-fall-main/blob/master/assignment/a2/Text_classification_BERT.ipynb)\n",
        "\n",
        "The overall assignment structure is as follows:\n",
        "\n",
        "\n",
        "0. Setup\n",
        "  \n",
        "  0.1 Libraries and Helper Functions\n",
        "\n",
        "  0.2 Data Acquisition\n",
        "\n",
        "  0.3. Data Preparation\n",
        "\n",
        "\n",
        "1. Classification with BERT\n",
        "\n",
        "  1.1. BERT Basics\n",
        "\n",
        "  1.2 CLS-Token-based Classification\n",
        "\n",
        "  1.3 Averaging of BERT Outputs\n",
        "\n",
        "  1.4. Adding a CNN on top of BERT\n",
        "\n",
        "\n",
        "\n",
        "**INSTRUCTIONS:**:\n",
        "\n",
        "* Questions are always indicated as **QUESTION**, so you can search for this string to make sure you answered all of the questions. You are expected to fill out, run, and submit this notebook, as well as to answer the questions in the **answers** file as you did in a1.  Please do **not** remove the output from your notebooks when you submit them as we'll look at the output as well as your code for grading purposes.  We cannot award points if the output cells are empty.\n",
        "\n",
        "* **### YOUR CODE HERE** indicates that you are supposed to write code.\n",
        "\n",
        "* If you want to, you can run all of the cells in section 0 in bulk. This is setup work and no questions are in there. At the end of section 0 we will state all of the relevant variables that were defined and created in section 1.\n",
        "\n",
        "* Finally, unless otherwise indicated your validation accuracy will be 0.65 or higher if you have correctly implemented the model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "so-yur1S9mS4"
      },
      "source": [
        "## 0. Setup\n",
        "\n",
        "### 0.1. Libraries and Helper Functions\n",
        "\n",
        "This notebook requires the Hugging Face datasets and other prerequisites that you must download.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8uQnMctL9mS5",
        "outputId": "f8707e52-0f88-4c03-85a3-0d4e0382afa8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m112.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# Install uv, the fast package manager\n",
        "!pip install uv --quiet\n",
        "\n",
        "# Install your libs via uv (quiet)\n",
        "!uv pip install transformers torchinfo datasets fsspec huggingface_hub evaluate --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vYfBw-rWab2M"
      },
      "outputs": [],
      "source": [
        "# This command forces the session to restart.\n",
        "# Run this cell after your installations.\n",
        "# It will cause a notification \"your session crashed for an unknown reason\". This is OK.\n",
        "import os\n",
        "os.kill(os.getpid(), 9)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZFFBvPMR9mS8"
      },
      "source": [
        "Now we are ready to do the imports."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Q8b9aykE9mS8"
      },
      "outputs": [],
      "source": [
        "#@title Imports\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import transformers\n",
        "import evaluate\n",
        "\n",
        "from datasets import load_dataset\n",
        "from torchinfo import summary\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModel, AutoModelForSequenceClassification\n",
        "from transformers import TrainingArguments, Trainer, DataCollatorWithPadding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YIL1eUtV9mTC"
      },
      "source": [
        "### 0.2 Data Acquisition\n",
        "\n",
        "\n",
        "We will use the IMDB dataset delivered as part of the TensorFlow-datasets library, and split into training and test sets. For expedience, we will limit ourselves in terms of train and test examples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 345,
          "referenced_widgets": [
            "aa3e789f8bed406f9db236586d05d056",
            "9ab4fff33d844ba2b4e0841faadfafcf",
            "0164189d1eac4922a5b759568948acb4",
            "8c5d26e092124516893ec24317f58cb0",
            "2b612fac2e14423c8bda6c1580ee121a",
            "29ee84e28c1c4a6aa915c5c1a20b66b2",
            "ef65fa5dadc84c3293cdfbc7c150c25c",
            "8ee583793c554d07b26714bf0e4bfb59",
            "61bbbe888628473a8656d8cd53098b9a",
            "1859f9c40755451781b3d8b1153cf6a6",
            "676a5c0266e64e7982c23de9854b5b92",
            "4f74839850bd4c968f8894b1b5fa7b7f",
            "81f6a7432c694d419cc0d50f03d6e711",
            "fdd437d0dca44a49b1141cd67d9d35f0",
            "0bf79d508b8e424199d399405a0927f2",
            "de16dda5944243e8b539176376140588",
            "9b767e0ac2cc4ecdb083468714ed604a",
            "944b4f3ff09d476887d8ee0366eded7d",
            "0ea721a91bf14f02879fd61b19de5164",
            "30a2b422b6b248238018e9f79b249c98",
            "6c772f46dca14b759b6fb9324d5832e3",
            "8f49b438fb7248ffbc13b1643b33cebc",
            "b2c1a5677251414e83f552ddddf31889",
            "5fb4894edc1c41298354f63aa40b211f",
            "36bb1f08013740ffa3cef0f30c970f74",
            "ef609de192fe43ee9fd0005f114b601f",
            "d7566ca39d844591948d4278a2036c72",
            "cb229f54c42b464b8fcea9e6b25cfff8",
            "6a589f93b03048038bb8d1163e9bd884",
            "dc429a7de4224f389446c2562faff45d",
            "ade7084a20f640cbb0a1d4a372ccf142",
            "3710f3da40f247d7a197c09dcdb41b77",
            "170815f9e9974eb39fe1341578371eea",
            "3563bce6ac08478cacb2ee164441fe2a",
            "b65a2d08bc0f47c5a8604fe08fac7837",
            "49c5f601ac37404fa5d26b3a0133e867",
            "94da97f591f34b5ab45f42164d53ffc3",
            "bba79402dec54a14ab879f3dc7e5a9fc",
            "bfb9bf6f3af243059f3db8c3d0057f98",
            "d8a82ddeacfc4ef783593ed04cfa75c8",
            "33a361c70ccd43839a81d7f266f16ec8",
            "6e4c45f906aa48a68dd7d44632a48506",
            "ffe563940338414f8e95bf7784d6988d",
            "a52491fa9ae7474681695446e832375a",
            "1490009dcf4149848588c9dc20cf65dc",
            "d349ab556f25497597045ad751b94876",
            "48d2cf06582c4a8f84f1c68eab42b947",
            "7879578aa2b34742997575906105de5a",
            "fb2c286a70534a7cb04afb98d3552207",
            "3f83e6ba4b0c4d6fb9974b700bda9363",
            "e8db05654fa341b186d97997da94ac39",
            "38b710bc5d2b4b7ca2589344824b2589",
            "5c7bc4d8546c41138202f37f60359f10",
            "dc74e41617964c9c886286222870c270",
            "0ceecda753ba461b8b4ce4e03344ca38",
            "0f119497c88e45be8ec6937200e851b3",
            "f2e805955b694124a3cdbf03edf7d197",
            "ac18e70b8d0a478c8aff434fb062e812",
            "e1b921bdd29b403997538da8ace85a02",
            "84f6e3c9f86e41d0b38560e2d2989987",
            "51efc6048b0c4192b4af6119d82e3af8",
            "a4b8e1c565fb4e48abcd156fd51184c7",
            "97691e0750b743f3be548b0198445d3c",
            "b79ea8d33daa45bc9ab4184f500a7296",
            "d49ade11816f4fa18012a496e2d65848",
            "bbe1bb9a8a2d41988555de90fca95550",
            "35b100a8018c46048f9320cc5f788dbe",
            "f5399b64b72f42b4bf9a764168bf486a",
            "083e4b74a33d4060bf8bfa6ced9db30c",
            "fd98b7a4f7b24c41a68f2e72d2bb9461",
            "2c8155a5947f4c708faf1ecc0f5eedc8",
            "3fa81a8d58374e2abb0495012ae18363",
            "481253e0d2094d009c722c26645d7da3",
            "56de86e0f82d49aebce123dec73a4a68",
            "dbcd9d8ad5734519b04c4017fdcd5e5b",
            "b755d6600620414f9bd8f5087dc00784",
            "a42786e8d5c5498c9969be13c5427c35"
          ]
        },
        "id": "uwOF0qYb9mTC",
        "outputId": "3977db37-8dbc-4bd8-bc62-b4c1863b9347"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "plain_text/train-00000-of-00001.parquet:   0%|          | 0.00/21.0M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "plain_text/test-00000-of-00001.parquet:   0%|          | 0.00/20.5M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "plain_text/unsupervised-00000-of-00001.p(…):   0%|          | 0.00/42.0M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train split:   0%|          | 0/25000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating test split:   0%|          | 0/25000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating unsupervised split:   0%|          | 0/50000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "imdb_dataset = load_dataset(\"imdb\")\n",
        "\n",
        "imdb_train_dataset = imdb_dataset['train'].shuffle()\n",
        "imdb_dev_dataset = imdb_dataset['test'].shuffle().select(range(5000)) # take first 5000 rows after shuffle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mPHFtgGkHNOQ"
      },
      "source": [
        "It is always highly recommended to look at the data. What do the records look like? Are they clean or do they contain a lot of cruft (potential noise)?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wvmWKdVQ9mTC",
        "outputId": "94238d2d-d615-41fd-84df-799e04e21ef2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['text', 'label'],\n",
              "    num_rows: 25000\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "imdb_train_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BzEnCspD9mTD",
        "outputId": "50b92098-0bf4-4d16-bfaa-804667bf40b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After the unexpected accident that killed an inexperienced climber (Michelle Joyner). Eight months has passed... The Rocky Mountain Rescue receive a distress call set by a brilliant terrorist mastermind Eric Quaien (John Lithgow). Quaien has lost three large cases that has millions of dollars inside. Two experienced climbers Walker (Sylvester Stallone) and Tucker (Micheal Rooker) and a helicopter pilot (Janine Turner) are to the rescue but they are set by a trap by Quaien and his men. Now the two climbers and pilot are forced to play a deadly game of hide and seek. While Quaien is trying to find the millions of dollars and he kidnapped Tucker to find the money. Once Tucker finds the money, Tucker will be dead. Against explosive firepower, bitter cold and dizzying heights. Walker must outwit Quaien for survival.<br /><br />Directed by Renny Harlin (Driven, Mindhunters, A Nightmare on Elm Street 4:The Dream Master) made an entertaining non-stop action picture. This film is a spectacular, exciting, visually exciting action picture with plenty of dark humour as well. This was one of the biggest hits of 1993. This is one of Harlin's best film. Lithgow is a terrific entertaining villain. Stallone certainly made an short comeback of this sharp thriller. This is probably Harlin's best work as a filmmaker.<br /><br />DVD has an sharp anamorphic Widescreen (2.35:1) transfer and an terrific-Dolby Digital 5.1 Surround Sound. DVD has an running commentary track by the director with comments by Stallone. DVD also has technical crew commentary as well. DVD has behind the scenes featurette, two deleted scenes with introduction by the director and more. Do not miss this great action film. Screenplay by Micheal France (Fantastic Four) and actor:Stallone (The Rocky Series). Based on a premise by John Long. Excellent Cinematography by Alex Thomson, B.S.C. (Alien³, Demolition Man, Legend). Oscar Nominated for Best Sound, Best Sound Editing and Best Visual Effects. Panavision. (****/*****).\n",
            "1\n",
            "\n",
            "I think this is one of the weakest of the Kenneth Branagh Shakespearian works. After such great efforts as Much Ado About Nothing, etc. I thought this was poor. The cast was weaker (Alicia Silverstone, Nivoli, McElhone???) but my biggest gripe was that they messed with the Bard's work and cut out some of the play to put in the musical/dance sequences.<br /><br />You just don't do Shakespeare and then mess with the play. Sorry, but that is just wrong. I love some Cole Porter just like the next person, but jeez, don't mess with the Shakespeare. Skip this and watch \"Prospero's Books\" if you want to see a brilliant Shakespearean adaptation of the Tempest.\n",
            "0\n",
            "\n",
            "I reached the end of this and I was almost shouting \"No, no, no, NO! It cannot end here! There are too many unanswered questions! The engagement of the dishwashers? Mona's disappearance? Helmer's comeuppance? The \"zombie\"? Was Little Brother saved by his father? And what about the head???????\" ARGH!! Then I read that at least two of the cast members had passed on and I have to say, I know it probably wouldn't be true to Lars von Trier's vision, but I would gladly look past replacement actors just to see the ending he had planned! Granted, it would be hard to find someone to play Helmer as the character deserves. Helmer, the doctor you love to hate! I think I have yet to see a more self-absorbed, oblivious, self-righteous character on screen! But, I could overlook a change in actors....I just have to know how it ends!\n",
            "1\n",
            "\n",
            "As an employee of the Swedish Air Force I enjoyed the nice Gripen and Hkp 9 (MBB Bo 105) flight scenes in this movie. One of the few disappointments was the EWS 39 jammer pod, in this case an inert Rb 75 (Maverick) missile painted black with the letters \"EWS 39\" in white along the side. Real jammer pods definitely do not look like that, at least not the ones I've seen.<br /><br />But apart from that, it's an entertaining movie with a very amusing ending (the last minute). Anyone interested in seeing various Swedish military units, including the now-legendary SSG, on film should see this one.<br /><br />\n",
            "1\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# This is looking like a sentiment classificaiton dataset with 0 as neg and 1 as pos\n",
        "for i in range(4):\n",
        "  print(imdb_train_dataset['text'][i])\n",
        "  print(imdb_train_dataset['label'][i])\n",
        "  print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "94gU6o4H-vtB",
        "outputId": "96399257-a649-4fa4-bc8e-bb7935fdf019"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['neg', 'pos']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "imdb_train_dataset.features['label'].names # confirmed out thoughts above"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CplHsqSDMKCa"
      },
      "source": [
        "For convenience, in this assignment we will define a sequence length and truncate all records at that length. For records that are shorter than our defined sequence length we will add padding characters to insure that our input shapes are consistent across all records."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Zxu9U3qXMKTW"
      },
      "outputs": [],
      "source": [
        "MAX_SEQUENCE_LENGTH = 100"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1bHwj4vu9mTD"
      },
      "source": [
        "## 0.3. Data Preparation\n",
        "\n",
        "We will need to tokenize the text into vocab_ids to pass into a BERT model. To do so, we'll need to use the specific tokenizer that goes with the model we're using. In this notebook, we will try several different BERT-style models. Let's\n",
        "first write a function that will take the text from our dataset and a tokenizer, and encode the text using that tokenizer. Then we'll apply the function to our dataset for each tokenizer and model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "G1wstKkJBVeb"
      },
      "outputs": [],
      "source": [
        "def preprocess_imdb(data, tokenizer):\n",
        "    review_text = data['text']\n",
        "    # Extract the raw text from the input dictionary.\n",
        "    # For IMDB datasets (like Hugging Face `datasets`), each item is often a dict with keys like \"text\" and \"label\".\n",
        "\n",
        "    encoded = tokenizer.batch_encode_plus(\n",
        "            review_text,              # A string or list of strings to tokenize.\n",
        "            max_length=MAX_SEQUENCE_LENGTH,    # Cap sequence length at 100.\n",
        "            padding='max_length',     # Pad shorter sequences with [PAD] (ID=0) to reach length 100.\n",
        "            truncation=True,          # Truncate longer sequences to 100 tokens.\n",
        "            return_attention_mask=True, # Return an attention mask (1 = real token, 0 = padding).\n",
        "            return_token_type_ids=True, # Return token type IDs (all 0 for single sentences).\n",
        "            return_tensors=\"pt\"       # Return results as PyTorch tensors.\n",
        "        )\n",
        "\n",
        "    return encoded\n",
        "    # The result is a dict with keys:\n",
        "    #   - 'input_ids': shape (batch_size, 100)\n",
        "    #   - 'attention_mask': shape (batch_size, 100)\n",
        "    #   - 'token_type_ids': shape (batch_size, 100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4BGRT1g6a0T6"
      },
      "source": [
        "\n",
        "## 1. BERT-based Classification Models\n",
        "\n",
        "Now we turn to classification with BERT. We will perform classifications with various models that are based on pre-trained BERT models.  If you turn off GPU access while coding and debugging the setup steps, make sure you change the Notebook settings so you can access a GPU when you're ready to train the models.\n",
        "\n",
        "\n",
        "### 1.1. Basics\n",
        "\n",
        "Let us first explore some basics of BERT. We'll start by loading the first pretrained BERT model and tokenizer that we'll use ('bert-base-cased').\n",
        "\n",
        "To explore just the pre-trained portion of the model, we'll use the AutoModel class (equivalent to BertModel, but works for any architecture including BERT). This class gives us the pre-trained model layers up until the last hidden layer (but not any output layer)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 177,
          "referenced_widgets": [
            "b571e1e27df642c6940c45115156d9bc",
            "8361ea6557164aa6aebe95544784f5ac",
            "9fe596e5b1284aff978e8c61c3ae57d6",
            "4bb7f7900ecd4a1a88af71f3bca00c4e",
            "3e4ba00c36c74e0f98bc18a03b234085",
            "7c75baff65dd4a46bf31bf5a33296421",
            "3be967c542c04ed68c4f28be03bf6781",
            "4b951ab77b134d3f8b41a3844abdb2c2",
            "b1b5d225635e414ea807227ab6e89a76",
            "4b521ad2bd7c420e9d966184e396230d",
            "c9a1301fc520477b85c4e2ba0f7e641a",
            "9ab1ad5e087f479a9a04bf6b47ad885d",
            "3421a6a2fcbe414d86bb4c0f442a453a",
            "f613cefcba3e40418725d1ba8129fb32",
            "b8144613581846b2b6953644f72ea9ba",
            "92f9681b9e544087bc5f0c72adcdca95",
            "8d5367bc2cc3467cb59a73a6a754550c",
            "4dedbc714338435db43ffa723f86d597",
            "070aef7693d74780941d16b8e3fca5e4",
            "b7a59d8683744ee6bbd3bde5a320e022",
            "b1d5fcd5f80641b692d3051ff9803ae7",
            "bbd8cc83a77947bfb08ac8cd1a3a9d5a",
            "92f0c7842d9d420aa8a7db00623f3103",
            "224f1d49a6c345838d70bf2e8ad6bdd1",
            "3c7e20fd62034d07816cab8774206909",
            "9c2db654b5d140a08df33e506049e445",
            "a99572dd0c2b4871b94c10af9bc6d48d",
            "cefb6ae899004f389fb9004a66cb4035",
            "25b8be92b5e146adb1e6fba4e03a12da",
            "e0339fdaecae4b50ad9f8363043bb3e9",
            "0ac3643804684840a86765e5d67aac1d",
            "63ad620ee8414c1c8e77bf80685b2f7b",
            "6c36cc8704a64bbba41d65703985a2ee",
            "74eaf5b7a915407b81b5167b77c81a3f",
            "f423d0c42f3945d19b1833138fe84598",
            "ef11a29df125408a83995e177057d862",
            "87fe7c76c8bf4bef82f5079d2a7780f9",
            "1b67eb23376a430bbc21f7894a0f55e1",
            "776ea2873bc04b179491e22d4e43a826",
            "9d9fb987f46c4fd39ecf5c25e88614a3",
            "56c33047d30a4d709d140edfe3eba485",
            "04c6a516adee4ec79b1b8aa15198ce2c",
            "c632fe5f97f94b53b99695485dea56c9",
            "833f9d05b1ce4ba0b1e061f838bc68f6",
            "314742639a004e66858143b1ed4f26bf",
            "5b0aeaa1d8534c43ae9cad34c4a94b8e",
            "b57398c8dce74e7487b075babcca3785",
            "723829e285d140759cb0ac0a2ace0bbc",
            "c2c427dc48044b32ba2fd330d55eebba",
            "8beab0e201b94c399c6b82235f41b94e",
            "252f89b448e54c899a1f4e10de0da95a",
            "9f2da9ca963c4f4790668e7dcf914467",
            "344b70f9f3ee49f2aca319bc0223960c",
            "7438e236d5cd46bcb873db04d3d62154",
            "bd9e8a54a5d6465ea7a452f70a2160e9"
          ]
        },
        "id": "dj9IybD-BtWk",
        "outputId": "63d2d36b-bea4-44d4-a8ac-b497da0747d7"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/436k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/436M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "bert_tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')\n",
        "bert_model = AutoModel.from_pretrained('bert-base-cased')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OU-H_JP7K8Tj"
      },
      "source": [
        "Let's look at a couple of example sentences:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "aM3UggLagPn4"
      },
      "outputs": [],
      "source": [
        "test_input = ['this bank is closed on Sunday', 'the steepest bank of the river is dangerous']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWaNDy5UbmGU"
      },
      "source": [
        "Apply the BERT tokenizer to tokenize them:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nmoptRz0bq1o",
        "outputId": "d8d7cb47-7afb-4ede-83b8-8cf4ffbdba6b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': tensor([[ 101, 1142, 3085, 1110, 1804, 1113, 3625,  102,    0,    0,    0,    0],\n",
              "        [ 101, 1103, 9458, 2556, 3085, 1104, 1103, 2186, 1110, 4249,  102,    0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
              "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]])}"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "tokenized_input = bert_tokenizer(test_input,\n",
        "                                 max_length=12,\n",
        "                                 truncation=True,\n",
        "                                 padding='max_length',\n",
        "                                 return_tensors='pt')\n",
        "\n",
        "tokenized_input"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q8WYd810dQwh"
      },
      "source": [
        " **QUESTION:**\n",
        "\n",
        " 1.a  Why do the attention_masks have 4 and 1 zeros, respectively?  Choose the correct one and enter it in the answers file.\n",
        "\n",
        "  *  For the first example the last four tokens belong to a different segment. For the second one it is only the last token.\n",
        "\n",
        "  *  For the first example 4 positions are padded while for the second one it is only one."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4hpNQPvBehMc",
        "outputId": "31dc8eea-ec55-42c7-c7fe-5e622f816c94"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[ 0.3945,  0.0420,  0.0648,  ...,  0.0505,  0.2236,  0.2424],\n",
              "         [-0.0946,  0.0667, -0.0361,  ...,  0.2193, -0.0697,  0.7445],\n",
              "         [ 0.0056,  0.3132, -0.1798,  ...,  0.1956, -0.1061,  0.4777],\n",
              "         ...,\n",
              "         [ 0.2227, -0.1156,  0.1585,  ...,  0.3003,  0.0163,  0.5133],\n",
              "         [ 0.3164, -0.1099,  0.2366,  ...,  0.1092, -0.1434,  0.3284],\n",
              "         [ 0.3483, -0.1008,  0.2690,  ...,  0.1271, -0.1843,  0.2618]],\n",
              "\n",
              "        [[ 0.4451,  0.2226, -0.0997,  ..., -0.2374,  0.1272,  0.0778],\n",
              "         [ 0.0741, -0.3181, -0.1192,  ..., -0.0668, -0.3062,  0.4692],\n",
              "         [ 0.3146,  0.6266,  0.0061,  ..., -0.0370, -0.0846,  0.7268],\n",
              "         ...,\n",
              "         [ 0.6999, -0.1163,  0.0161,  ..., -0.4744,  0.0573,  0.2183],\n",
              "         [ 0.5603,  0.0854, -0.9192,  ..., -0.3102, -0.0938,  0.3491],\n",
              "         [-0.2686,  0.1133,  0.0756,  ...,  0.3738,  0.0074,  0.1668]]],\n",
              "       grad_fn=<NativeLayerNormBackward0>), pooler_output=tensor([[-0.6653,  0.4733,  0.9999,  ...,  0.9999, -0.7301,  0.9783],\n",
              "        [-0.6259,  0.4442,  0.9998,  ...,  0.9999, -0.7422,  0.9726]],\n",
              "       grad_fn=<TanhBackward0>), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "bert_output = bert_model(**tokenized_input)\n",
        "\n",
        "bert_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o7RyPN6MjUyN",
        "outputId": "9ae18c90-b131-437b-dc28-8cc72ada60f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The shape of last_hidden_state torch.Size([2, 12, 768])\n"
          ]
        }
      ],
      "source": [
        "bert_output.last_hidden_state.shape\n",
        "print(f'The shape of last_hidden_state {bert_output.last_hidden_state.shape}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QSIMG_6RjepS",
        "outputId": "8f7c298e-730f-43f5-c472-a6ff95fd7735"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The shape of pooler_output torch.Size([2, 768])\n"
          ]
        }
      ],
      "source": [
        "# pooler_output = tanh( W @ cls_raw + b )\n",
        "bert_output.pooler_output.shape\n",
        "print(f'The shape of pooler_output {bert_output.pooler_output.shape}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XVNsqd6QRepy"
      },
      "source": [
        " **QUESTION:**\n",
        "\n",
        " 1.b How many outputs are there?\n",
        " - 2\n",
        "\n",
        " Enter your code below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qAfOnO9zov-y",
        "outputId": "dfc5bcaa-478f-4d33-a9f2-1cf17254afb6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The elements in the bert_output: last_hidden_state\n",
            "The elements in the bert_output: pooler_output\n"
          ]
        }
      ],
      "source": [
        "for key in bert_output.keys():\n",
        "  print(f'The elements in the bert_output: {key}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DYM-7tMItaal"
      },
      "source": [
        "**QUESTION:**\n",
        "\n",
        "1.c Which output do we need to use to get token-level embeddings?\n",
        "\n",
        "the first\n",
        "\n",
        "Put your answer in the answers file.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7EYXhams6Bs6"
      },
      "source": [
        "**QUESTION:**\n",
        "\n",
        " 1.d In the tokenized input, which input_id number (i.e. the vocabulary id) corresponds to 'bank' in the two sentences? ('bert_tokenizer.tokenize()' may come in handy.. and don't forget the CLS token! )\n",
        "\n",
        "  - 3085\n",
        "\n",
        "**QUESTION:**\n",
        "\n",
        " 1.e In the array of tokens, which position index number corresponds to 'bank' in the first sentence? ('bert_tokenizer.tokenize()' may come in handy.. and don't forget the CLS token! )\n",
        "  - 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9X-bPMr56Bs6",
        "outputId": "aab51faa-17df-4e70-a2e8-15f39ffc2b78"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': tensor([[ 101, 1142, 3085, 1110, 1804, 1113, 3625,  102,    0,    0,    0,    0],\n",
              "        [ 101, 1103, 9458, 2556, 3085, 1104, 1103, 2186, 1110, 4249,  102,    0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
              "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]])}"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "tokenized_input"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fmC3H1-96Bs6"
      },
      "source": [
        "**QUESTION:**\n",
        "\n",
        "1.f Which array position index number corresponds to 'bank' in the second sentence?\n",
        " - 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "OiJrrKo26Bs6"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nd-Q-3MA6Bs6"
      },
      "source": [
        "**QUESTION:**\n",
        "\n",
        " 1.g What is the cosine similarity between the BERT embeddings for the two occurences of 'bank' in the two sentences?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xVIt83S26Bs6",
        "outputId": "90391650-1bcf-489b-bcae-3fa1a1a06dec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The cosine similarity between the BERT embeddings for the two occurences of bank in the two sentences is 0.74783\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "tok = tokenized_input\n",
        "outs = bert_output                 # from your forward pass\n",
        "H = outs.last_hidden_state         # [B, L, 768]\n",
        "\n",
        "# --- 'bank' in both sentences ---\n",
        "bank_s1 = H[0, 2, :]               # sentence 1, index 2\n",
        "bank_s2 = H[1, 4, :]               # sentence 2, index 4\n",
        "cos_bank = F.cosine_similarity(bank_s1, bank_s2, dim=0)\n",
        "\n",
        "print(f'The cosine similarity between the BERT embeddings for the two occurences of bank in the two sentences is {cos_bank:.5f}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7a2zCWHP6Bs6"
      },
      "source": [
        "**QUESTION:**\n",
        "\n",
        "1.h How does this relate to the cosine similarity of 'this' (in sentence 1) and the first 'the' (in sentence 2). Compute their cosine similarity.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TnEWs6St6Bs6",
        "outputId": "fbf3d2b2-9241-4bd6-8c3f-fae484f89003"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The cosine similarity between the BERT embeddings for the two occurences of \"this\" and \"the\" in the two sentences is 0.81103\n"
          ]
        }
      ],
      "source": [
        "# --- 'this' (sent1) vs 'the' (sent2) ---\n",
        "this_s1 = H[0, 1, :]               # 'this' at index 1 in sent 1\n",
        "the_s2  = H[1, 1, :]               # 'the'  at index 1 in sent 2\n",
        "cos_det = F.cosine_similarity(this_s1, the_s2, dim=0)\n",
        "\n",
        "print(f'The cosine similarity between the BERT embeddings for the two occurences of \"this\" and \"the\" in the two sentences is {cos_det:.5f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KBOvsTBwm_Vi"
      },
      "source": [
        "### 2. Testing Different Pre-Trained BERT Models\n",
        "\n",
        "In the live session we discussed classification with the `bert-base-cased` model, using the Huggingface class BertForSequenceClassification, which comes with a new output layer for our task that we need to train on our dataset.\n",
        "\n",
        "We're going to try different pre-trained models now. Like in the lesson 4 notebook, we'll want to fine-tune each model on our IMDB reviews dataset and compare them with a metric like the validation accuracy. We'll use the model class AutoModelForSequenceClassification, which is equivalent to BertForSequenceClassification, but works for other similar architectures too.\n",
        "\n",
        "Let's write the code we'll need as a function that takes the model and tokenizer as arguments, along with the raw train and dev data. The function will need to tokenize the inputs using the provided tokenizer, so that we can repeat the same code for different pre-trained models. Then the function should create the training args and trainer class, and call trainer.train().\n",
        "\n",
        "The other hyperparameters you'll need are provided in the function definition, including batch_size and num_epochs. You should use the default values provided for those. Use the function provided below for compute_metrics.\n",
        "\n",
        "For now, keep all layers of the pre-trained models you load unfrozen."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "548d1ff13bfb4801a5de2edddbd6f13e",
            "e5c73ea085cf4f03b118447af8b012ee",
            "f531a76e2555405cbf11d2e939fce72a",
            "75e9797360bd428f8d5cacb86c86faf1",
            "7e5c4845100a49fd9c621be46b862dcc",
            "3ecde9e4a4694aacb4876878a201d25e",
            "82af1c66dcf24572b222cc76e4d7a335",
            "7fd8bac47ad441d5a67d195c68d79fe6",
            "4735808d8ef94ab18341e157e5ee8cad",
            "505db53874874ff0b67641ddd5babe37",
            "091647818ac64b6d9efbdec616be65a2"
          ]
        },
        "id": "cTSG8Nuwz2Hm",
        "outputId": "cdaf0d01-d292-413d-85cf-24592c35d434"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading builder script: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "metric = evaluate.load('accuracy')\n",
        "\n",
        "def compute_metrics(p):\n",
        "    # Unpack EvalPrediction to raw arrays\n",
        "    predictions, labels = p\n",
        "    predictions = np.argmax(predictions, axis=1)\n",
        "    return metric.compute(predictions=predictions, references=labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "OkSlB7ZzKb7v"
      },
      "outputs": [],
      "source": [
        "def fine_tune_classification_model(classification_model,\n",
        "                                   tokenizer,\n",
        "                                   train_data,\n",
        "                                   dev_data,\n",
        "                                   batch_size = 16,\n",
        "                                   num_epochs = 2):\n",
        "\n",
        "    preprocessed_train_data = train_data.map(\n",
        "        preprocess_imdb, batched=True, fn_kwargs={'tokenizer': tokenizer}\n",
        "    )\n",
        "    preprocessed_dev_data = dev_data.map(\n",
        "        preprocess_imdb, batched=True, fn_kwargs={'tokenizer': tokenizer}\n",
        "    )\n",
        "\n",
        "    # Ensure expected columns\n",
        "    if 'label' in preprocessed_train_data.column_names:\n",
        "        preprocessed_train_data = preprocessed_train_data.rename_column('label', 'labels')\n",
        "    if 'label' in preprocessed_dev_data.column_names:\n",
        "        preprocessed_dev_data = preprocessed_dev_data.rename_column('label', 'labels')\n",
        "\n",
        "    cols = ['input_ids', 'attention_mask', 'labels']\n",
        "    preprocessed_train_data = preprocessed_train_data.remove_columns(\n",
        "        [c for c in preprocessed_train_data.column_names if c not in cols]\n",
        "    )\n",
        "    preprocessed_dev_data = preprocessed_dev_data.remove_columns(\n",
        "        [c for c in preprocessed_dev_data.column_names if c not in cols]\n",
        "    )\n",
        "\n",
        "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=\"./imdb-bert-out\",\n",
        "        per_device_train_batch_size=batch_size,\n",
        "        per_device_eval_batch_size=batch_size,\n",
        "        num_train_epochs=num_epochs,\n",
        "        eval_strategy=\"epoch\",\n",
        "        save_strategy=\"epoch\",\n",
        "        load_best_model_at_end=True,\n",
        "        metric_for_best_model=\"accuracy\",\n",
        "        greater_is_better=True,\n",
        "        logging_steps=50,\n",
        "        report_to=\"none\",\n",
        "        seed=42,\n",
        "\n",
        "        # Key changes for TPU/CPU safety:\n",
        "        optim=\"adamw_torch\",\n",
        "        torch_compile=False,\n",
        "    )\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=classification_model,\n",
        "        args=training_args,\n",
        "        train_dataset=preprocessed_train_data,\n",
        "        eval_dataset=preprocessed_dev_data,\n",
        "        processing_class=tokenizer,\n",
        "        data_collator=data_collator,\n",
        "        compute_metrics=compute_metrics,\n",
        "    )\n",
        "\n",
        "    trainer.train()\n",
        "    print(\"Validation metrics:\", trainer.evaluate())\n",
        "    return trainer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KkolanVUpoXR"
      },
      "source": [
        "Let's try BERT-base-case first, the same model that was used in the lesson 4 notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292,
          "referenced_widgets": [
            "bdd8006af9b6476cbe7bb836981e6a3c",
            "bc010453d04947f9811991eb55001a58",
            "8050d5f0c42b4ffa94bc40e11da53a65",
            "350651ede0604f428e2146bd6f815ed4",
            "6dca4a58c8a04c17b3ec6861cc2032c0",
            "cac99cc622e14a6bae78915e1d6e6ce0",
            "7545448340874ace9777ead7d934379a",
            "65168744bf6f4227951c7cb4f15d68c5",
            "8b999c40d23c46c7b93cdd0b29e6d010",
            "d1b58a505fc84ecea4316e6b2d180b8b",
            "6e2cd533886640adbf9a382c4b0fa16b",
            "be7b9fae50af4ef1ab911e3abc5805c4",
            "0150f943347b45bbbd224fef48b9adcd",
            "07d25d0304154e169092b6aa104ac31f",
            "6a77bf8d5f614b4eb094f43fd65945a6",
            "242f472fd941477788cdf432dcc35301",
            "d8a293e7377d43fca6a0dbd0e30f2abc",
            "b0be7ff538024bc296430ce557888846",
            "7abed7b6c8014997b9666b412bf05030",
            "ff1c7c478e8748438e170051f7444b6c",
            "df88d60df5374ab4ad410226494f25fd",
            "17a3f6f187a54b458aed484b39848a39"
          ]
        },
        "id": "uC1ovEW5pmGy",
        "outputId": "e502f26f-83fd-429c-e75f-31bc6e954f8c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/5000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='3126' max='3126' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [3126/3126 04:02, Epoch 2/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.368900</td>\n",
              "      <td>0.337474</td>\n",
              "      <td>0.859400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.201800</td>\n",
              "      <td>0.361170</td>\n",
              "      <td>0.864600</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='313' max='313' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [313/313 00:07]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation metrics: {'eval_loss': 0.3611699640750885, 'eval_accuracy': 0.8646, 'eval_runtime': 7.3342, 'eval_samples_per_second': 681.735, 'eval_steps_per_second': 42.677, 'epoch': 2.0}\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<transformers.trainer.Trainer at 0x794619dae030>"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "\"\"\"\n",
        "Show the output from training BERT-base-cased on the IMDB movie reviews dataset.\n",
        "\"\"\"\n",
        "\n",
        "model_checkpoint_name = \"bert-base-cased\"\n",
        "bert_tokenizer = AutoTokenizer.from_pretrained(model_checkpoint_name)\n",
        "bert_classification_model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    \"bert-base-cased\", num_labels=2\n",
        ")\n",
        "\n",
        "fine_tune_classification_model(bert_classification_model, bert_tokenizer, imdb_train_dataset, imdb_dev_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KcK2PyPNoNc2"
      },
      "source": [
        "Often, one of the first choices you have is what pre-trained model you'll want to use. There are quite a few options, especially because other researchers and practitioners fine-tune their own versions of existing models and sometimes make theirs available for others to continue building on.\n",
        "\n",
        "You can search through models available on [Huggingface at this website](https://huggingface.co/models?pipeline_tag=text-classification&sort=trending). Some models were made by Huggingface or other large companies/organizations; other models may have been uploaded by individual users. Notice the search tags on the left, we've already clicked the tag for \"Text Classification\" in the link above. You should see various versions of BERT-style models.\n",
        "\n",
        "For our IMDB classification, we might want to try a model that has been trained on another dataset related to sentiment or emotions. We also want to find models that have a complete model card with documentation about the model architecture and how it was trained, and potentially a link to an associated research paper, and/or a good number of downloads and likes.\n",
        "\n",
        "Take a look at this model: [cardiffnlp/twitter-roberta-base-sentiment](https://huggingface.co/cardiffnlp/twitter-roberta-base-sentiment). It's a RoBERTa model (similar to BERT with slightly different pre-training, often popular for classification tasks), that has already been fine-tuned on the TweetEval benchmark set of tasks for sentiment analysis.\n",
        "\n",
        "The model card indicates that there is an updated version of this model now available. Follow the link to the latest version of the model, and look at that most recent model's card to answer the following questions. Then load that most recent model to train on our task."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m-Exxp4UqAMV"
      },
      "source": [
        "**QUESTION:**\n",
        "\n",
        " 2.a What is the model checkpoint name for the most recent version of this Twitter Roberta-base sentiment analysis model? (Copy and paste the model checkpoint name into the answers file. It should be the full name that you put inside the quotes to load the file below.)\n",
        "\n",
        " **QUESTION:**\n",
        "\n",
        " 2.b Approximately how many tweets was this latest model trained on? (Put the answer in the answers file. You can use the abbreviation for millions like in the model card, e.g. a number like 12M or 85M.)\n",
        "\n",
        " **QUESTION:**\n",
        "\n",
        " 2.c What is the title of the published reference paper for this most recent model? (Copy the full title of the paper and paste it into the answers file.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 573,
          "referenced_widgets": [
            "f89f44d2c2fc4893b37e1e461c14b845",
            "7fabbd3a7d0046d49337b881be03a433",
            "e62ed014055c440d8c1760e835916f63",
            "58199865cba545f6af4ebefb5e1c51da",
            "75f988574ad94b36baf7839c281dc4c3",
            "7fe74864d82b4ab0a0cd7873f1cc4b64",
            "8fa2e4e51bf1441aae9cc35546f2b0e8",
            "5c8cf682d2f043f989fbce6d2c044395",
            "8ee1b488873e400b891873f3a1fe3856",
            "82e5c0f4e1514a4ba9c8e80d8725b715",
            "37b149cdca7f430e84d5dc714a8999eb",
            "9fa094874ef246dcaf1a4572c0a21457",
            "7df51415ba0b496e804a105738a2b729",
            "ea15e9b2470f4e4788508aabb44c2a2a",
            "3a23d1f7fb9f4cb980752734da42b928",
            "30fb628902fd44f7a5ed93c46dbbf490",
            "4943e6eb8cdf45f2961f1d229d12e2c8",
            "dd44859f2ace42988a4161adba2da507",
            "3cbdc74a9f2a42c3a6ae023efa0e2162",
            "e044048a6f3a4cb1bda5ec741d631d5f",
            "df7bb60e909149f48b50b5a3cd4edad8",
            "651dadcd1c0b4d3a828cc0a40a0e5bfa",
            "831f5107dc904af8a698a8882bff4e5d",
            "a38126e345fc4882adab19d0d246c03b",
            "937ddd855958416284b4f63c7d60e665",
            "73701dfec31f46f9a2b49d998197de12",
            "cd47b386187749cc9049a7ed6c9a9bdd",
            "2dc46b6400a646358d4b75d2cf14b82d",
            "b4bc327238994bbcaf5ebce5754581a5",
            "68dea62ce4b7482d9ee49eac0fa8db2a",
            "fac50303c8e7478a8e81e68e59431024",
            "7467cda272c84c65bc9de340b927dc36",
            "c079a022938141f3880f09d0392e1208",
            "cb3c651785c042f68ef00b9951da1c79",
            "28ca9971418947bd82c69236500f33cf",
            "62d1ad58ca33474c85b519fdfd091acd",
            "9650c7c079274b84b90341351efcd479",
            "4ded9c5380604cb1b7c67bb8bc6b69df",
            "eb48c1c029a4443b9a8a808cd19ecf11",
            "f30837b6a22740d3808178f44458a905",
            "850341107bad42bb8e304b481e903fc4",
            "8d4d15546b9b4d90ab7cc1cd43530f89",
            "18c8eef58d8f48bc828922965fc99f5c",
            "ff646032be7f47f7960395eae79742e4",
            "0b06e802324c4e05bf5ee2fb9b93f3f0",
            "9eb6b7bd8f104e558c7bf94846e2f7b5",
            "ecbd5ffd411a422f8963caba39ac8ac3",
            "b1857d5117924977a547b232c920f232",
            "22fef59b5d1a43fda3e821c5bc0f4533",
            "b6fe1b85b28e4c56927421c077b757b4",
            "cde772e8eff64042a351036c8b275d13",
            "6d73cb2e40ca48abb71298e1a8b61760",
            "cfd8378e10f04fb7aa19132b1c726873",
            "c73bc6a3bb1348da8c2955783a74b02e",
            "749f400096444ced9ee60f964869fc9b",
            "191fbe47d08f4f1585c5bc1de5a7498c",
            "2f478b9dc7954689959717570f7c4ecf",
            "9f5d47905c874bfdb62547b00ccbbc05",
            "3c046e1566e34c25967ba08702977cd2",
            "983d2374ebe34d2592437b9f388f2c89",
            "03eca4feb1234e3cb436aa88c0e21118",
            "dad09737e9ac438ebfeeedf6b1a6a710",
            "08cc751dcfab4c0085472217be69dd98",
            "c33831f181d3435caba97fe0058676f7",
            "6f9a6efa878e4baabb18ae86c61bb0e5",
            "5b53531d4c09454f9650bd42f4a4dd4b",
            "d87df60a6e66404db8737f8b5c35147b",
            "72e96ea4a3bc4d769306a7fd324076c0",
            "28370fc22f124bb8b54b3aa9241da9c5",
            "e521ed3dcdce4982ac8b9a36981316ae",
            "2a0c6b24a83040668c973b2be9204ac9",
            "5dc65b213fcf4fb58831ffba8322d1e5",
            "e0da9e90edd04802873e0c3a29c847e3",
            "b9acf1bb1e1346ed97334657f11be322",
            "99642fdac8144ab19209216d7bab8d9b",
            "dcaafaf120104ab09f4da9a371204c47",
            "b92d62c6e7544fe7b69dcc73f5ec9b06",
            "96bb89760d6040d0b951986d51107d3c",
            "03c4a87ae1c74b6cabcc7be0bc24e8ab",
            "dba034752b254b6187abaa830279fc86",
            "0ebe8ba7c9454449a3ca06ef8b9c02b8",
            "110ab3b99aca485eb6898d3197f205bd",
            "820063cbd0904fc682258b75fbcb69ea",
            "0f845085e82946ecaf7e461219e8d57c",
            "d18df210412e4e07b83354d8646ae079",
            "afc683c498db4862a5bbaf9947a353ce",
            "a8b9a2de5e6c4b46bc4459c9539d8225",
            "1ac722f781be40e19b407aac56038b3d"
          ]
        },
        "id": "p9VO4fPfTsQV",
        "outputId": "79959f6f-11c9-4ae6-f372-a2fd5c12c486"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/929 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/501M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest and are newly initialized because the shapes did not match:\n",
            "- classifier.out_proj.weight: found shape torch.Size([3, 768]) in the checkpoint and torch.Size([2, 768]) in the model instantiated\n",
            "- classifier.out_proj.bias: found shape torch.Size([3]) in the checkpoint and torch.Size([2]) in the model instantiated\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/501M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/5000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='3126' max='3126' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [3126/3126 04:08, Epoch 2/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.326700</td>\n",
              "      <td>0.288680</td>\n",
              "      <td>0.878200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.180200</td>\n",
              "      <td>0.305795</td>\n",
              "      <td>0.896000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='313' max='313' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [313/313 00:07]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation metrics: {'eval_loss': 0.30579498410224915, 'eval_accuracy': 0.896, 'eval_runtime': 7.4561, 'eval_samples_per_second': 670.588, 'eval_steps_per_second': 41.979, 'epoch': 2.0}\n"
          ]
        }
      ],
      "source": [
        "# Most recent Twitter RoBERTa sentiment model\n",
        "model_checkpoint_name = \"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
        "\n",
        "# Tokenizer + Model\n",
        "bert_tokenizer = AutoTokenizer.from_pretrained(model_checkpoint_name)\n",
        "# For IMDB (binary), override the head to 2 labels:\n",
        "bert_classification_model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    model_checkpoint_name, num_labels=2, ignore_mismatched_sizes=True\n",
        ")\n",
        "\n",
        "# Train\n",
        "imdb_roberta_trainer = fine_tune_classification_model(\n",
        "    bert_classification_model, bert_tokenizer,\n",
        "    imdb_train_dataset, imdb_dev_dataset\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "vm0UYy4TIsxV",
        "outputId": "73793980-2285-4061-8612-49f557ad83c0"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='939' max='313' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [313/313 00:22]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The validation accuracy of the imdb_roberta_trainer is 0.896\n",
            "Other metrics are {'eval_loss': 0.30579498410224915, 'eval_accuracy': 0.896, 'eval_runtime': 7.7109, 'eval_samples_per_second': 648.433, 'eval_steps_per_second': 40.592, 'epoch': 2.0}\n"
          ]
        }
      ],
      "source": [
        "# Print the val accuracy of the imdb_roberta_trainer\n",
        "print(f'The validation accuracy of the imdb_roberta_trainer is {imdb_roberta_trainer.evaluate()[\"eval_accuracy\"]}')\n",
        "\n",
        "print(f'Other metrics are {imdb_roberta_trainer.evaluate()}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LLjgxylMnC0x"
      },
      "source": [
        "**QUESTION:**\n",
        "\n",
        "2.d What is the final validation accuracy that you observed for the Twitter RoBERTa sentiment-trained model after training for 2 epochs? (Copy and paste the decimal value for the final validation accuracy, e.g. a number like 0.567 or 0.876. Use up to 5 significant digits, though fewer is fine if the output shown in the notebook only has 3 or 4. Put the answer in the answers file; it should match the value shown in your output in this notebook.)\n",
        "\n",
        "**QUESTION:**\n",
        "\n",
        "2.e Did the Twitter RoBERTa sentiment-trained model do better or worse or the same as the BERT-base?\n",
        "\n",
        "\n",
        "**(Answer 2.f below but do NOT enter your sentences in the answers file)**\n",
        "\n",
        "**QUESTION:**\n",
        "\n",
        "2.f Why do you think that happened? (Put your two to three sentence answer in the cell below.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NrzZGZyFsz6K"
      },
      "source": [
        "Please answer 2.f in two to three sentences right here:\n",
        "\n",
        "** BEGIN Q 2.f ANSWER HERE **\n",
        "\n",
        "RoBERTa benefits from stronger pretraining (more data, no NSP, dynamic masking) and this checkpoint was already fine-tuned for sentiment (TweetEval), so its representations separate polarity cues more cleanly than a vanilla BERT-base headstart. After replacing the 3-class head with a 2-class head, fine-tuning quickly adapts those sentiment features to IMDB, overcoming most of the tweet→review domain shift.\n",
        "\n",
        "** END Q 2.f ANSWER HERE. **\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3cMVEBuxro4j"
      },
      "source": [
        "### 3. Unfreezing Different Pre-Trained Layers\n",
        "\n",
        "In the lesson 4 notebook, we tested freezing most or all of the pre-trained BERT model layers. We used the .named_parameters() method, looking at the specific names of each set of model parameters.\n",
        "\n",
        "As in the lesson notebook, we will always want to make sure we keep the classification layer parameters unfrozen, since those need to be trained for our specific task. We will also keep the pooler layer unfrozen, since it's next closest to the classification layer and was only pre-trained in standard BERT models with the next sentence prediction task.\n",
        "\n",
        "For the remaining layers, what happens if we unfreeze lower transformer blocks and keep higher transformer blocks frozen (the opposite of what we did in the lesson notebook)? What if we instead try unfreezing specific types of layers within each transformer block, e.g. all of the self attention layers, or all of the dense layers?\n",
        "\n",
        "Let's modify our fine-tuning function, to add an argument for the layers that we want to train. We'll make that argument a list of strings, and we'll set the default to just unfreeze the classification layer. You'll need to write the code to compare those strings to the names of the model parameters (after loading the specified model) and freeze all parameters that don't match (as in the lesson 4 notebook)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vNwvUHJvbjFy",
        "outputId": "6b179003-367d-45a7-a8e2-edac926fc60c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "roberta.embeddings.word_embeddings.weight\n",
            "roberta.embeddings.position_embeddings.weight\n",
            "roberta.embeddings.token_type_embeddings.weight\n",
            "roberta.embeddings.LayerNorm.weight\n",
            "roberta.embeddings.LayerNorm.bias\n",
            "roberta.encoder.layer.0.attention.self.query.weight\n",
            "roberta.encoder.layer.0.attention.self.query.bias\n",
            "roberta.encoder.layer.0.attention.self.key.weight\n",
            "roberta.encoder.layer.0.attention.self.key.bias\n",
            "roberta.encoder.layer.0.attention.self.value.weight\n",
            "roberta.encoder.layer.0.attention.self.value.bias\n",
            "roberta.encoder.layer.0.attention.output.dense.weight\n",
            "roberta.encoder.layer.0.attention.output.dense.bias\n",
            "roberta.encoder.layer.0.attention.output.LayerNorm.weight\n",
            "roberta.encoder.layer.0.attention.output.LayerNorm.bias\n",
            "roberta.encoder.layer.0.intermediate.dense.weight\n",
            "roberta.encoder.layer.0.intermediate.dense.bias\n",
            "roberta.encoder.layer.0.output.dense.weight\n",
            "roberta.encoder.layer.0.output.dense.bias\n",
            "roberta.encoder.layer.0.output.LayerNorm.weight\n",
            "roberta.encoder.layer.0.output.LayerNorm.bias\n",
            "roberta.encoder.layer.1.attention.self.query.weight\n",
            "roberta.encoder.layer.1.attention.self.query.bias\n",
            "roberta.encoder.layer.1.attention.self.key.weight\n",
            "roberta.encoder.layer.1.attention.self.key.bias\n",
            "roberta.encoder.layer.1.attention.self.value.weight\n",
            "roberta.encoder.layer.1.attention.self.value.bias\n",
            "roberta.encoder.layer.1.attention.output.dense.weight\n",
            "roberta.encoder.layer.1.attention.output.dense.bias\n",
            "roberta.encoder.layer.1.attention.output.LayerNorm.weight\n",
            "roberta.encoder.layer.1.attention.output.LayerNorm.bias\n",
            "roberta.encoder.layer.1.intermediate.dense.weight\n",
            "roberta.encoder.layer.1.intermediate.dense.bias\n",
            "roberta.encoder.layer.1.output.dense.weight\n",
            "roberta.encoder.layer.1.output.dense.bias\n",
            "roberta.encoder.layer.1.output.LayerNorm.weight\n",
            "roberta.encoder.layer.1.output.LayerNorm.bias\n",
            "roberta.encoder.layer.2.attention.self.query.weight\n",
            "roberta.encoder.layer.2.attention.self.query.bias\n",
            "roberta.encoder.layer.2.attention.self.key.weight\n",
            "roberta.encoder.layer.2.attention.self.key.bias\n",
            "roberta.encoder.layer.2.attention.self.value.weight\n",
            "roberta.encoder.layer.2.attention.self.value.bias\n",
            "roberta.encoder.layer.2.attention.output.dense.weight\n",
            "roberta.encoder.layer.2.attention.output.dense.bias\n",
            "roberta.encoder.layer.2.attention.output.LayerNorm.weight\n",
            "roberta.encoder.layer.2.attention.output.LayerNorm.bias\n",
            "roberta.encoder.layer.2.intermediate.dense.weight\n",
            "roberta.encoder.layer.2.intermediate.dense.bias\n",
            "roberta.encoder.layer.2.output.dense.weight\n",
            "roberta.encoder.layer.2.output.dense.bias\n",
            "roberta.encoder.layer.2.output.LayerNorm.weight\n",
            "roberta.encoder.layer.2.output.LayerNorm.bias\n",
            "roberta.encoder.layer.3.attention.self.query.weight\n",
            "roberta.encoder.layer.3.attention.self.query.bias\n",
            "roberta.encoder.layer.3.attention.self.key.weight\n",
            "roberta.encoder.layer.3.attention.self.key.bias\n",
            "roberta.encoder.layer.3.attention.self.value.weight\n",
            "roberta.encoder.layer.3.attention.self.value.bias\n",
            "roberta.encoder.layer.3.attention.output.dense.weight\n",
            "roberta.encoder.layer.3.attention.output.dense.bias\n",
            "roberta.encoder.layer.3.attention.output.LayerNorm.weight\n",
            "roberta.encoder.layer.3.attention.output.LayerNorm.bias\n",
            "roberta.encoder.layer.3.intermediate.dense.weight\n",
            "roberta.encoder.layer.3.intermediate.dense.bias\n",
            "roberta.encoder.layer.3.output.dense.weight\n",
            "roberta.encoder.layer.3.output.dense.bias\n",
            "roberta.encoder.layer.3.output.LayerNorm.weight\n",
            "roberta.encoder.layer.3.output.LayerNorm.bias\n",
            "roberta.encoder.layer.4.attention.self.query.weight\n",
            "roberta.encoder.layer.4.attention.self.query.bias\n",
            "roberta.encoder.layer.4.attention.self.key.weight\n",
            "roberta.encoder.layer.4.attention.self.key.bias\n",
            "roberta.encoder.layer.4.attention.self.value.weight\n",
            "roberta.encoder.layer.4.attention.self.value.bias\n",
            "roberta.encoder.layer.4.attention.output.dense.weight\n",
            "roberta.encoder.layer.4.attention.output.dense.bias\n",
            "roberta.encoder.layer.4.attention.output.LayerNorm.weight\n",
            "roberta.encoder.layer.4.attention.output.LayerNorm.bias\n",
            "roberta.encoder.layer.4.intermediate.dense.weight\n",
            "roberta.encoder.layer.4.intermediate.dense.bias\n",
            "roberta.encoder.layer.4.output.dense.weight\n",
            "roberta.encoder.layer.4.output.dense.bias\n",
            "roberta.encoder.layer.4.output.LayerNorm.weight\n",
            "roberta.encoder.layer.4.output.LayerNorm.bias\n",
            "roberta.encoder.layer.5.attention.self.query.weight\n",
            "roberta.encoder.layer.5.attention.self.query.bias\n",
            "roberta.encoder.layer.5.attention.self.key.weight\n",
            "roberta.encoder.layer.5.attention.self.key.bias\n",
            "roberta.encoder.layer.5.attention.self.value.weight\n",
            "roberta.encoder.layer.5.attention.self.value.bias\n",
            "roberta.encoder.layer.5.attention.output.dense.weight\n",
            "roberta.encoder.layer.5.attention.output.dense.bias\n",
            "roberta.encoder.layer.5.attention.output.LayerNorm.weight\n",
            "roberta.encoder.layer.5.attention.output.LayerNorm.bias\n",
            "roberta.encoder.layer.5.intermediate.dense.weight\n",
            "roberta.encoder.layer.5.intermediate.dense.bias\n",
            "roberta.encoder.layer.5.output.dense.weight\n",
            "roberta.encoder.layer.5.output.dense.bias\n",
            "roberta.encoder.layer.5.output.LayerNorm.weight\n",
            "roberta.encoder.layer.5.output.LayerNorm.bias\n",
            "roberta.encoder.layer.6.attention.self.query.weight\n",
            "roberta.encoder.layer.6.attention.self.query.bias\n",
            "roberta.encoder.layer.6.attention.self.key.weight\n",
            "roberta.encoder.layer.6.attention.self.key.bias\n",
            "roberta.encoder.layer.6.attention.self.value.weight\n",
            "roberta.encoder.layer.6.attention.self.value.bias\n",
            "roberta.encoder.layer.6.attention.output.dense.weight\n",
            "roberta.encoder.layer.6.attention.output.dense.bias\n",
            "roberta.encoder.layer.6.attention.output.LayerNorm.weight\n",
            "roberta.encoder.layer.6.attention.output.LayerNorm.bias\n",
            "roberta.encoder.layer.6.intermediate.dense.weight\n",
            "roberta.encoder.layer.6.intermediate.dense.bias\n",
            "roberta.encoder.layer.6.output.dense.weight\n",
            "roberta.encoder.layer.6.output.dense.bias\n",
            "roberta.encoder.layer.6.output.LayerNorm.weight\n",
            "roberta.encoder.layer.6.output.LayerNorm.bias\n",
            "roberta.encoder.layer.7.attention.self.query.weight\n",
            "roberta.encoder.layer.7.attention.self.query.bias\n",
            "roberta.encoder.layer.7.attention.self.key.weight\n",
            "roberta.encoder.layer.7.attention.self.key.bias\n",
            "roberta.encoder.layer.7.attention.self.value.weight\n",
            "roberta.encoder.layer.7.attention.self.value.bias\n",
            "roberta.encoder.layer.7.attention.output.dense.weight\n",
            "roberta.encoder.layer.7.attention.output.dense.bias\n",
            "roberta.encoder.layer.7.attention.output.LayerNorm.weight\n",
            "roberta.encoder.layer.7.attention.output.LayerNorm.bias\n",
            "roberta.encoder.layer.7.intermediate.dense.weight\n",
            "roberta.encoder.layer.7.intermediate.dense.bias\n",
            "roberta.encoder.layer.7.output.dense.weight\n",
            "roberta.encoder.layer.7.output.dense.bias\n",
            "roberta.encoder.layer.7.output.LayerNorm.weight\n",
            "roberta.encoder.layer.7.output.LayerNorm.bias\n",
            "roberta.encoder.layer.8.attention.self.query.weight\n",
            "roberta.encoder.layer.8.attention.self.query.bias\n",
            "roberta.encoder.layer.8.attention.self.key.weight\n",
            "roberta.encoder.layer.8.attention.self.key.bias\n",
            "roberta.encoder.layer.8.attention.self.value.weight\n",
            "roberta.encoder.layer.8.attention.self.value.bias\n",
            "roberta.encoder.layer.8.attention.output.dense.weight\n",
            "roberta.encoder.layer.8.attention.output.dense.bias\n",
            "roberta.encoder.layer.8.attention.output.LayerNorm.weight\n",
            "roberta.encoder.layer.8.attention.output.LayerNorm.bias\n",
            "roberta.encoder.layer.8.intermediate.dense.weight\n",
            "roberta.encoder.layer.8.intermediate.dense.bias\n",
            "roberta.encoder.layer.8.output.dense.weight\n",
            "roberta.encoder.layer.8.output.dense.bias\n",
            "roberta.encoder.layer.8.output.LayerNorm.weight\n",
            "roberta.encoder.layer.8.output.LayerNorm.bias\n",
            "roberta.encoder.layer.9.attention.self.query.weight\n",
            "roberta.encoder.layer.9.attention.self.query.bias\n",
            "roberta.encoder.layer.9.attention.self.key.weight\n",
            "roberta.encoder.layer.9.attention.self.key.bias\n",
            "roberta.encoder.layer.9.attention.self.value.weight\n",
            "roberta.encoder.layer.9.attention.self.value.bias\n",
            "roberta.encoder.layer.9.attention.output.dense.weight\n",
            "roberta.encoder.layer.9.attention.output.dense.bias\n",
            "roberta.encoder.layer.9.attention.output.LayerNorm.weight\n",
            "roberta.encoder.layer.9.attention.output.LayerNorm.bias\n",
            "roberta.encoder.layer.9.intermediate.dense.weight\n",
            "roberta.encoder.layer.9.intermediate.dense.bias\n",
            "roberta.encoder.layer.9.output.dense.weight\n",
            "roberta.encoder.layer.9.output.dense.bias\n",
            "roberta.encoder.layer.9.output.LayerNorm.weight\n",
            "roberta.encoder.layer.9.output.LayerNorm.bias\n",
            "roberta.encoder.layer.10.attention.self.query.weight\n",
            "roberta.encoder.layer.10.attention.self.query.bias\n",
            "roberta.encoder.layer.10.attention.self.key.weight\n",
            "roberta.encoder.layer.10.attention.self.key.bias\n",
            "roberta.encoder.layer.10.attention.self.value.weight\n",
            "roberta.encoder.layer.10.attention.self.value.bias\n",
            "roberta.encoder.layer.10.attention.output.dense.weight\n",
            "roberta.encoder.layer.10.attention.output.dense.bias\n",
            "roberta.encoder.layer.10.attention.output.LayerNorm.weight\n",
            "roberta.encoder.layer.10.attention.output.LayerNorm.bias\n",
            "roberta.encoder.layer.10.intermediate.dense.weight\n",
            "roberta.encoder.layer.10.intermediate.dense.bias\n",
            "roberta.encoder.layer.10.output.dense.weight\n",
            "roberta.encoder.layer.10.output.dense.bias\n",
            "roberta.encoder.layer.10.output.LayerNorm.weight\n",
            "roberta.encoder.layer.10.output.LayerNorm.bias\n",
            "roberta.encoder.layer.11.attention.self.query.weight\n",
            "roberta.encoder.layer.11.attention.self.query.bias\n",
            "roberta.encoder.layer.11.attention.self.key.weight\n",
            "roberta.encoder.layer.11.attention.self.key.bias\n",
            "roberta.encoder.layer.11.attention.self.value.weight\n",
            "roberta.encoder.layer.11.attention.self.value.bias\n",
            "roberta.encoder.layer.11.attention.output.dense.weight\n",
            "roberta.encoder.layer.11.attention.output.dense.bias\n",
            "roberta.encoder.layer.11.attention.output.LayerNorm.weight\n",
            "roberta.encoder.layer.11.attention.output.LayerNorm.bias\n",
            "roberta.encoder.layer.11.intermediate.dense.weight\n",
            "roberta.encoder.layer.11.intermediate.dense.bias\n",
            "roberta.encoder.layer.11.output.dense.weight\n",
            "roberta.encoder.layer.11.output.dense.bias\n",
            "roberta.encoder.layer.11.output.LayerNorm.weight\n",
            "roberta.encoder.layer.11.output.LayerNorm.bias\n",
            "classifier.dense.weight\n",
            "classifier.dense.bias\n",
            "classifier.out_proj.weight\n",
            "classifier.out_proj.bias\n"
          ]
        }
      ],
      "source": [
        "# Refresh your memory on what the parameter names look like\n",
        "for name, param in bert_classification_model.named_parameters():\n",
        "    print(name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "SRlDR1jaaBEj"
      },
      "outputs": [],
      "source": [
        "def fine_tune_classif_model_freeze_layers(\n",
        "    classification_model,\n",
        "    tokenizer,\n",
        "    train_data,\n",
        "    dev_data,\n",
        "    layers_to_train=(\"classifier.\",),\n",
        "    batch_size=16,\n",
        "    num_epochs=2,\n",
        "):\n",
        "    \"\"\"\n",
        "    Freeze all params whose names DO NOT contain any substring in layers_to_train.\n",
        "    Keep only inputs needed by the model; use dynamic padding and accuracy metric.\n",
        "    \"\"\"\n",
        "    # Preprocess\n",
        "    preprocessed_train_data = train_data.map(\n",
        "        preprocess_imdb, batched=True, fn_kwargs={\"tokenizer\": tokenizer}\n",
        "    )\n",
        "    preprocessed_dev_data = dev_data.map(\n",
        "        preprocess_imdb, batched=True, fn_kwargs={\"tokenizer\": tokenizer}\n",
        "    )\n",
        "    if \"label\" in preprocessed_train_data.column_names:\n",
        "        preprocessed_train_data = preprocessed_train_data.rename_column(\"label\", \"labels\")\n",
        "    if \"label\" in preprocessed_dev_data.column_names:\n",
        "        preprocessed_dev_data = preprocessed_dev_data.rename_column(\"label\", \"labels\")\n",
        "\n",
        "    keep_cols = [\"input_ids\", \"attention_mask\", \"labels\"]\n",
        "    preprocessed_train_data = preprocessed_train_data.remove_columns(\n",
        "        [c for c in preprocessed_train_data.column_names if c not in keep_cols]\n",
        "    )\n",
        "    preprocessed_dev_data = preprocessed_dev_data.remove_columns(\n",
        "        [c for c in preprocessed_dev_data.column_names if c not in keep_cols]\n",
        "    )\n",
        "\n",
        "    # Freeze/unfreeze by substring match\n",
        "    def _is_trainable(name: str, patterns) -> bool:\n",
        "        return any(pat in name for pat in patterns)\n",
        "\n",
        "    trainable_names, frozen_names = [], []\n",
        "    for name, param in classification_model.named_parameters():\n",
        "        if _is_trainable(name, layers_to_train):\n",
        "            param.requires_grad = True\n",
        "            trainable_names.append(name)\n",
        "        else:\n",
        "            param.requires_grad = False\n",
        "            frozen_names.append(name)\n",
        "\n",
        "    print(\"\\nTrainable parameter name substrings:\", layers_to_train)\n",
        "    print(f\"Trainable tensors: {len(trainable_names)} | Frozen tensors: {len(frozen_names)}\")\n",
        "\n",
        "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=\"./imdb-bert-freeze-out\",\n",
        "        per_device_train_batch_size=batch_size,\n",
        "        per_device_eval_batch_size=batch_size,\n",
        "        num_train_epochs=num_epochs,\n",
        "        eval_strategy=\"epoch\",      # (kept as-is per your code)\n",
        "        save_strategy=\"no\",\n",
        "        load_best_model_at_end=False,\n",
        "        logging_steps=50,\n",
        "        report_to=\"none\",\n",
        "        seed=42,\n",
        "        optim=\"adamw_torch\",\n",
        "        torch_compile=False,\n",
        "    )\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=classification_model,\n",
        "        args=training_args,\n",
        "        train_dataset=preprocessed_train_data,\n",
        "        eval_dataset=preprocessed_dev_data,\n",
        "        tokenizer=tokenizer,        # keep (even though deprec warning)\n",
        "        data_collator=data_collator,\n",
        "        compute_metrics=compute_metrics,\n",
        "    )\n",
        "\n",
        "    # ---- Train --------------------------------------------------------------\n",
        "    trainer.train()\n",
        "\n",
        "    # Ensure we have a final eval entry in log_history\n",
        "    _ = trainer.evaluate()\n",
        "\n",
        "    # ---- Extract from log_history ------------------------------------------\n",
        "    logs = trainer.state.log_history\n",
        "\n",
        "    # Last eval record (final epoch)\n",
        "    last_eval = next((rec for rec in reversed(logs) if \"eval_loss\" in rec), {})\n",
        "    final_val_loss = float(last_eval.get(\"eval_loss\", float(\"nan\")))\n",
        "    final_val_acc  = float(last_eval.get(\"eval_accuracy\", float(\"nan\")))\n",
        "    last_eval_step = last_eval.get(\"step\", None)\n",
        "\n",
        "    # Last training loss BEFORE (or at) that eval step\n",
        "    final_train_loss = float(\"nan\")\n",
        "    for rec in reversed(logs):\n",
        "        # training logs have 'loss' (and not 'eval_loss')\n",
        "        if \"loss\" in rec and \"eval_loss\" not in rec:\n",
        "            if last_eval_step is None or rec.get(\"step\", 0) <= last_eval_step:\n",
        "                final_train_loss = float(rec[\"loss\"])\n",
        "                break\n",
        "\n",
        "    ratio = (\n",
        "        final_train_loss / final_val_loss\n",
        "        if (final_val_loss == final_val_loss and final_val_loss != 0.0)\n",
        "        else float(\"nan\")\n",
        "    )\n",
        "\n",
        "    print(\"\\n=== FINAL METRICS (from log_history) ===\")\n",
        "    print(f\"Training loss (final epoch): {final_train_loss:.5f}\")\n",
        "    print(f\"Validation loss (final eval): {final_val_loss:.5f}\")\n",
        "    print(f\"Train/Val loss ratio:         {ratio:.5f}\")\n",
        "    print(f\"Val accuracy (final eval):    {final_val_acc:.5f}\")\n",
        "\n",
        "    return trainer, {\n",
        "        \"train_loss\": final_train_loss,\n",
        "        \"eval_loss\": final_val_loss,\n",
        "        \"eval_accuracy\": final_val_acc,\n",
        "        \"trainval_loss_ratio\": ratio,\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xcLrgI49tBde"
      },
      "source": [
        "We'll go back to using bert-base-cased for this part. First, try freezing the parameters in transformer layers 1-11 (including all parameters with \"layer.#\" in the name). That means you're leaving unfrozen the initial embedding layers, the first transformer layer (numbered 0), and the classification layer.\n",
        "\n",
        "Unfreezing the bottom transformer layer(s) rather than the top one(s) is uncommon, but it's always good to try to understand why. Since we're learning, we'll try doing it this way and see what happens. We've given you the code for this exercise, so that the way to specify layers_to_freeze is clear."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 416,
          "referenced_widgets": [
            "39c14f636e404bd59f7040f1a0e1ed50",
            "db989eb68c1b40ad81c9c34080862681",
            "5637bf3fc7a54f689bf7baa23cd8b921",
            "3567b372cd8848a988c0b1d30f8f444e",
            "ebf8dceb42ad41c997e2f7facd0e58ef",
            "ea1478b4194c45308c995baca32b22b2",
            "e2b8cff9610d4c688eb2f963a14ae323",
            "965057cf0e9746938c033ae3597ec5cc",
            "9836c61809434625ad48f55c7fd22f07",
            "9a8dd090281c4fee89cc17f3c8a2e4a7",
            "49ea16ad79604bb8a106240b4f1a6d04"
          ]
        },
        "id": "AtS29uRbk4Os",
        "outputId": "7411de27-e004-4f3d-fdbc-29899adf17c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/5000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Trainable parameter name substrings: ['embeddings.', 'encoder.layer.0.', 'classifier.']\n",
            "Trainable tensors: 23 | Frozen tensors: 178\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-274536414.py:67: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='3126' max='3126' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [3126/3126 02:53, Epoch 2/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.430400</td>\n",
              "      <td>0.428129</td>\n",
              "      <td>0.800400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.313400</td>\n",
              "      <td>0.422927</td>\n",
              "      <td>0.808000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='313' max='313' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [313/313 00:07]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== FINAL METRICS (from log_history) ===\n",
            "Training loss (final epoch): 0.31340\n",
            "Validation loss (final eval): 0.42293\n",
            "Train/Val loss ratio:         0.74103\n",
            "Val accuracy (final eval):    0.80800\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Show the output from training a BERT-base-cased classification model, when unfreezing\n",
        "only the parameters in the embedding layers, first transformer layer (layer 0), and classifier layer.\n",
        "\"\"\"\n",
        "\n",
        "model_checkpoint_name = \"bert-base-cased\"\n",
        "bert_tokenizer = AutoTokenizer.from_pretrained(model_checkpoint_name)\n",
        "bert_classification_model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    model_checkpoint_name, num_labels=2\n",
        ")\n",
        "\n",
        "# Unfreeze ONLY: embeddings, first encoder block (layer 0), and classifier\n",
        "layers_to_train = [\"embeddings.\", \"encoder.layer.0.\", \"classifier.\"]  # note BERT uses \"encoder.layer\", not \"layer.\"\n",
        "\n",
        "trainer_low_unfrozen, metrics = fine_tune_classif_model_freeze_layers(\n",
        "    bert_classification_model,\n",
        "    bert_tokenizer,\n",
        "    imdb_train_dataset,\n",
        "    imdb_dev_dataset,\n",
        "    layers_to_train=layers_to_train,\n",
        "    batch_size=16,\n",
        "    num_epochs=2\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jmqBmeLjPP8F",
        "outputId": "f01a6e02-5c2c-43f0-91b8-2a605efff71c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bert_text_classification_3.1.a:\n",
            "The validation accuracy of the bert-base-cased trainer_low_unfrozen is 0.85900\n"
          ]
        }
      ],
      "source": [
        "# print the val accuracy\n",
        "print('bert_text_classification_3.1.a:')\n",
        "print(f'The validation accuracy of the bert-base-cased trainer_low_unfrozen is {metrics[\"eval_accuracy\"]:.5f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NiWb3y9anNlG"
      },
      "source": [
        " **QUESTION:**\n",
        "\n",
        "3.a What is the final validation accuracy that you observed for this lowest level unfrozen version of the BERT classification model after training for 2 epochs? (Copy and paste the decimal value into the answers file, as instructed in 2.b)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9CgRacR2cyku"
      },
      "source": [
        "Now try two more versions, this time choosing which layers to train yourself. Instead of focusing on the number of the transformer block (layer.#), focus on the type of layer within each block (the stuff that comes after layer.# in the name).\n",
        "\n",
        "Keep the pooler and classification layers unfrozen in all model versions. Your options to also train include the initial embedding layers and the different components within the transformer blocks (e.g. self attention matrices, dense layers, layer norms).\n",
        "\n",
        "Try to find one combination that does better than the version you just ran above (higher validation accuracy after 2 epochs), without much more overfitting (training_loss / eval_loss > 0.7). Also try to find one version that overfits a lot more after 2 epochs (training_loss / eval_loss < 0.5)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "id": "UC5rFV2ocyqd",
        "outputId": "1fd11231-f8f8-4deb-ae04-989cf6e2d19a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/tmp/ipython-input-274536414.py:67: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Trainable parameter name substrings: ['encoder.layer.10.', 'encoder.layer.11.', 'pooler.', 'classifier.']\n",
            "Trainable tensors: 36 | Frozen tensors: 165\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='3126' max='3126' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [3126/3126 02:03, Epoch 2/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.355500</td>\n",
              "      <td>0.365575</td>\n",
              "      <td>0.839800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.282600</td>\n",
              "      <td>0.328858</td>\n",
              "      <td>0.859000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='313' max='313' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [313/313 00:07]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== FINAL METRICS (from log_history) ===\n",
            "Training loss (final epoch): 0.28260\n",
            "Validation loss (final eval): 0.32886\n",
            "Train/Val loss ratio:         0.85934\n",
            "Val accuracy (final eval):    0.85900\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Show the output from training a particular model on the IMDB movie reviews dataset.\n",
        "Choose layers to train that lead the model to perform better than the one in question 3.a, without overfitting much more.\n",
        "\"\"\"\n",
        "\n",
        "model_checkpoint_name = \"bert-base-cased\"\n",
        "\n",
        "bert_tokenizer = AutoTokenizer.from_pretrained(model_checkpoint_name)\n",
        "bert_classification_model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint_name)\n",
        "\n",
        "### YOUR CODE HERE\n",
        "\n",
        "# Unfreeze TOP layers (domain-adapt the task-relevant features), keep lower layers/embeddings frozen\n",
        "layers_to_train = [\n",
        "    \"encoder.layer.10.\",  # last-2 block\n",
        "    \"encoder.layer.11.\",  # last block\n",
        "    \"pooler.\",            # optional but helpful for CLS projection\n",
        "    \"classifier.\"         # always train the head\n",
        "]\n",
        "\n",
        "### END YOUR CODE\n",
        "\n",
        "trainer_last4_unfrozen, metrics = fine_tune_classif_model_freeze_layers(\n",
        "    bert_classification_model,\n",
        "    bert_tokenizer,\n",
        "    imdb_train_dataset,\n",
        "    imdb_dev_dataset,\n",
        "    layers_to_train\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "jfb3hN3gRwtt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5beb7a46-7c20-4ad4-e7ec-8ee3eb446531"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bert_text_classification_3_2_3_3_b - e:\n",
            "  The training loss is 0.28260\n",
            "  The validation loss is 0.32886\n",
            "  The loss ratio (train/val) is 0.85934\n",
            "  The validation accuracy is 0.85900\n"
          ]
        }
      ],
      "source": [
        "# Print training loss, val loss, ratio of training/val loss, and val accuracy\n",
        "print('bert_text_classification_3_2_3_3_b - e:')\n",
        "print(f\"  The training loss is {metrics['train_loss']:.5f}\")\n",
        "print(f\"  The validation loss is {metrics['eval_loss']:.5f}\")\n",
        "print(f\"  The loss ratio (train/val) is {metrics['trainval_loss_ratio']:.5f}\")\n",
        "print(f\"  The validation accuracy is {metrics['eval_accuracy']:.5f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E0F7EnTzcy1r"
      },
      "source": [
        " **QUESTION:**\n",
        "\n",
        "3.b What is the final training loss that you observed for this better performing version of the BERT classification model after training for 2 epochs? (Copy and paste the decimal value into the answers file, as instructed in 2.b)\n",
        "\n",
        "3.c What is the final validation loss that you observed for this better performing version of the BERT classification model after training for 2 epochs? (Copy and paste the decimal value into the answers file, as instructed in 2.b)\n",
        "\n",
        "3.d What is the ratio of your final training loss/final validation loss? For this better version the ratio must be greater than 0.7.\n",
        "\n",
        "3.e What is the final validation accuracy that you observed for this better performing version of the BERT classification model after training for 2 epochs? (Copy and paste the decimal value into the answers file, as instructed in 2.b)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "4GR6YpCZesdi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "outputId": "b5f89c51-6aee-4e1b-f247-90d6d8e2b18b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/tmp/ipython-input-274536414.py:67: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Trainable parameter name substrings: ['']\n",
            "Trainable tensors: 201 | Frozen tensors: 0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='3126' max='3126' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [3126/3126 03:59, Epoch 2/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.354900</td>\n",
              "      <td>0.331667</td>\n",
              "      <td>0.859800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.149600</td>\n",
              "      <td>0.383825</td>\n",
              "      <td>0.864600</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='313' max='313' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [313/313 00:07]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== FINAL METRICS (from log_history) ===\n",
            "Training loss (final epoch): 0.14960\n",
            "Validation loss (final eval): 0.38383\n",
            "Train/Val loss ratio:         0.38976\n",
            "Val accuracy (final eval):    0.86460\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Show the output from training a particular model on the IMDB movie reviews dataset.\n",
        "Choose layers to train that lead the model to overfit.\n",
        "\"\"\"\n",
        "\n",
        "model_checkpoint_name = \"bert-base-cased\"\n",
        "\n",
        "bert_tokenizer = AutoTokenizer.from_pretrained(model_checkpoint_name)\n",
        "bert_classification_model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint_name)\n",
        "\n",
        "### YOUR CODE HERE\n",
        "\n",
        "layers_to_train = [\"\"] # The empty string is a substring of every name → everything stays trainable.=\n",
        "\n",
        "### END YOUR CODE\n",
        "\n",
        "\n",
        "overfit_trainer, overfit_metrics = fine_tune_classif_model_freeze_layers(\n",
        "    bert_classification_model,\n",
        "    bert_tokenizer,\n",
        "    imdb_train_dataset,\n",
        "    imdb_dev_dataset,\n",
        "    layers_to_train\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'3.f Training loss is {overfit_metrics[\"train_loss\"]:.5f}')\n",
        "print(f'3.g Validation loss is {overfit_metrics[\"eval_loss\"]:.5f}')\n",
        "print(f'3.h Train/Val loss ratio is {overfit_metrics[\"trainval_loss_ratio\"]:.5f}')\n",
        "print(f'3.i Validation accuracy is {overfit_metrics[\"eval_accuracy\"]:.5f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "76dcJEHnsCAh",
        "outputId": "3f1edd7c-f43b-44ce-9f9e-5cba375727e1"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.f Training loss is 0.14960\n",
            "3.g Validation loss is 0.38383\n",
            "3.h Train/Val loss ratio is 0.38976\n",
            "3.i Validation accuracy is 0.86460\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "overfit_trainer.state.log_history"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5gP3HWaOuOXW",
        "outputId": "576bec67-59bf-4734-cd04-40e081df27c6"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'loss': 0.5931,\n",
              "  'grad_norm': 4.214748382568359,\n",
              "  'learning_rate': 4.921625079974408e-05,\n",
              "  'epoch': 0.03198976327575176,\n",
              "  'step': 50},\n",
              " {'loss': 0.5086,\n",
              "  'grad_norm': 5.423033714294434,\n",
              "  'learning_rate': 4.841650671785029e-05,\n",
              "  'epoch': 0.06397952655150352,\n",
              "  'step': 100},\n",
              " {'loss': 0.4685,\n",
              "  'grad_norm': 9.648335456848145,\n",
              "  'learning_rate': 4.7616762635956495e-05,\n",
              "  'epoch': 0.09596928982725528,\n",
              "  'step': 150},\n",
              " {'loss': 0.4863,\n",
              "  'grad_norm': 8.309975624084473,\n",
              "  'learning_rate': 4.68170185540627e-05,\n",
              "  'epoch': 0.12795905310300704,\n",
              "  'step': 200},\n",
              " {'loss': 0.4415,\n",
              "  'grad_norm': 9.238158226013184,\n",
              "  'learning_rate': 4.601727447216891e-05,\n",
              "  'epoch': 0.1599488163787588,\n",
              "  'step': 250},\n",
              " {'loss': 0.4217,\n",
              "  'grad_norm': 14.247499465942383,\n",
              "  'learning_rate': 4.5217530390275114e-05,\n",
              "  'epoch': 0.19193857965451055,\n",
              "  'step': 300},\n",
              " {'loss': 0.3936,\n",
              "  'grad_norm': 6.219278812408447,\n",
              "  'learning_rate': 4.441778630838132e-05,\n",
              "  'epoch': 0.22392834293026231,\n",
              "  'step': 350},\n",
              " {'loss': 0.4606,\n",
              "  'grad_norm': 6.277467250823975,\n",
              "  'learning_rate': 4.3618042226487526e-05,\n",
              "  'epoch': 0.2559181062060141,\n",
              "  'step': 400},\n",
              " {'loss': 0.4323,\n",
              "  'grad_norm': 4.448141098022461,\n",
              "  'learning_rate': 4.281829814459373e-05,\n",
              "  'epoch': 0.28790786948176583,\n",
              "  'step': 450},\n",
              " {'loss': 0.4178,\n",
              "  'grad_norm': 7.168575763702393,\n",
              "  'learning_rate': 4.201855406269994e-05,\n",
              "  'epoch': 0.3198976327575176,\n",
              "  'step': 500},\n",
              " {'loss': 0.386,\n",
              "  'grad_norm': 3.6031229496002197,\n",
              "  'learning_rate': 4.1218809980806145e-05,\n",
              "  'epoch': 0.35188739603326935,\n",
              "  'step': 550},\n",
              " {'loss': 0.4157,\n",
              "  'grad_norm': 5.958076477050781,\n",
              "  'learning_rate': 4.041906589891235e-05,\n",
              "  'epoch': 0.3838771593090211,\n",
              "  'step': 600},\n",
              " {'loss': 0.3851,\n",
              "  'grad_norm': 4.998651027679443,\n",
              "  'learning_rate': 3.961932181701856e-05,\n",
              "  'epoch': 0.41586692258477287,\n",
              "  'step': 650},\n",
              " {'loss': 0.4244,\n",
              "  'grad_norm': 9.745521545410156,\n",
              "  'learning_rate': 3.8819577735124764e-05,\n",
              "  'epoch': 0.44785668586052463,\n",
              "  'step': 700},\n",
              " {'loss': 0.3791,\n",
              "  'grad_norm': 4.097250461578369,\n",
              "  'learning_rate': 3.801983365323097e-05,\n",
              "  'epoch': 0.4798464491362764,\n",
              "  'step': 750},\n",
              " {'loss': 0.3771,\n",
              "  'grad_norm': 4.455612659454346,\n",
              "  'learning_rate': 3.722008957133718e-05,\n",
              "  'epoch': 0.5118362124120281,\n",
              "  'step': 800},\n",
              " {'loss': 0.3582,\n",
              "  'grad_norm': 5.362653732299805,\n",
              "  'learning_rate': 3.642034548944338e-05,\n",
              "  'epoch': 0.5438259756877799,\n",
              "  'step': 850},\n",
              " {'loss': 0.4185,\n",
              "  'grad_norm': 9.552873611450195,\n",
              "  'learning_rate': 3.562060140754959e-05,\n",
              "  'epoch': 0.5758157389635317,\n",
              "  'step': 900},\n",
              " {'loss': 0.3844,\n",
              "  'grad_norm': 6.280353546142578,\n",
              "  'learning_rate': 3.4820857325655796e-05,\n",
              "  'epoch': 0.6078055022392834,\n",
              "  'step': 950},\n",
              " {'loss': 0.3788,\n",
              "  'grad_norm': 5.570363521575928,\n",
              "  'learning_rate': 3.4021113243761995e-05,\n",
              "  'epoch': 0.6397952655150352,\n",
              "  'step': 1000},\n",
              " {'loss': 0.3719,\n",
              "  'grad_norm': 8.149304389953613,\n",
              "  'learning_rate': 3.32213691618682e-05,\n",
              "  'epoch': 0.6717850287907869,\n",
              "  'step': 1050},\n",
              " {'loss': 0.3721,\n",
              "  'grad_norm': 6.846200466156006,\n",
              "  'learning_rate': 3.242162507997441e-05,\n",
              "  'epoch': 0.7037747920665387,\n",
              "  'step': 1100},\n",
              " {'loss': 0.38,\n",
              "  'grad_norm': 4.001773834228516,\n",
              "  'learning_rate': 3.1621880998080614e-05,\n",
              "  'epoch': 0.7357645553422905,\n",
              "  'step': 1150},\n",
              " {'loss': 0.3658,\n",
              "  'grad_norm': 4.113867282867432,\n",
              "  'learning_rate': 3.082213691618682e-05,\n",
              "  'epoch': 0.7677543186180422,\n",
              "  'step': 1200},\n",
              " {'loss': 0.3699,\n",
              "  'grad_norm': 9.529008865356445,\n",
              "  'learning_rate': 3.0022392834293027e-05,\n",
              "  'epoch': 0.799744081893794,\n",
              "  'step': 1250},\n",
              " {'loss': 0.3647,\n",
              "  'grad_norm': 3.987001419067383,\n",
              "  'learning_rate': 2.9222648752399233e-05,\n",
              "  'epoch': 0.8317338451695457,\n",
              "  'step': 1300},\n",
              " {'loss': 0.353,\n",
              "  'grad_norm': 6.1474223136901855,\n",
              "  'learning_rate': 2.842290467050544e-05,\n",
              "  'epoch': 0.8637236084452975,\n",
              "  'step': 1350},\n",
              " {'loss': 0.3571,\n",
              "  'grad_norm': 5.902243137359619,\n",
              "  'learning_rate': 2.7623160588611646e-05,\n",
              "  'epoch': 0.8957133717210493,\n",
              "  'step': 1400},\n",
              " {'loss': 0.3233,\n",
              "  'grad_norm': 8.520042419433594,\n",
              "  'learning_rate': 2.6823416506717852e-05,\n",
              "  'epoch': 0.927703134996801,\n",
              "  'step': 1450},\n",
              " {'loss': 0.3349,\n",
              "  'grad_norm': 6.573343276977539,\n",
              "  'learning_rate': 2.602367242482406e-05,\n",
              "  'epoch': 0.9596928982725528,\n",
              "  'step': 1500},\n",
              " {'loss': 0.3549,\n",
              "  'grad_norm': 1.816605567932129,\n",
              "  'learning_rate': 2.5223928342930265e-05,\n",
              "  'epoch': 0.9916826615483045,\n",
              "  'step': 1550},\n",
              " {'eval_loss': 0.3316672444343567,\n",
              "  'eval_accuracy': 0.8598,\n",
              "  'eval_runtime': 7.3308,\n",
              "  'eval_samples_per_second': 682.052,\n",
              "  'eval_steps_per_second': 42.696,\n",
              "  'epoch': 1.0,\n",
              "  'step': 1563},\n",
              " {'loss': 0.2335,\n",
              "  'grad_norm': 5.629666328430176,\n",
              "  'learning_rate': 2.442418426103647e-05,\n",
              "  'epoch': 1.0236724248240563,\n",
              "  'step': 1600},\n",
              " {'loss': 0.2075,\n",
              "  'grad_norm': 1.003231406211853,\n",
              "  'learning_rate': 2.3624440179142677e-05,\n",
              "  'epoch': 1.055662188099808,\n",
              "  'step': 1650},\n",
              " {'loss': 0.2413,\n",
              "  'grad_norm': 3.291485548019409,\n",
              "  'learning_rate': 2.2824696097248884e-05,\n",
              "  'epoch': 1.0876519513755598,\n",
              "  'step': 1700},\n",
              " {'loss': 0.1848,\n",
              "  'grad_norm': 7.887112617492676,\n",
              "  'learning_rate': 2.2024952015355087e-05,\n",
              "  'epoch': 1.1196417146513116,\n",
              "  'step': 1750},\n",
              " {'loss': 0.2873,\n",
              "  'grad_norm': 20.054710388183594,\n",
              "  'learning_rate': 2.1225207933461293e-05,\n",
              "  'epoch': 1.1516314779270633,\n",
              "  'step': 1800},\n",
              " {'loss': 0.2171,\n",
              "  'grad_norm': 11.664530754089355,\n",
              "  'learning_rate': 2.04254638515675e-05,\n",
              "  'epoch': 1.183621241202815,\n",
              "  'step': 1850},\n",
              " {'loss': 0.2327,\n",
              "  'grad_norm': 8.509485244750977,\n",
              "  'learning_rate': 1.9625719769673705e-05,\n",
              "  'epoch': 1.2156110044785668,\n",
              "  'step': 1900},\n",
              " {'loss': 0.2371,\n",
              "  'grad_norm': 8.542061805725098,\n",
              "  'learning_rate': 1.8825975687779912e-05,\n",
              "  'epoch': 1.2476007677543186,\n",
              "  'step': 1950},\n",
              " {'loss': 0.2187,\n",
              "  'grad_norm': 9.464882850646973,\n",
              "  'learning_rate': 1.8026231605886118e-05,\n",
              "  'epoch': 1.2795905310300704,\n",
              "  'step': 2000},\n",
              " {'loss': 0.2514,\n",
              "  'grad_norm': 8.328486442565918,\n",
              "  'learning_rate': 1.7226487523992324e-05,\n",
              "  'epoch': 1.3115802943058221,\n",
              "  'step': 2050},\n",
              " {'loss': 0.1606,\n",
              "  'grad_norm': 1.1817160844802856,\n",
              "  'learning_rate': 1.642674344209853e-05,\n",
              "  'epoch': 1.3435700575815739,\n",
              "  'step': 2100},\n",
              " {'loss': 0.1867,\n",
              "  'grad_norm': 23.904653549194336,\n",
              "  'learning_rate': 1.5626999360204737e-05,\n",
              "  'epoch': 1.3755598208573256,\n",
              "  'step': 2150},\n",
              " {'loss': 0.2473,\n",
              "  'grad_norm': 20.1584415435791,\n",
              "  'learning_rate': 1.4827255278310942e-05,\n",
              "  'epoch': 1.4075495841330774,\n",
              "  'step': 2200},\n",
              " {'loss': 0.2076,\n",
              "  'grad_norm': 14.85402774810791,\n",
              "  'learning_rate': 1.4027511196417148e-05,\n",
              "  'epoch': 1.4395393474088292,\n",
              "  'step': 2250},\n",
              " {'loss': 0.226,\n",
              "  'grad_norm': 6.9463887214660645,\n",
              "  'learning_rate': 1.3227767114523354e-05,\n",
              "  'epoch': 1.471529110684581,\n",
              "  'step': 2300},\n",
              " {'loss': 0.2408,\n",
              "  'grad_norm': 13.803781509399414,\n",
              "  'learning_rate': 1.2428023032629559e-05,\n",
              "  'epoch': 1.5035188739603327,\n",
              "  'step': 2350},\n",
              " {'loss': 0.2185,\n",
              "  'grad_norm': 7.106577396392822,\n",
              "  'learning_rate': 1.1628278950735765e-05,\n",
              "  'epoch': 1.5355086372360844,\n",
              "  'step': 2400},\n",
              " {'loss': 0.1776,\n",
              "  'grad_norm': 1.9820913076400757,\n",
              "  'learning_rate': 1.082853486884197e-05,\n",
              "  'epoch': 1.5674984005118362,\n",
              "  'step': 2450},\n",
              " {'loss': 0.2025,\n",
              "  'grad_norm': 4.112473964691162,\n",
              "  'learning_rate': 1.0028790786948176e-05,\n",
              "  'epoch': 1.599488163787588,\n",
              "  'step': 2500},\n",
              " {'loss': 0.2088,\n",
              "  'grad_norm': 11.566238403320312,\n",
              "  'learning_rate': 9.229046705054382e-06,\n",
              "  'epoch': 1.6314779270633397,\n",
              "  'step': 2550},\n",
              " {'loss': 0.2453,\n",
              "  'grad_norm': 6.2098517417907715,\n",
              "  'learning_rate': 8.429302623160589e-06,\n",
              "  'epoch': 1.6634676903390915,\n",
              "  'step': 2600},\n",
              " {'loss': 0.1922,\n",
              "  'grad_norm': 19.597496032714844,\n",
              "  'learning_rate': 7.629558541266795e-06,\n",
              "  'epoch': 1.6954574536148432,\n",
              "  'step': 2650},\n",
              " {'loss': 0.2233,\n",
              "  'grad_norm': 9.167903900146484,\n",
              "  'learning_rate': 6.8298144593730006e-06,\n",
              "  'epoch': 1.727447216890595,\n",
              "  'step': 2700},\n",
              " {'loss': 0.1616,\n",
              "  'grad_norm': 1.1714521646499634,\n",
              "  'learning_rate': 6.030070377479207e-06,\n",
              "  'epoch': 1.7594369801663468,\n",
              "  'step': 2750},\n",
              " {'loss': 0.2083,\n",
              "  'grad_norm': 4.178428649902344,\n",
              "  'learning_rate': 5.230326295585413e-06,\n",
              "  'epoch': 1.7914267434420985,\n",
              "  'step': 2800},\n",
              " {'loss': 0.25,\n",
              "  'grad_norm': 6.472029685974121,\n",
              "  'learning_rate': 4.430582213691619e-06,\n",
              "  'epoch': 1.8234165067178503,\n",
              "  'step': 2850},\n",
              " {'loss': 0.1874,\n",
              "  'grad_norm': 5.965070724487305,\n",
              "  'learning_rate': 3.630838131797825e-06,\n",
              "  'epoch': 1.855406269993602,\n",
              "  'step': 2900},\n",
              " {'loss': 0.2057,\n",
              "  'grad_norm': 10.813212394714355,\n",
              "  'learning_rate': 2.831094049904031e-06,\n",
              "  'epoch': 1.8873960332693538,\n",
              "  'step': 2950},\n",
              " {'loss': 0.1954,\n",
              "  'grad_norm': 2.942232370376587,\n",
              "  'learning_rate': 2.0313499680102367e-06,\n",
              "  'epoch': 1.9193857965451055,\n",
              "  'step': 3000},\n",
              " {'loss': 0.1899,\n",
              "  'grad_norm': 6.2927680015563965,\n",
              "  'learning_rate': 1.2316058861164428e-06,\n",
              "  'epoch': 1.9513755598208573,\n",
              "  'step': 3050},\n",
              " {'loss': 0.1496,\n",
              "  'grad_norm': 8.19305419921875,\n",
              "  'learning_rate': 4.318618042226488e-07,\n",
              "  'epoch': 1.983365323096609,\n",
              "  'step': 3100},\n",
              " {'eval_loss': 0.3838253915309906,\n",
              "  'eval_accuracy': 0.8646,\n",
              "  'eval_runtime': 7.3608,\n",
              "  'eval_samples_per_second': 679.271,\n",
              "  'eval_steps_per_second': 42.522,\n",
              "  'epoch': 2.0,\n",
              "  'step': 3126},\n",
              " {'train_runtime': 239.8631,\n",
              "  'train_samples_per_second': 208.452,\n",
              "  'train_steps_per_second': 13.032,\n",
              "  'total_flos': 2569443900000000.0,\n",
              "  'train_loss': 0.30662757924788286,\n",
              "  'epoch': 2.0,\n",
              "  'step': 3126},\n",
              " {'eval_loss': 0.3838253915309906,\n",
              "  'eval_accuracy': 0.8646,\n",
              "  'eval_runtime': 7.322,\n",
              "  'eval_samples_per_second': 682.876,\n",
              "  'eval_steps_per_second': 42.748,\n",
              "  'epoch': 2.0,\n",
              "  'step': 3126}]"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pkZ3k_s3gEAa"
      },
      "source": [
        " **QUESTION:**\n",
        "\n",
        "3.f What is the final training loss that you observed for this overfitting version of the BERT classification model after training for 2 epochs? (Copy and paste the decimal value into the answers file, as instructed in 2.b)\n",
        "\n",
        "3.g What is the final validation loss that you observed for this overfitting version of the BERT classification model after training for 2 epochs? (Copy and paste the decimal value into the answers file, as instructed in 2.b)\n",
        "\n",
        "3.h What is the ratio of your final training loss/final validation loss? For this overfitting version the ratio must be less than 0.5.\n",
        "\n",
        "3.i What is the final validation accuracy that you observed for this overfitting version of the BERT classification model after training for 2 epochs? (Copy and paste the decimal value into the answers file, as instructed in 2.b)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Y3e9X8bvhZf"
      },
      "source": [
        "## Congratulations... You are done!"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
