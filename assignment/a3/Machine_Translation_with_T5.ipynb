{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ELZzyfJxvt1X"
      },
      "source": [
        "# Assignment 3: Machine Translation with T5\n",
        "\n",
        "**Description:** This assignment notebook builds on the material from the\n",
        "[lesson 6 notebook](https://github.com/datasci-w266/2025-fall-main/blob/master/materials/lesson_notebooks/lesson_6_Machine_Translation_With_Transformer.ipynb), in which we set up a new, very small version of a T5 encoder decoder model to train from scratch on translations from Shakespearean to Modern English. Since the model was trained from scratch, it didn't work very well. In this notebook, we'll first try to make that model work a little better, changing the model configuration and output generation parameters. Then we'll fine tune a small pre-trained T5 model on this task, to see how much better we can do with even a small pre-trained model. We'll apply several evaluation metrics, find some trade-offs, and try adding a secondary dataset to address some of the remaining challenges.\n",
        "\n",
        "This notebook should be run on a Google Colab leveraging a GPU. By default, when you open the notebook in Colab it will try to use a GPU. Since colab is providing free access to a GPU they place constraints on that access.  Therefore you might want to turn off the GPU access (Edit -> Notebook Settings) while editing and initially debugging your code (at least the setup before you train each model). You will need a GPU to full train or evaluate each of the models. Total runtime of the entire notebook (with solutions and a Colab GPU) should be about 1-2h, but potentially more depending on how much you experiment. If Colab tells you that you have reached your GPU limit, wait 10-24 hours and you should be able to access a GPU again.\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/datasci-w266/2024-fall-main/blob/master/assignment/a3/Machine_Translation_T5.ipynb)\n",
        "\n",
        "The overall assignment structure is as follows:\n",
        "\n",
        "\n",
        "0. Setup\n",
        "  \n",
        "  0.1 Libraries\n",
        "\n",
        "  0.2 Data Acquisition\n",
        "\n",
        "  0.3. Data Preparation\n",
        "\n",
        "\n",
        "1. Tiny Seq2Seq Model Trained From Scratch\n",
        "  \n",
        "  1.1 Tokenizer and Model Setup\n",
        "\n",
        "  1.2 Experimenting with Model Dimensions\n",
        "\n",
        "  1.3 Text Generation Parameters\n",
        "\n",
        "  1.4 Test Set Evaluation Metrics\n",
        "\n",
        "2. Small Pre-Trained T5 Model\n",
        "\n",
        "  2.1 Pre-Trained Model Setup and Tokenization\n",
        "\n",
        "  2.2 Fine-Tuning the Pre-Trained Model\n",
        "\n",
        "  2.3 Fine-Tuned Model Evaluation\n",
        "\n",
        "  2.4 Style Classifier\n",
        "\n",
        "  2.5 Revisit Decoder .Generate() Options\n",
        "\n",
        "3. Adding Supplementary Paraphrase Dataset\n",
        "\n",
        "  3.1 Load and preprocess the supplemental dataset\n",
        "\n",
        "  3.2 Train T5 on Paraphrasing Task\n",
        "\n",
        "  3.3 Fine-Tune Paraphrase-Trained Model on Main Task\n",
        "  \n",
        "  3.4 Paraphrase-Trained Model Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nUtX9PLp2Ytz"
      },
      "source": [
        "## 0. Setup\n",
        "\n",
        "### 0.1 Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vIz3zJzLvuCp",
        "outputId": "000e06d4-bfa6-4e5a-a6b4-318e4c786747"
      },
      "outputs": [],
      "source": [
        "# !pip install -q -U transformers\n",
        "# !pip install -q -U datasets\n",
        "# !pip install -q -U evaluate\n",
        "# !pip install -q -U tokenizers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "b0sbeNQSLnjt"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/share/crsp/lab/pkaiser/ddlin/mids/datasci-266/2025-fall-main/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import random\n",
        "import numpy as np\n",
        "from scipy.special import softmax\n",
        "\n",
        "import torch\n",
        "import transformers\n",
        "import evaluate\n",
        "from datasets import Dataset, load_dataset\n",
        "\n",
        "# For from-scratch T5 model\n",
        "from transformers import T5TokenizerFast, T5Config, T5ForConditionalGeneration\n",
        "\n",
        "# For pre-trained T5 model\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration  # this won't import twice, just noting here what's for each model\n",
        "\n",
        "# For all T5 models\n",
        "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
        "\n",
        "# For BLEURT (to load a trained model for evaluation)\n",
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
        "\n",
        "# For style classifier model (also for evaluating the seq2seq model output)\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "from transformers import TrainingArguments, Trainer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mJM6ndNBPaSk"
      },
      "source": [
        "### O.2 Data Acquisition\n",
        "\n",
        "We'll use the Shakespeare-to-Modern-English translation dataset from Lesson 6. The data includes aligned sentences from a number of plays by William Shakespeare.\n",
        "\n",
        "The data was copied from this repo --[https://github.com/cocoxu/Shakespeare](https://github.com/cocoxu/Shakespeare) -- and consolidated into one file for easier handling.\n",
        "\n",
        "You will to grab a copy from our git repo and import it to your Google drive.  From there you'll be able to easily load it in to a Colab notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l8rtGshYA3PG",
        "outputId": "16d7bfdb-d9d9-4971-cf38-1376eb2397f6"
      },
      "outputs": [],
      "source": [
        "# # This cell will authenticate you and mount your Drive in the Colab.\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Current working directory: /share/crsp/lab/pkaiser/ddlin/mids/datasci-266/2025-fall-main/assignment/a3\n",
            "GPU available: True\n",
            "GPU device name: NVIDIA A30\n"
          ]
        }
      ],
      "source": [
        "# Check the working directory and GPU\n",
        "import os\n",
        "print(\"Current working directory:\", os.getcwd())\n",
        "print(\"GPU available:\", torch.cuda.is_available())\n",
        "print(\"GPU device name:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU found\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "K7L5o9M3A3SN"
      },
      "outputs": [],
      "source": [
        "# # Modify this path to the appropriate location in your Drive\n",
        "# text_file = 'drive/MyDrive/ISchool/MIDS/266/data/train_plays-org-mod.txt'\n",
        "text_file = 'train_plays-org-mod.txt'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0h92xVgt2nRo"
      },
      "source": [
        "### O.3 Data Preparation\n",
        "\n",
        "Each line contains a Shakespearean sentence and its corresponding modern English translation.\n",
        "\n",
        "The Shakesperean sentence is the *source sequence* and modern English one is the *target sequence*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "0uUVC8dEA3VY"
      },
      "outputs": [],
      "source": [
        "# Read the Shakespeare-to-Modern-English translation data from the text file\n",
        "with open(text_file) as f:\n",
        "    lines = f.read().split(\"\\n\")[:-1]  # Read all lines except the last (which may be empty)\n",
        "\n",
        "text_pairs = []\n",
        "for line in lines:\n",
        "    old, mod = line.split(\"\\t\")  # Split each line into Shakespearean and Modern English\n",
        "    old = old  # Shakespearean sentence\n",
        "    mod = mod  # Modern English translation\n",
        "    text_pairs.append((old, mod))  # Add the pair as a tuple to the list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "94IzHT9OPk92",
        "outputId": "8e090385-05a4-4eab-bfdc-442794a60a41"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "('Be aidant and remediate In the good man’s distress.', 'May they relieve a sick old man’s suffering.')\n",
            "('Go softly on.', 'Go softly on.')\n",
            "('I do follow here in the chase, not like a hound that hunts, but one that fills up the cry.', 'I followed you here in the chase, not like a hound that hunts, but like the hunted by the hound.')\n",
            "('Belovèd Regan, Thy sister’s naught.', 'My dear Regan, your sister’s not worth anything.')\n",
            "('Farewell!', 'Goodbye!')\n"
          ]
        }
      ],
      "source": [
        "# Look at some examples\n",
        "for _ in range(5):\n",
        "    print(random.choice(text_pairs))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uf-BizHPPlUn",
        "outputId": "23851aea-e75d-4881-fb93-bb54fd1047e0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "19088 total pairs\n",
            "16798 training pairs\n",
            "1145 validation pairs\n",
            "1145 test pairs\n"
          ]
        }
      ],
      "source": [
        "# Shuffle in-place so splits are random (not reproducible unless we set a seed)\n",
        "random.shuffle(text_pairs)\n",
        "\n",
        "# Reserve ~6% for validation and ~6% for test; the rest for training.\n",
        "# Note: int() floors; with small datasets this can make val/test = 0.\n",
        "num_val_samples = int(0.06 * len(text_pairs))\n",
        "\n",
        "# Train gets what's left after taking 2 * num_val_samples for val+test\n",
        "num_train_samples = len(text_pairs) - 2 * num_val_samples\n",
        "\n",
        "# Slice the shuffled list into splits\n",
        "train_pairs = text_pairs[:num_train_samples]\n",
        "val_pairs   = text_pairs[num_train_samples : num_train_samples + num_val_samples]\n",
        "test_pairs  = text_pairs[num_train_samples + num_val_samples :]\n",
        "\n",
        "# Report counts\n",
        "print(f\"{len(text_pairs)} total pairs\")\n",
        "print(f\"{len(train_pairs)} training pairs\")\n",
        "print(f\"{len(val_pairs)} validation pairs\")\n",
        "print(f\"{len(test_pairs)} test pairs\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mLrZzsXW_X7W"
      },
      "source": [
        "Like we did in the lesson notebook, let's create a Huggingface dataset object from our data, so that it's easy to work with and pass to our model trainer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "gilXYB0oQgDM"
      },
      "outputs": [],
      "source": [
        "def make_dataset(pairs):\n",
        "    \"\"\"\n",
        "    Build a Hugging Face `Dataset` from (original, modernized) text pairs.\n",
        "\n",
        "    Args:\n",
        "        pairs: Iterable of 2-tuples (original_text, modern_text).\n",
        "               Example: [(\"To be, or not to be\", \"To live or not\"), ...]\n",
        "\n",
        "    Returns:\n",
        "        datasets.Dataset: A shuffled dataset with two string columns:\n",
        "            - \"shakespeare\": original texts\n",
        "            - \"modern\": modernized texts\n",
        "\n",
        "    Notes:\n",
        "        - Uses `Dataset.from_dict` to construct columns.\n",
        "        - Calls `.shuffle()` with the library's default seed. For reproducible\n",
        "          shuffling, call `.shuffle(seed=...)` on the returned dataset.\n",
        "        - Assumes every element of `pairs` unpacks cleanly into two strings.\n",
        "\n",
        "    Example:\n",
        "        >>> ds = make_dataset([(\"a\", \"A\"), (\"b\", \"B\")])\n",
        "        >>> ds.column_names\n",
        "        ['shakespeare', 'modern']\n",
        "        >>> len(ds)\n",
        "        2\n",
        "    \"\"\"\n",
        "    # Unzip list of (orig, modern) tuples into two parallel sequences.\n",
        "    org_texts, mod_texts = zip(*pairs)\n",
        "\n",
        "    # Materialize as lists to ensure they are concrete (not iterators).\n",
        "    org_texts = list(org_texts)\n",
        "    mod_texts = list(mod_texts)\n",
        "\n",
        "    # Construct a Dataset with explicit column names.\n",
        "    dataset = Dataset.from_dict({\"shakespeare\": org_texts, \"modern\": mod_texts})\n",
        "\n",
        "    # Return a shuffled view (use seed=... here if you need determinism).\n",
        "    return dataset.shuffle()\n",
        "\n",
        "# Make the training data\n",
        "train_dataset = make_dataset(train_pairs)\n",
        "\n",
        "# Make the validation data\n",
        "val_dataset = make_dataset(val_pairs)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CyOtMY5wPq6J"
      },
      "source": [
        "## 1. Tiny Seq2Seq Model Trained From Scratch\n",
        "\n",
        "As in the lesson 6 notebook, for our first model, we'll make a new tokenizer and model based on the T5 architecture, which we'll train from scratch only on our task dataset.\n",
        "\n",
        "### 1.1 Tokenizer and Model Setup\n",
        "\n",
        "The easiest way to make a new tokenizer is to load an existing T5 one, then call .train_new_from_iterator(), providing our own dataset and vocab size."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7lBXb7hVPoJ1"
      },
      "outputs": [],
      "source": [
        "# Vocab size = how many distinct tokens your tokenizer can produce (rows in the embedding table).\n",
        "\n",
        "# Embedding size (a.k.a. hidden size, d_model in T5) = the dimensionality of each token vector (columns in the embedding table).\n",
        "\n",
        "VOCAB_SIZE = 15000\n",
        "\n",
        "def get_word_piece_tokenizer(text_samples, vocab_size):\n",
        "    \"\"\"\n",
        "    Train a new T5-style subword tokenizer from raw text.\n",
        "\n",
        "    Args:\n",
        "        text_samples: An iterable (list/generator) of strings. Each item is a training sample.\n",
        "                      Large corpora can be streamed to avoid loading everything in RAM.\n",
        "        vocab_size:   Target vocabulary size (e.g., 15_000). Special tokens are handled\n",
        "                      by the base tokenizer and counted toward the size.\n",
        "\n",
        "    Returns:\n",
        "        transformers.T5TokenizerFast: A newly trained fast tokenizer that keeps\n",
        "        T5's special tokens (e.g., <pad>, </s>) and normalization/pre-tokenization\n",
        "        behavior, but with a vocabulary learned from `text_samples`.\n",
        "\n",
        "    Notes:\n",
        "        - Uses `train_new_from_iterator` on a T5 *fast* tokenizer, which trains a\n",
        "          T5-compatible subword model (Unigram/SentencePiece-like) via 🤗 Tokenizers.\n",
        "        - The iterator will be consumed once. If you pass a generator, it cannot be reused.\n",
        "        - For reproducibility, ensure `text_samples` order is deterministic.\n",
        "        - If you need custom special tokens, pass them when loading the base tokenizer\n",
        "          (or add them after training with `tokenizer.add_special_tokens`).\n",
        "\n",
        "    Example:\n",
        "        >>> corpus = (line.strip() for line in open(\"corpus.txt\"))\n",
        "        >>> tok = get_word_piece_tokenizer(corpus, 15000)\n",
        "        >>> tok(\"To be, or not to be.\")\n",
        "        {'input_ids': [...], 'attention_mask': [...]}\n",
        "    \"\"\"\n",
        "    # Start from a pretrained T5 tokenizer so we inherit T5 specials & processing.\n",
        "    base_tokenizer = T5TokenizerFast.from_pretrained(\"t5-base\")\n",
        "\n",
        "    # Train a new subword vocabulary on your samples, keeping T5 conventions.\n",
        "    new_tokenizer = base_tokenizer.train_new_from_iterator(\n",
        "        text_samples,\n",
        "        vocab_size=vocab_size  # use the function arg (was hardcoded before)\n",
        "    )\n",
        "\n",
        "    return new_tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Training a **single tokenizer** that will be used on **both** styles (Shakespeare → Modern). To work well, that tokenizer needs to learn subword units that cover **both domains**. Splitting into two lists and then concatenating them is just a clear way to say: “train on the **union** of the corpora.”\n",
        "\n",
        "Why this helps:\n",
        "\n",
        "* **Shared vocabulary:** One model/tokenizer handles inputs and targets. Joint training lets it learn pieces common to both (e.g., roots, affixes), improving compression and reducing OOVs for either side.\n",
        "* **Balanced coverage:** If you only fed Shakespeare (or only Modern), the tokenizer would overfit that style and fragment rare words in the other. Concatenating the two lists is the simplest way to include both. (You can also balance explicitly if one side is much bigger.)\n",
        "* **Better sequence lengths:** Joint subwords lead to shorter, more consistent token sequences across styles, which helps training speed and quality.\n",
        "* **Consistency for seq2seq:** In style-transfer/translation setups, sharing the tokenizer eases learning alignments between source and target tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 113,
          "referenced_widgets": [
            "0cee723bc2024390b1cd9dc099120733",
            "dcc28c44883243f59721a0a703460aed",
            "9d8bb85ef8b44a6ba2609c7e592abb69",
            "35ee691203b542109e4a8d5b1109760d",
            "7e09715fde7b4b899e842903db335e0c",
            "2a979ceb96ec4eaca8983c0ab501cb29",
            "c2e7e36c0495456e800b54fb2ab3d6ba",
            "4f27e0cf0ef140919b20336f2b5870de",
            "ec4bf3849f3c4a969b7fa320100e2861",
            "1c6f67039bcb4b59915f911af674c954",
            "66a66c9df68c4cb6a19b7597921d30eb",
            "9a2f9caadcbd451fb34bb07f1a4c3a65",
            "8154f1fafac64a1cb273a80d38583793",
            "1837733d9adc4e66885a309d47bf77af",
            "57294d3fc9544c75b217941281541668",
            "434b02da522648cd95a0e8fcf4c63aab",
            "50bd5b2829754678828679b8b450fafc",
            "53390e0159734b5a8e68badbfc65971c",
            "44f87bf14bb14974ab7565b5913ceedc",
            "015271f5d0c34d8da6fe3d5f4ded771e",
            "e9c807a8ffc14ec5ac8e16b7eb83b9a0",
            "b1e8157479f04d4eafa89ae19ebea9d3",
            "d9eaf2254fd941dfa02f63768b437d70",
            "e44d46e036434a94ae6b244290f6b8f7",
            "f64507ff829b4ae29d5315712cbbb330",
            "8d1211d4f0f24f6a94e0d6cb6ebcdf93",
            "10bf09edd250479d91e8c7d2676225d0",
            "a2df8ac00c6243a0930d1ccfbad3abce",
            "542afb6fc2cf4a83bd3282d3e80f6cf7",
            "be5b09cfec8d4737b744e154baa287b1",
            "f2dd313d38fa45c3a8a7fcf5f945858e",
            "ffd327ce75224bb9afdc22c0a5856e8c",
            "d5c59020a6f84b6ba32ddb0b9ed91475"
          ]
        },
        "id": "-rmXZ8AWQHzI",
        "outputId": "8ff07323-dc4e-4d8b-c94d-b4a812f3b7c1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "shakespeare_samples = [text_pair[0] for text_pair in train_pairs]\n",
        "modern_samples = [text_pair[1] for text_pair in train_pairs]\n",
        "\n",
        "part1_tokenizer = get_word_piece_tokenizer(shakespeare_samples + modern_samples, VOCAB_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Are these the wretches that we threw dice for?'"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "modern_samples[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IRqREBXE_DQV"
      },
      "source": [
        "We'll need to preprocess the data using the tokenizer. Since our task is to translate from Shakespearean to Modern English, the Shakespeare text will be our input_ids and the Modern English will be the labels we use for training and evaluation. We'll create a function to do the tokenization, and then map it to our Huggingface datasets containing the train and validation data. We'll have the function take a tokenizer, because later we'll use a different pre-trained one."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "Xyypul0YQH1s"
      },
      "outputs": [],
      "source": [
        "MAX_SEQUENCE_LENGTH = 40\n",
        "\n",
        "def preprocess_translation_batch(batch_text_pairs, tokenizer, prefix=\"\"):\n",
        "    \"\"\"\n",
        "    Prepare a mini-batch for seq2seq training (e.g., T5) from parallel text.\n",
        "\n",
        "    Args:\n",
        "        batch_text_pairs: A dict-like batch with two string lists of equal length:\n",
        "            - \"shakespeare\": source texts (list[str])\n",
        "            - \"modern\":      target texts (list[str])\n",
        "        tokenizer: A Hugging Face tokenizer with `batch_encode_plus`.\n",
        "                   (For T5-style models, this should already include special tokens.)\n",
        "        prefix: Optional instruction/task prefix prepended to each source string.\n",
        "                Common for T5 prompts (e.g., \"translate Old English to modern: \").\n",
        "\n",
        "    Returns:\n",
        "        dict with:\n",
        "          - 'input_ids': Tensor[int] shape (batch, MAX_SEQUENCE_LENGTH)\n",
        "                         Tokenized + padded + truncated source.\n",
        "          - 'labels':    Tensor[int] shape (batch, MAX_SEQUENCE_LENGTH)\n",
        "                         Tokenized + padded + truncated target.\n",
        "\n",
        "    Notes:\n",
        "        - Uses fixed-length padding ('max_length') to `MAX_SEQUENCE_LENGTH`.\n",
        "        - Truncation will cut off longer samples; pick length carefully.\n",
        "        - Attention masks are not returned here; the model/Trainer may\n",
        "          infer them or you can compute them from padding if needed.\n",
        "        - For label padding masking (-100), handle downstream (e.g., via\n",
        "          data collator) if the loss expects it.\n",
        "    \"\"\"\n",
        "    # If a task prefix is provided, prepend it to every source example.\n",
        "    if prefix:\n",
        "        batch_text_pairs[\"shakespeare\"] = [prefix + text for text in batch_text_pairs[\"shakespeare\"]]\n",
        "\n",
        "    # Tokenize the source side (inputs to the encoder).\n",
        "    shakespeare_encoded = tokenizer.batch_encode_plus(\n",
        "        batch_text_pairs[\"shakespeare\"],\n",
        "        max_length=MAX_SEQUENCE_LENGTH,  # hard cap on tokenized length, hardcoded here\n",
        "        padding='max_length',            # pad to exactly MAX_SEQUENCE_LENGTH\n",
        "        truncation=True,                 # truncate sequences longer than the cap\n",
        "        return_tensors='pt'              # return PyTorch tensors\n",
        "    )\n",
        "\n",
        "    # Tokenize the target side (labels for the decoder).\n",
        "    modern_encoded = tokenizer.batch_encode_plus(\n",
        "        batch_text_pairs[\"modern\"],\n",
        "        max_length=MAX_SEQUENCE_LENGTH,\n",
        "        padding='max_length',\n",
        "        truncation=True,\n",
        "        return_tensors='pt'\n",
        "    )\n",
        "\n",
        "    # Return only IDs expected by most Trainer setups:\n",
        "    #  - input_ids feed the encoder\n",
        "    #  - labels are the teacher-forced targets for the decoder\n",
        "    return {\n",
        "        'input_ids': shakespeare_encoded['input_ids'],\n",
        "        'labels': modern_encoded['input_ids']\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81,
          "referenced_widgets": [
            "20b28cf565d34748b2e34fdf7103241b",
            "822a72aa706f4068914d7362d8be921f",
            "5aede05e8d1448619eb132bc477fdd33",
            "1205832b8000412693338edd6f9c5922",
            "c7ee7663551f457590c689b5be700f4e",
            "4d581dce4ad3439d81762b50a006e5b3",
            "29da90f50c6144ca9f62c181156c4d61",
            "213f55481359481f92d1775c586c8e67",
            "ee4d225887ce488eb199b4ee99efbbeb",
            "eb01de9dda7b439cad4bca6376d032a9",
            "9cea2bac00244cb8a883b90ee2cfe11c",
            "b95a4350742446fe8883795e2f6ae793",
            "edfa69502e314779a9df499b645149d6",
            "6cf637d17d014b9b9ef5313db303aeaf",
            "986f50a7f9424c2f8a6bf2e8cba296fd",
            "43e21926964f4f79890fa334cc2c2d80",
            "9b85c61f4c5545cb9ae25e78fcca674e",
            "b425cf32df67448fb1d807b72bda0dcf",
            "eea69eeb5c1a4678919c3c6daf660237",
            "798870a5021349f2b475806e293cb612",
            "d0145e6c82e74a66a82e7cc6c230aa9e",
            "5ff97f7a7f11484f9e5f4e0f0179f9b4"
          ]
        },
        "id": "PP4hp7d9QH4V",
        "outputId": "5f686847-1adc-4b31-e632-8ba9fceb5e32"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Map: 100%|██████████| 16798/16798 [00:02<00:00, 7654.01 examples/s]\n",
            "Map: 100%|██████████| 1145/1145 [00:00<00:00, 5967.26 examples/s]\n"
          ]
        }
      ],
      "source": [
        "train_ds_part1 = train_dataset.map(preprocess_translation_batch, batched=True,\n",
        "                                   fn_kwargs={'tokenizer': part1_tokenizer})\n",
        "val_ds_part1 = val_dataset.map(preprocess_translation_batch, batched=True,\n",
        "                               fn_kwargs={'tokenizer': part1_tokenizer})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lrqQr1Zby0O3"
      },
      "source": [
        "We'll need to create the new model from a config, specifying the model's dimensions. Then we'll need to make training arguments and trainer objects to be able to train the model. Let's create a function for each of those purposes, so that later we can use the functions to experiment with the available options.\n",
        "\n",
        "First, make a function to create the model config and the model itself. Use the Lesson 6 notebook as a guide, and make sure to include all of the arguments that we've included in the function definition below. Those are what you'll experiment with next."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "uLvnNTVGy0cw"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Fill in the code to create a T5Config and new T5 model, using all of the function arguments\n",
        "\"\"\"\n",
        "\n",
        "def create_from_scratch_model(num_layers, embed_dim, keyvalue_dim, dense_dim, num_heads):\n",
        "    # Build a minimal-but-complete T5 configuration using your hyperparameters.\n",
        "    \n",
        "    \"\"\"\n",
        "    Args:\n",
        "        num_layers: Number of Transformer blocks in both encoder and decoder (T5 shares this count).\n",
        "        embed_dim:  Model hidden size (T5 `d_model`); also the token embedding width.\n",
        "        keyvalue_dim: Dimension per-head for key/value projections (`d_kv` in T5Config).\n",
        "        dense_dim:  Feed-forward (MLP) hidden size (T5 `d_ff`).\n",
        "        num_heads:  Number of attention heads (must divide `embed_dim` evenly).\n",
        "\n",
        "    Returns:\n",
        "        A `T5ForConditionalGeneration` initialized from a fresh `T5Config`.\n",
        "\n",
        "    Notes:\n",
        "        - `vocab_size` is taken from the global VOCAB_SIZE you defined earlier to match your tokenizer.\n",
        "        - `decoder_start_token_id` is set to 0 (T5’s default pad token id). If you have a tokenizer,\n",
        "        you can later do: `model.config.decoder_start_token_id = tokenizer.pad_token_id`.\n",
        "        - If you change your tokenizer’s size, call `model.resize_token_embeddings(len(tokenizer))`.\n",
        "    \"\"\"\n",
        "    \n",
        "    t5_config = T5Config(\n",
        "        vocab_size=VOCAB_SIZE,        # must match your trained tokenizer\n",
        "        d_model=embed_dim,            # hidden size\n",
        "        d_ff=dense_dim,               # feed-forward width\n",
        "        num_layers=num_layers,        # encoder/decoder depth\n",
        "        num_heads=num_heads,          # attention heads\n",
        "        d_kv=keyvalue_dim,            # per-head key/value size\n",
        "        decoder_start_token_id=0      # T5 default (<pad>=0); can be overwritten later\n",
        "    )\n",
        "\n",
        "    # Create an untrained (randomly initialized) seq2seq model from the config.\n",
        "    t5_model = T5ForConditionalGeneration(config=t5_config)\n",
        "    return t5_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sSdOLcYszpda"
      },
      "source": [
        "We'll also need to specify training arguments and a trainer for our model. Use the Seq2SeqTrainingArguments and Seq2SeqTrainer classes imported at the top of this notebook. You can use the Lesson 6 notebook as a guide for this too."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "h6kk5VtzzpoX"
      },
      "outputs": [],
      "source": [
        "\n",
        "def create_seq2seq_training_args(batch_size, num_epochs):\n",
        "    \n",
        "    \"\"\"\n",
        "    Create Hugging Face Seq2SeqTrainingArguments with fixed fields matching the template.\n",
        "\n",
        "    Args:\n",
        "        batch_size: Per-device batch size for both training and evaluation.\n",
        "        num_epochs: Total number of training epochs.\n",
        "\n",
        "    Returns:\n",
        "        A `Seq2SeqTrainingArguments` object configured as in the provided template.\n",
        "    \"\"\"\n",
        "    training_args = Seq2SeqTrainingArguments(\n",
        "        output_dir=\"shakespeare_translation_model\",  # checkpoints/logs directory\n",
        "        eval_strategy=\"epoch\",                       # evaluate at end of each epoch\n",
        "        per_device_train_batch_size=batch_size,      # train batch size per device\n",
        "        per_device_eval_batch_size=batch_size,       # eval batch size per device\n",
        "        num_train_epochs=num_epochs,                 # total epochs\n",
        "        report_to=\"none\"                             # disable external loggers (W&B/Comet)\n",
        "    )\n",
        "    return training_args"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "T-tiQ4GIzpuQ"
      },
      "outputs": [],
      "source": [
        "\n",
        "def create_seq2seq_trainer(model, training_args, train_ds, val_ds):\n",
        "    \n",
        "    \"\"\"\n",
        "    Create a Seq2SeqTrainer that wires model, args, and datasets together.\n",
        "\n",
        "    Args:\n",
        "        model: A `T5ForConditionalGeneration` (or compatible seq2seq) model.\n",
        "        training_args: The `Seq2SeqTrainingArguments` returned above.\n",
        "        train_ds: Tokenized training dataset (must provide 'input_ids' and 'labels').\n",
        "        val_ds: Tokenized validation dataset (same feature keys as train_ds).\n",
        "\n",
        "    Returns:\n",
        "        A `Seq2SeqTrainer` ready to `.train()`.\n",
        "    \"\"\"\n",
        "    trainer = Seq2SeqTrainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_ds,\n",
        "        eval_dataset=val_ds\n",
        "    )\n",
        "    return trainer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ITxgfivHQpuP"
      },
      "source": [
        "### 1.2: Experimenting with Model Dimensions\n",
        "\n",
        "In the Lesson 6 Notebook, we created a very small T5-style model with just one transformer layer and smaller dimensions for some of the internal layers. Now, you'll explore these options yourself, to see if you can get the model to work a little better when trained on this task.\n",
        "\n",
        "Without adding any additional training data, can we configure the model to perform better when trained on this task? What happens if we add another one or more transformer layers to the encoder and decoder, or make some of the internal dimensions smaller or larger?\n",
        "\n",
        "The T5Config gives us several hyperparameters to adjust the model's parameter dimensions. You can see the available arguments and their default values in the [T5Config documentation](https://huggingface.co/docs/transformers/v4.46.3/en/model_doc/t5#transformers.T5Config).\n",
        "\n",
        "We'll give you the batch size and num_epochs:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "B6AgagA7XkMP"
      },
      "outputs": [],
      "source": [
        "part1_batch_size = 64\n",
        "part1_num_epochs = 30"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4qOChzNoTWOd"
      },
      "source": [
        "Now you decide the rest.\n",
        "\n",
        "Try changing the values for *num_layers* (number of transformer blocks), *d_model* (size of embedding and pooler layers), *d_kv* (size of query, key, and value vectors per attention head), *num_heads* (the number of attention heads), and *d_ff* (size of feed forward layers after each attention layer).\n",
        "\n",
        "Find hyperparameters that finish training 30 epochs in 10-20 minutes on a free Colab T4 GPU, and that give you as low of a validation loss as you can, at least below 1.8. Also try to do this without overwhelming overfitting, i.e. try to keep training_loss / validation_loss > 0.6 after 30 epochs.\n",
        "\n",
        "Then answer the questions below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "babjqnZQIPcA"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Define the values you want to use for d_model, d_kv, num_heads, and d_ff, for the T5Config below.\n",
        "\"\"\"\n",
        "\n",
        "### YOUR CODE HERE\n",
        "\n",
        "embed_dim   = 256     # d_model\n",
        "keyvalue_dim= 32      # d_kv (per-head size; 256 / 8)\n",
        "num_heads   = 8       # must divide d_model\n",
        "dense_dim   = 1024    # d_ff ≈ 4 * d_model\n",
        "num_layers  = 3       # encoder/decoder depth (balanced speed vs quality)\n",
        "\n",
        "\n",
        "### END YOUR CODE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "54FG64BEIXDb",
        "outputId": "fa3dd8fd-d78e-40cb-94a9-2b9ff82aecdb"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='7890' max='7890' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [7890/7890 08:11, Epoch 30/30]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>2.569462</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>2.686400</td>\n",
              "      <td>2.430478</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>2.686400</td>\n",
              "      <td>2.343031</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>2.375000</td>\n",
              "      <td>2.272528</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>2.375000</td>\n",
              "      <td>2.210593</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>2.249500</td>\n",
              "      <td>2.165639</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>2.249500</td>\n",
              "      <td>2.122116</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>2.147800</td>\n",
              "      <td>2.079524</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>2.147800</td>\n",
              "      <td>2.043637</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>2.068100</td>\n",
              "      <td>2.014134</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>2.068100</td>\n",
              "      <td>1.988580</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>1.990700</td>\n",
              "      <td>1.962418</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>1.990700</td>\n",
              "      <td>1.945435</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>1.945700</td>\n",
              "      <td>1.919889</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>1.945700</td>\n",
              "      <td>1.904985</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>1.906200</td>\n",
              "      <td>1.890423</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>1.906200</td>\n",
              "      <td>1.879721</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>1.868100</td>\n",
              "      <td>1.865306</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>1.868100</td>\n",
              "      <td>1.854893</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>1.838200</td>\n",
              "      <td>1.845437</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21</td>\n",
              "      <td>1.814200</td>\n",
              "      <td>1.839150</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22</td>\n",
              "      <td>1.814200</td>\n",
              "      <td>1.833182</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23</td>\n",
              "      <td>1.799500</td>\n",
              "      <td>1.827077</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24</td>\n",
              "      <td>1.799500</td>\n",
              "      <td>1.821321</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>1.785500</td>\n",
              "      <td>1.815217</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26</td>\n",
              "      <td>1.785500</td>\n",
              "      <td>1.812934</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27</td>\n",
              "      <td>1.762800</td>\n",
              "      <td>1.811240</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>1.762800</td>\n",
              "      <td>1.807869</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29</td>\n",
              "      <td>1.763400</td>\n",
              "      <td>1.806570</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>1.763400</td>\n",
              "      <td>1.806125</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=7890, training_loss=1.9883010999633635, metrics={'train_runtime': 491.8976, 'train_samples_per_second': 1024.482, 'train_steps_per_second': 16.04, 'total_flos': 666334785945600.0, 'train_loss': 1.9883010999633635, 'epoch': 30.0})"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "part1_model = create_from_scratch_model(num_layers, embed_dim, keyvalue_dim, dense_dim, num_heads)\n",
        "part1_training_args = create_seq2seq_training_args(part1_batch_size, part1_num_epochs)\n",
        "part1_trainer = create_seq2seq_trainer(part1_model, part1_training_args,\n",
        "                                       train_ds_part1, val_ds_part1)\n",
        "\n",
        "part1_trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5cwO4nWICZa6"
      },
      "source": [
        "**QUESTION:**\n",
        "\n",
        " 1.a What is the final validation loss that you were able to achieve for the part1 model after training for 30 epochs? (Copy and paste the decimal value for the final validation loss, to 5 significant digits, e.g. a number like 0.56781 or 0.87632. Put the answer in the answers file; it should match the value shown in your output in this notebook.)\n",
        " - 1.8061\n",
        "\n",
        "**QUESTION:**\n",
        "\n",
        " 1.b Which model config parameters (if any) did you increase, to achieve a lower validation loss, while staying within the training time and overfitting guidelines? (List the names of the parameters you increased, e.g. embed_dim, keyvalue_dim, num_heads, dense_dim, num_layers. Put this list in square brackets in the answers file.)\n",
        " - num_layers\n",
        "\n",
        "**QUESTION:**\n",
        "\n",
        " 1.c Which model config parameters (if any) did you decrease, to achieve a lower validation loss, while staying within the training time and overfitting guidelines? (List the names of the parameters you decreased, e.g. embed_dim, keyvalue_dim, num_heads, dense_dim, num_layers. Put this list in square brackets in the answers file.)\n",
        "  - dense_dim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "Tdtz6k6ZwG4G"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Before moving on, save a checkpoint of the model you just trained in your Drive,\n",
        "So that you can pick up where you left off later if needed\n",
        "\"\"\"\n",
        "\n",
        "# Modify this path to the location in your Drive where you want to save the part1 model\n",
        "# part1_model_checkpoint_filepath = 'drive/MyDrive/ISchool/MIDS/266/model_checkpoints/part1_model'\n",
        "part1_model_checkpoint_filepath = 'model_checkpoints/part1_model'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "Ucobq-zywIYQ"
      },
      "outputs": [],
      "source": [
        "# Run this line only after you've trained the part1 model\n",
        "part1_model.save_pretrained(part1_model_checkpoint_filepath, from_pt=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "cou4qT7-wIah"
      },
      "outputs": [],
      "source": [
        "# Run this line only if you need to reload the model you trained earlier\n",
        "part1_model = T5ForConditionalGeneration.from_pretrained(part1_model_checkpoint_filepath)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Us-I-FEMAkue"
      },
      "source": [
        "### 1.3: Text Generation Parameters\n",
        "\n",
        "Cross-entropy loss is great for training, but it's not a very interpretable metric for manually reviewing how well the model is doing as we experiment with available options. Ultimately, we want to actually look at the translations the model outputs, compare them to human translations, and potentially judge other aspects of the actual output.\n",
        "\n",
        "To do that, we need to actually generate some model output. Remember that the model itself predicts probabilities for each word in the vocabulary, based on what words have already been generated, at each decoder time-step. In order to select which actual words to output, there are multiple decoder strategies we can use that are build on top of the model's predicted probabilities. (E.g. beam search, top-k or top-p sampling, repeat ngram constraints, min/max length constraints, etc.)\n",
        "\n",
        "Let's define a function below to generate translations for new inputs. Then we'll define another function to translate the validation set and calculate some standard evaluation metrics for translation, as well as print out some translations for manual inspection. We'll include some arguments that you'll experiment with next."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "mL_ldFqfAiH5"
      },
      "outputs": [],
      "source": [
        "def generate_output(model, tokenizer, input_sentences, batch_size, **kwargs):\n",
        "    \"\"\"\n",
        "    Generate decoded text for a list of input sentences in mini-batches.\n",
        "\n",
        "    Args:\n",
        "        model:       A seq2seq HF model with `.generate(...)` (e.g., T5ForConditionalGeneration).\n",
        "        tokenizer:   Matching tokenizer; must support `__call__` and `batch_decode`.\n",
        "        input_sentences: List[str] of source texts to translate/generate from.\n",
        "        batch_size:  Integer batch size used to chunk `input_sentences`.\n",
        "        **kwargs:    Extra arguments passed through to `model.generate(...)`\n",
        "                     (e.g., num_beams, do_sample, top_p, temperature, max_new_tokens, etc.).\n",
        "\n",
        "    Returns:\n",
        "        List[str]: One decoded string per input sentence, in original order.\n",
        "\n",
        "    Notes:\n",
        "        - This moves the model to CUDA inside the loop (`model.cuda()`), which works but is\n",
        "          inefficient; typically we'll move the model to device once before the loop.\n",
        "        - Only `input_ids` are passed to `generate`; if your model benefits from attention\n",
        "          masks, consider including `inputs_encoded['attention_mask']` (outside this function).\n",
        "        - The batching loop uses `range(int(len(...) / batch_size) + 1)` to cover the final\n",
        "          partial batch; we break when `start_i` exceeds the input length.\n",
        "        - `skip_special_tokens=True` strips tokens like <pad>, </s>, etc.; set to False if\n",
        "          we need to inspect them.\n",
        "    \"\"\"\n",
        "    all_outputs = []\n",
        "\n",
        "    # Iterate over contiguous chunks of size `batch_size`\n",
        "    for i in range(int(len(input_sentences) / batch_size) + 1):\n",
        "        start_i, end_i = i * batch_size, (i + 1) * batch_size\n",
        "        if start_i >= len(input_sentences):  # stop when no items remain\n",
        "            break\n",
        "\n",
        "        # Tokenize the current chunk and return PyTorch tensors.\n",
        "        # padding=True pads to the longest sequence in this mini-batch.\n",
        "        inputs_encoded = tokenizer(\n",
        "            input_sentences[start_i:end_i],\n",
        "            padding=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        # Move model and inputs to GPU (as written). `**kwargs` forwards decoding options.\n",
        "        output_ids = model.cuda().generate(\n",
        "            inputs_encoded['input_ids'].cuda(),\n",
        "            **kwargs\n",
        "        )\n",
        "\n",
        "        # Convert generated token IDs back to strings.\n",
        "        generated_sentences = tokenizer.batch_decode(\n",
        "            output_ids,\n",
        "            skip_special_tokens=True,\n",
        "            clean_up_tokenization_spaces=False\n",
        "        )\n",
        "\n",
        "        # Accumulate this batch’s outputs\n",
        "        all_outputs.extend(generated_sentences)\n",
        "\n",
        "    return all_outputs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 305,
          "referenced_widgets": [
            "e2783b1660b84198b6eb991d22afc167",
            "ae302947034b42f194221a5437526636",
            "8ceb218cf12c4bddb6375234a246bfd2",
            "22343b2f03a0463fa72d2c2d6e98e6da",
            "2369e7363a424828a885f41c571405c9",
            "aaa436f7859a4770865512aceee2227e",
            "9d6d4ea09d1e47c1a79b40977f15822f",
            "1a352d3fee424f0589208d258eef5a9e",
            "d7c32fff440046d0889b7fed207d7323",
            "b9edd75bcaaa4f02aec0b1bfe735f295",
            "790180d69f4f497f8d9f4f7304f441c2",
            "c9a912764c1d480bae7628525cd6f98e",
            "a3c1afd1744c4f04b1c00b528565947b",
            "db07430f04aa46de9fb21f749aa8486b",
            "a310dd00642c4747b3a1229803300db5",
            "3bf5734268e14119abfd90739f6216bd",
            "b43fc14f244643abb06c9883f54e5319",
            "62747153d54a44a59d70483835b565d3",
            "417532ae8560402e80e9815b3b09084e",
            "e0344068c0e440338a696eccb282bca2",
            "1a30b4d7f7c14cfeb7585b989338a1d8",
            "98cf36401c6c4f8196753ad1cf82045e",
            "079649b864484915aa1b22e16217838b",
            "26ac78c58347474ca0e8bb0d9aaece27",
            "9b8f681444734c2e99b122ceed2f58f5",
            "ec23ada8a6f74105857cc77e4ca828b1",
            "63b25a5f160c4429ba3137525841bf72",
            "e20d1f88fac84250844c22b2abee25a1",
            "462d5162420a4aff867e00561ac124dd",
            "89e79deeb9f148788b57e1ad9e7b4901",
            "a2715188bd1b4120b70eaa1df5b79cf4",
            "2fc7eeecd6d346a4b175414a44d62b6b",
            "e8b179e252514e78a00a3e2eda3d950c",
            "0d7dc188fa2e47be9022af11838fba12",
            "d3c5865bb970437aa4797a1f9478e3f3",
            "3a791b60cbf14d77a6a9cf2b6739192f",
            "bc1b5e0c93c44db3ae6bb5e644608001",
            "1e5d421e93064fc69545613258405dcc",
            "a87814859c744658b1179d9d2fdb39ee",
            "3905dc52fb6c47d09820092a3db875e6",
            "298c7339b5de44fd917666d576246c07",
            "2e79fdddfb3940f989e7c2adeb9bea36",
            "a839d4efe65d47f78a3d6cdd2f66649d",
            "766fcf4b146644d48d0c6aa2c37717a7",
            "3c28a0d23d68477e988c6582f0ef9ae2",
            "27487c3bdf74456b8c9d7eba02172b26",
            "7430a5f2bdd044c1a2fde981aa8be06b",
            "eb1e905077394c27957e640e615c977a",
            "a0e1f831e4fd44f3990ea6d9fa38ef83",
            "113c480b8ef34fa2bc8a2a783c481238",
            "3f0f19676cd445f3bbc251b4d16240cd",
            "ee20eceb129c442689d65882aa68a14c",
            "44d9416ed9d748a5bf9c2230a1598d86",
            "37fe6ae682c14878b75b2716eb6fa559",
            "714679346ae24bcd990269f020c28655",
            "81e1814ebc5f4cf494d26d4b2ae567bf",
            "222493064e1e4581a904a7e9aec35ac6",
            "cc05de348295429983a1fbe05a9a6f16",
            "a5ab49e2d424466884167e2c8f6ece55",
            "23b5188ab9be4a4291c14297c4170eef",
            "0b30bb49b48d4ac4a5bae2b88b36997d",
            "d99cfcbc779b413da0133f18f71cb0ff",
            "7df7cb33c8334ace915baf62ca3e60b9",
            "aa59d91e789a457ea1bcfc19a0384e07",
            "f888cc40e2b942b79082d4453a5bced1",
            "bb24219376b4438cb0464b1890b99793",
            "40f940323db0495d940180f39e82a064",
            "164a69f7312e49b18390709f85b59011",
            "203ff355623a4ed9be39fa9c1c0b19d3",
            "5a8c561bd7c5436196c04726a04fdbd8",
            "6e2b5cc9c5a84bdbb3a6c1c208bb504f",
            "b649a847adca448bab0c5f8eb90dabf9",
            "ab2c7d9a38e5485ea1227de0f165ce03",
            "a20562a50bfd4ea3a9fd41aaa199f454",
            "539953c122504ad996a566209ec26a47",
            "508112d0e1284321912bba67b4681d86",
            "01ebdf15b72245ea9e7a26feedd6a11e",
            "b47805bd54a34077b2de2a1db64f938f",
            "365b5d0703344670904e2731d64deeaa",
            "982e572653754556bdfdb4cf011267b9",
            "d06314c83e884f91927993e4891962b6",
            "662a99198f594d8f8722faad18a8ec61",
            "fb1d759eb4374067b03a8c3f177ae4df",
            "e6004e368a1640fbbe0eeb9b52b061ab",
            "1279077499ab4d9cad37c20de1b5a17d",
            "d2b31b8702264e5ab919a67d5c7eeceb",
            "c23219c76c8748bfb41ac328c1bc3b7e",
            "6731a14c30cf4206a579205411da9578",
            "a46a1e2a62ad47e595014ab5afb9da0e",
            "711f8c8d1d1d459f94fc8fca165acc16",
            "330e2582bb334dc2b2bd75c46778b41b",
            "28ac427c424e47bfa219592929c69e26",
            "74ecf893de194760ba9eef8cff17b54b",
            "5c9098f8cc664428a80af1505447b261",
            "9fa331758cdd4210aeef5c06dc08d7ac",
            "8c9f66d81c944f25a0a39f13a8d186da",
            "e6e7f1d44a5742998c12a53c564a8028",
            "fe20f6bfb04447979a733811008dc023",
            "516c860f160740b6986ba540495c5c79"
          ]
        },
        "id": "LGd3bA67rEX2",
        "outputId": "38c30607-d403-49cc-8447-46aa73648183"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading builder script: 5.94kB [00:00, 3.14MB/s]\n",
            "Downloading extra modules: 4.07kB [00:00, 2.83MB/s]                   \n",
            "Downloading extra modules: 3.34kB [00:00, 3.10MB/s]\n"
          ]
        }
      ],
      "source": [
        "# Load the BLEU metric and the trained BLEURT model for semantic similarity scoring\n",
        "\n",
        "# BLEU (n-gram overlap metric): quick, lexical similarity score between hypothesis and reference(s).\n",
        "#   - Pros: fast, standard.\n",
        "#   - Cons: surface-level; penalizes valid paraphrases; insensitive to meaning.\n",
        "bleu = evaluate.load(\"bleu\")\n",
        "\n",
        "# BLEURT (learned semantic metric): a fine-tuned transformer that scores how well a hypothesis\n",
        "# matches a reference in meaning (trained with human judgments).\n",
        "#   - Pros: captures semantics beyond exact n-grams; better correlation with human eval.\n",
        "#   - Cons: heavier to run; model-specific; requires tokenizer+model.\n",
        "bleurt_tokenizer = AutoTokenizer.from_pretrained(\"Elron/bleurt-base-512\")\n",
        "bleurt_model = AutoModelForSequenceClassification.from_pretrained(\"Elron/bleurt-base-512\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I3WkHhO_2KQJ"
      },
      "outputs": [],
      "source": [
        "def calculate_eval_metrics(text_pairs, model, tokenizer, batch_size, prefix=\"\", **kwargs):\n",
        "    \"\"\"\n",
        "    Generate translations for (source, target) pairs, then compute BLEU and BLEURT.\n",
        "\n",
        "    Args:\n",
        "        text_pairs:  Iterable of (source_text, target_text) tuples.\n",
        "        model:       Seq2seq HF model supporting `.generate(...)` (e.g., T5ForConditionalGeneration).\n",
        "        tokenizer:   Matching tokenizer for the model.\n",
        "        batch_size:  Mini-batch size used for generation and BLEURT scoring.\n",
        "        prefix:      Optional prompt prefix prepended to every source (e.g., \"translate ...: \").\n",
        "        **kwargs:    Passed directly to `model.generate(...)` (e.g., num_beams, max_new_tokens, etc.).\n",
        "\n",
        "    Returns:\n",
        "        translations: List[str] of model outputs aligned with `text_pairs`.\n",
        "\n",
        "    Notes:\n",
        "        - BLEU here is from `evaluate.load(\"bleu\")`. It performs its own tokenization.\n",
        "          If we need strict control, pre-tokenize and pass tokens instead.\n",
        "        - BLEURT is computed in mini-batches for speed. This snippet uses\n",
        "          `max_length=MAX_SEQUENCE_LENGTH` when tokenizing for BLEURT; the\n",
        "          `Elron/bleurt-base-512` model supports up to 512 tokens, so we may\n",
        "          consider increasing this (e.g., 512) if your texts are long.\n",
        "        - For GPU acceleration, move `bleurt_model` and tokenized tensors to CUDA.\n",
        "          (Not done here to keep the function minimal.)\n",
        "    \"\"\"\n",
        "    # Build source (optionally with a task prefix) and gold labels\n",
        "    original_texts = [prefix + pair[0] for pair in text_pairs]\n",
        "    label_texts = [pair[1] for pair in text_pairs]\n",
        "\n",
        "    # Translate original texts using the provided generation kwargs\n",
        "    translations = generate_output(model, tokenizer, original_texts, batch_size, **kwargs)\n",
        "\n",
        "    # ---- BLEU (lexical overlap) ----\n",
        "    bleu_results = bleu.compute(predictions=translations, references=label_texts)\n",
        "    print('BLEU: ', bleu_results)\n",
        "\n",
        "    # ---- BLEURT (semantic similarity) ----\n",
        "    bleurt_scores = []\n",
        "    for i in range(int(len(translations) / batch_size) + 1):\n",
        "        start_i, end_i = i * batch_size, (i + 1) * batch_size\n",
        "        if start_i >= len(translations):\n",
        "            break\n",
        "\n",
        "        with torch.no_grad():\n",
        "            scores = bleurt_model(\n",
        "                **bleurt_tokenizer(\n",
        "                    label_texts[start_i:end_i],          # references\n",
        "                    translations[start_i:end_i],         # hypotheses\n",
        "                    truncation=True,\n",
        "                    max_length=MAX_SEQUENCE_LENGTH,      # BLEURT-base supports up to 512; adjust if needed\n",
        "                    padding='max_length',\n",
        "                    return_tensors='pt'\n",
        "                )\n",
        "            )[0].squeeze().numpy()\n",
        "\n",
        "            # If the last batch has a single item, .squeeze() yields a scalar.\n",
        "            if scores.shape:\n",
        "                bleurt_scores.extend(scores)\n",
        "            else:\n",
        "                bleurt_scores.append(float(scores))\n",
        "\n",
        "    print('BLEURT: ', np.mean(bleurt_scores))\n",
        "\n",
        "    return translations\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n_s2S5nzHS2R"
      },
      "source": [
        "First, choose some keyword arguments to pass to the generate_output() function. These can be any parameters for the .generate() method (e.g. beam search or top-k or top-p sampling, no_repeat_ngram_size, etc). You will want to try the options listed in Question 1.e below, to be able to answer that question (but some of them can't be used at the same time). More info on each can be found in the [Huggingface documentation on text generation here](https://huggingface.co/docs/transformers/en/main_classes/text_generation).\n",
        "\n",
        "Then run the function to translate the validation set and print out eval metrics. The function returns the translations, so we'll also print out a sample of those to manually inspect. Use what you see to iterate on the .generate() arguments, trying to find the most reasonable .generate() arguments that you can for the model you trained.\n",
        "\n",
        "The output will not be great no matter what you do, but you should be able to make it a little more readable, with slightly better BLEU and BLEURT metrics, than the basic options specified in the Lesson 6 notebook.\n",
        "\n",
        "Then answer the questions below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104,
          "referenced_widgets": [
            "2415fb920caf4f5fbfb58214f29bf8f6",
            "0f5e4552ed6a4e9c8ad48397644a8ae3",
            "b6b7c20fc2c149fa91cb1ab29f904eb6",
            "a5882bdb38b1435085e9461088cf15ff",
            "3f5977af6669463587671812af30c43e",
            "b73edebb88a64ffb9d5eba5e41a1deb3",
            "c2de707084654be897292f10d555b705",
            "707b3a81a17a48f5a0d619037a523a8e",
            "cfef8020bbf247589b9cc422d6f35c04",
            "eaec34ff5cf24e8a8e9cb3984f7c680a",
            "b499458984fb46c5b550b4c18fa67523"
          ]
        },
        "id": "iFBbfA4I4Anr",
        "outputId": "8ec3d7e5-d1fd-4117-a432-428290fa9760"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BLEU:  {'bleu': 0.032094072946544074, 'precisions': [0.2832250622305871, 0.06407412240636827, 0.015578739602424925, 0.003752776288580838], 'brevity_penalty': 1.0, 'length_ratio': 1.1775092936802973, 'translation_length': 16471, 'reference_length': 13988}\n",
            "BLEURT:  -1.1488445\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Fill in the decoder .generate() arguments that you want to use, like num_beams or top_p, etc.\n",
        "\"\"\"\n",
        "\n",
        "part1_generate_kwargs = {\n",
        "    # Decode strategy: moderate beams for quality\n",
        "    \"num_beams\": 4,\n",
        "    \"early_stopping\": True,\n",
        "\n",
        "    # Length control: cap new tokens so outputs don’t run on\n",
        "    # (use new-token cap so input length doesn’t affect it)\n",
        "    \"max_new_tokens\": 40,          # ~ encoder cap; tweak 32–64 if needed\n",
        "\n",
        "    # Repetition controls: stop obvious loops/phrases\n",
        "    \"no_repeat_ngram_size\": 3,     # blocks repeated 3-grams, smaller the stricter\n",
        "    \"repetition_penalty\": 1.3,     # soft deterrent; 1.05–1.2 is common\n",
        "\n",
        "    # Beam length bias: mild preference for not-too-short outputs\n",
        "    \"length_penalty\": 1.05,        # 1.0–1.2 typical; >1 favors longer\n",
        "}\n",
        "\n",
        "\n",
        "part1_val_translations = calculate_eval_metrics(\n",
        "    val_pairs,\n",
        "    part1_model,\n",
        "    part1_tokenizer,\n",
        "    part1_batch_size,\n",
        "    **part1_generate_kwargs\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZFwH8PNhdzvC",
        "outputId": "a6ae7d51-22fa-4e80-d4f8-89dfa038e0bd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original:     Lovers and madmen have such seething brains, Such shaping fantasies, that apprehend More than cool reason ever comprehends.\n",
            "Reference:    Lovers and madmen have such busy brains, Such ability to shape fantasies, that they catch More than cool reason ever understands.\n",
            "Translation:  I have to see that, and you have a\n",
            "\n",
            "Original:     Thanks, Rosencrantz and gentle Guildenstern.\n",
            "Reference:    Thanks, Rosencrantz and gentle Guildenstern.\n",
            "Translation:  I’m not, and gentle gentle gentle it’s a good.\n",
            "\n",
            "Original:     Come, let us go.\n",
            "Reference:    Come, let’s go.\n",
            "Translation:  Come, Come on, Come to the Come, let let’s\n",
            "\n",
            "Original:     Drown thyself?\n",
            "Reference:    drown yourself!\n",
            "Translation:  ?\n",
            "\n",
            "Original:     No, you shall paint when you are old.\n",
            "Reference:    No, he means you’ll use makeup when you’re old.\n",
            "Translation:  No, you are so that you are as you are.\n",
            "\n",
            "Original:     What seest thou else In the dark backward and abyss of time?\n",
            "Reference:    What else do You see in the old, dark, bottomless pit of time?\n",
            "Translation:  What do you see thes of a man, and you have to see the\n",
            "\n",
            "Original:     What, dost thou scorn me for my gentle counsel, And soothe the devil that I warn thee from?\n",
            "Reference:    What, you ignore my kind advice and try to please the devil I’m warning you about?\n",
            "Translation:  What, you are me that I have to my father, And I’ll be the so that I am it?\n",
            "\n",
            "Original:     Come, let us go.\n",
            "Reference:    Come, let’s go.\n",
            "Translation:  Come, Come on, Come to the Come, let let’s\n",
            "\n",
            "Original:     My lord, fair Helen told me of their stealth, Of this their purpose hither to this wood; And I in fury hither followed them, Fair Helena in fancy following me.\n",
            "Reference:    My lord, fair Helena told me of their running away, And why they ran here to this forest; And in fury, I followed them here, Fair Helena, in love, following me.\n",
            "Translation:  My lord, I in this in this, And in this is in this; And I have to me in this of this in the\n",
            "\n",
            "Original:     What ish my nation?\n",
            "Reference:    What is my nation?\n",
            "Translation:  What, my a Tell my\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Print out a sample of outputs to manually review\n",
        "for i in range(10):\n",
        "    sample_i = random.choice(range(len(part1_val_translations)))\n",
        "    print('Original:    ', val_pairs[sample_i][0])\n",
        "    print('Reference:   ', val_pairs[sample_i][1])\n",
        "    print('Translation: ', part1_val_translations[sample_i])\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qJtxjFFYeUIq"
      },
      "source": [
        "**QUESTION:**\n",
        "\n",
        " 1.d What seems to be particularly bad about the part1 model's translations? (Choose one of the following options that you agree with most and put it in the answers file.)\n",
        "\n",
        " - A. The model keeps repeating the same common words or phrases over and over, which don't produce very meaningful statements.\n",
        "\n",
        " - B. The model is generating pretty good modern English, but it's quite offensive.\n",
        "\n",
        " - C. The model's output has mostly the same meaning as the input, but with minor grammatical mistakes.\n",
        "\n",
        " - D. The model is making up elaborate narrative details that don't appear in the original text.\n",
        "\n",
        " -> A\n",
        "\n",
        "**QUESTION:**\n",
        "\n",
        " 1.e Which .generate() parameter seemed to help the most in addressing the main shortcoming(s) that you noticed in the part1 model's output? (Choose one of the following options and put it in the answers file.)\n",
        "\n",
        " - A. num_beams\n",
        " - B. do_sample\n",
        " - C. top_k\n",
        " - D. top_p\n",
        " - E. temperature\n",
        " - F. no_repeat_ngram_size\n",
        "\n",
        " -> F"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YS7KfyCVeK-O"
      },
      "source": [
        "### 1.4 Test Set Evaluation Metrics\n",
        "\n",
        "Once you've settled on training hyperparameters that produce good validation loss, and generation options that produce the best output you can so far, go ahead and calculate evaluation metrics on the test set, to warp up this from-scratch model.\n",
        "\n",
        "Then answer the questions below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b-yWnDm_Cphf",
        "outputId": "00ab58cc-c537-48ce-fe79-65db2ae4e758"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BLEU:  {'bleu': 0.03598463879191276, 'precisions': [0.2962437550114106, 0.06975046456065835, 0.018530489118724413, 0.004379105411323116], 'brevity_penalty': 1.0, 'length_ratio': 1.1763041427845897, 'translation_length': 16213, 'reference_length': 13783}\n",
            "BLEURT:  -1.1321745\n"
          ]
        }
      ],
      "source": [
        "# Print out eval metrics for the part1_model on the TEST split.\n",
        "# - Reuses the same generation kwargs you tuned for validation (part1_generate_kwargs).\n",
        "# - Returns the list of test translations so you can inspect or save them later.\n",
        "part1_test_translations = calculate_eval_metrics(\n",
        "    test_pairs,          # [(source, target), ...] for the test set\n",
        "    part1_model,         # trained T5-style model\n",
        "    part1_tokenizer,     # matching tokenizer\n",
        "    part1_batch_size,    # batch size for generation/metric computation\n",
        "    **part1_generate_kwargs  # e.g., num_beams, max_new_tokens, no_repeat_ngram_size, etc.\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sn8KX1eOAiRa"
      },
      "source": [
        "**QUESTION:**\n",
        "\n",
        " 1.f What is the overall BLEU score that you achieved on the test set for the part1 model? (Copy and paste the decimal value for the overall BLEU score, to 5 significant digits, e.g. a number like 0.03671 or 0.09763. Put the answer in the answers file; it should match the value shown in your output in this notebook.)\n",
        "  - 0.035985\n",
        "\n",
        "**QUESTION:**\n",
        "\n",
        " 1.g What is the mean BLEURT score that you achieved on the test set for the part1 model? (Copy and paste the decimal value for the mean BLEURT score, to 5 significant digits, e.g. a number like -1.12345 or -0.54321. Put the answer in the answers file; it should match the value shown in your output in this notebook.)\n",
        "  - -1.13217"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "doyqgjOWzPy4"
      },
      "source": [
        "## 2. Small Pre-Trained T5 Model\n",
        "\n",
        "What if we use a model that has already been pre-trained to recognize English (at least modern English), even if it hasn't yet been trained for our particular translation task?\n",
        "\n",
        "We'll use a T5 small model, which should be able to generate good modern English, but we'll need to train it to encode and translate Shakespearean text.\n",
        "\n",
        "### 2.1 Pre-trained Model Setup and Tokenization\n",
        "\n",
        "The next two cells load the pre-trained model, and preprocess the data with the pre-trained tokenizer. Fill in the necessary code for each of these cells.\n",
        "\n",
        "For preprocessing, you'll need to map the `preprocess_translation_batch` function that we created earlier to the `train_dataset` and `val_dataset`. Use the code from part 1 as an example, but now pass in the pretrained T5 tokenizer as a function keyward argument (kwarg). Also pass in the given task_prefix as the \"prefix\" kwarg for the preprocessing function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 246,
          "referenced_widgets": [
            "e26edb9dd7c149958b94f45bf15d63b3",
            "513e2f9a46c14e47b9fb2b4044e85253",
            "c22e77382276484ca5ac1de4af9988ee",
            "32736a5e83eb4cb69a941c24b61ae7c3",
            "ba7ec22d57744e65bff6baed1c375362",
            "ce95e806d2bb41b99cf5a7b58153ed77",
            "3646bf95470c42c3bb9cc548b3878105",
            "2ad063bb2ee5457aa9034873b342bab8",
            "f69aca61830c4ad1be3f4e1b95a28dab",
            "068ed2d85c5349cf8c8d79d3a76674a6",
            "2aeee3839ece4eb089fa3229d7c437f2",
            "b252676ec7da49edad442d3385fbadc3",
            "f119c14a90c2435691dcfd49bed0f3c2",
            "56f45fb034ca4cf3820fba1b0eea1368",
            "ec531abd9dbd413cba54171cb3ed0a2a",
            "4190f8a565d04a4880a0dadd1352df8e",
            "0b98ec893f974838b88d795f62607fe8",
            "151c839d14324eacad9f804576f93062",
            "1f62bbd5a4174c989a6fad259ec66746",
            "a9a1512d4ee543188c1db09651b78706",
            "fd1a8d509a0e4649996591248dd65d84",
            "c47ffee438f2468bb38435cfe01485d2",
            "6627dfbcfd8e4599aa54124c551a441f",
            "1f4918324ee0456ab851fd1803abcbaa",
            "55f9a4348ac149c89faeea3062b70967",
            "1c46c3b5a7e8456bace2664a127863d7",
            "67f0c9a7caf443958a162f63ab86bc2e",
            "51b1e9eb5b2a42c594c4de4aa1509963",
            "c6c17e215bbb40568d76450ffff884ff",
            "b76fcf29e6634b9da6722fe0aca0fc96",
            "c82c1cf09a434f6797095ba23c34c8cf",
            "53069e6e502044fe8402a44be7c8bf3b",
            "e46d8352511d427fbb3b616df0f0bc7c",
            "78662004517642b093ac17bba65bb3d6",
            "6bbcac1c971149c88024e247920b8591",
            "c159c20af5c245c1a0ec05ccdecf6272",
            "f266551c6c0c4a7686e8d913d1e4e3db",
            "6b3ed4d4dc8a4b3fb3d96026ee7c9944",
            "676a47fd8f634befa295e013a4b0768c",
            "d663a01b17884c6d8e003421685e46b8",
            "a25edebfc0a24d60b826c315b6b7acf0",
            "13993688520d4643af2624cd91f5c4ab",
            "50274bbd5bba465097c566f043eab492",
            "37516db1624b4f2591340bc3c16e83e7",
            "b261abff72384f01bae2d3025212ec3b",
            "6e5aa70d9a5e4cfb83b14df87ab7d8f5",
            "61748c41c3a647b4abb57f0b2008c6b2",
            "e1e7930b2ef54118a75c4763e81701f2",
            "4ae207816c3c4eb79725b74234e3c6fa",
            "f453040932ab4a39b3f42375d5af8d1b",
            "46e0a1fd321e424f8b6c9cbb94cc38a3",
            "2f446f949efa4318931fa3c720aebb90",
            "80781a1cffd84e49b5dfa00790b716ea",
            "bd40eb589e3944a8bcefac170d27c026",
            "0b23770576a847baba3df6fb2cfc1d96",
            "01b38a7aa38a43a4ab3ba4edb63074b8",
            "5d3a1b97de064507a2417c184de8851a",
            "4fcedac3ecd24a4abcfeb16ca183d4d6",
            "ad594330de2447aaa0075c8d5d599d78",
            "816a5c07af494b6ea71cc16bf6a4049d",
            "20fe6087b1b644b8ae964ae4e1b12fa7",
            "929133839b2c4afc880f291e6c2302fe",
            "6500bb076b73420cb81b43f776c16052",
            "d954640a9b18431ead6e4405869e99bc",
            "7e3f40a8ccc54e798807276a33430b06",
            "1e86658c98254d6aacb4a49dcda8c5e8"
          ]
        },
        "id": "6GcSwKbx5s9k",
        "outputId": "88ac4bc9-d905-46a7-d996-f8f317594d0d"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Load the pre-trained model and tokenizer\n",
        "\"\"\"\n",
        "\n",
        "t5_pretrained_checkpoint_name = 'google-t5/t5-small'\n",
        "\n",
        "### YOUR CODE HERE\n",
        "\n",
        "# Pretrained T5 tokenizer (SentencePiece). Handles special tokens like <pad>, </s>.\n",
        "part2_tokenizer = T5TokenizerFast.from_pretrained(t5_pretrained_checkpoint_name)\n",
        "\n",
        "# Pretrained seq2seq model weights (encoder–decoder). Good modern-English prior.\n",
        "part2_model = T5ForConditionalGeneration.from_pretrained(t5_pretrained_checkpoint_name)\n",
        "\n",
        "# T5 convention: decoder starts with the pad token. (Usually set in config already; this ensures it.)\n",
        "part2_model.config.decoder_start_token_id = part2_tokenizer.pad_token_id\n",
        "\n",
        "### END YOUR CODE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81,
          "referenced_widgets": [
            "820838fe5b00400f9e6c46e464c59df4",
            "916d1c274428452e976436ceebe90d8b",
            "6e156ffe3b634e0497be32cf3a083e9c",
            "97a388154bf04c02a5197e7e0b53af89",
            "3663c50390ee478a8750c8d2e5e8e469",
            "e1d4d7e1625d4503b7e7dbe4c8f73b21",
            "1dc52e19c13644b4aeb671b62de300fe",
            "5e1f2bcb7b024294a5507ee40a6f24e1",
            "83185a699cbe4f7aa4c9566fe02f5143",
            "ac732c0972ff4da18f99baa3a11c4626",
            "4c2709b4c2434ce58733dcdb29496a53",
            "08c97dee7f9c4779af94fd38c223433e",
            "36502f3b0a064057993339bc4bde5c29",
            "adfe727c86e44e76a4c2735875632915",
            "b4827c5f70f04ed8becf086ef4b13197",
            "c6d9b7fdd3b14af188b5cbef388b4c18",
            "5a9eedd8d3364ee5b26b88be70c40c51",
            "34e420e304594e4ea1c88dc998a694c5",
            "bbbef86debd64f3c891a2e75f9731966",
            "abcd29792ce34b54a756272db7ba9082",
            "4efbdd01652e4a04b10b4400b7fd3e32",
            "01d64dafe68d4459815ff57ecb755d65"
          ]
        },
        "id": "e3W6E_j3-tV8",
        "outputId": "a20029ea-e0fb-4a5b-cd92-171935dd2ee8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Map: 100%|██████████| 16798/16798 [00:02<00:00, 6364.99 examples/s]\n",
            "Map: 100%|██████████| 1145/1145 [00:00<00:00, 8807.39 examples/s]\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Preprocess the datasets using the pretrained tokenizer, and the given task_prefix.\n",
        "Use the task_prefix as the \"prefix\" argument to the function preprocess_translation_batch().\n",
        "\"\"\"\n",
        "\n",
        "task_prefix = 'Translate Shakespeare to Modern English: '\n",
        "\n",
        "train_ds_part2 = train_dataset.map(\n",
        "    preprocess_translation_batch,\n",
        "    batched=True,\n",
        "    fn_kwargs={\n",
        "        # Use the pretrained tokenizer and prepend the task prefix to each source\n",
        "        \"tokenizer\": part2_tokenizer,\n",
        "        \"prefix\": task_prefix,\n",
        "    }\n",
        ")\n",
        "\n",
        "val_ds_part2 = val_dataset.map(\n",
        "    preprocess_translation_batch,\n",
        "    batched=True,\n",
        "    fn_kwargs={\n",
        "        # Same preprocessing for validation to ensure consistency\n",
        "        \"tokenizer\": part2_tokenizer,\n",
        "        \"prefix\": task_prefix,\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EP4FNVdVTO3V"
      },
      "source": [
        "### 2.2 Fine-Tuning the Pre-Trained Model\n",
        "\n",
        "Now create the training args and trainer to fine-tune this pre-trained model. We've given you part of the code: you'll use the same functions as above for `create_seq2seq_training_args` and `create_seq2seq_trainer`. Fill in the rest of the arguments that you need for this version of the model. Use the provided batch size and num_epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "7LyTR1ha-tYy"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Create the training args and trainer for the pre-trained model.\n",
        "Use the batch size and num_epochs provided below for this model.\n",
        "\"\"\"\n",
        "\n",
        "part2_batch_size = 32\n",
        "part2_num_epochs = 4\n",
        "\n",
        "# Use the helper to build Seq2SeqTrainingArguments (same fixed fields as Part 1)\n",
        "part2_training_args = create_seq2seq_training_args(\n",
        "    part2_batch_size,\n",
        "    part2_num_epochs\n",
        ")\n",
        "\n",
        "# Wire up the pretrained model, args, and the preprocessed datasets\n",
        "part2_trainer = create_seq2seq_trainer(\n",
        "    part2_model,\n",
        "    part2_training_args,\n",
        "    train_ds_part2,\n",
        "    val_ds_part2\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6u14PQfNAjdV"
      },
      "source": [
        "Run the cell below to fine-tune the part2 model, then answer the following questions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 236
        },
        "id": "jvffobrggOAG",
        "outputId": "e396360f-3226-49d4-f8bb-862055b38bf3"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2100' max='2100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2100/2100 03:16, Epoch 4/4]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.968400</td>\n",
              "      <td>0.711930</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.743700</td>\n",
              "      <td>0.693941</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.727800</td>\n",
              "      <td>0.687807</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.714000</td>\n",
              "      <td>0.684969</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=2100, training_loss=0.7855644952683222, metrics={'train_runtime': 196.8031, 'train_samples_per_second': 341.417, 'train_steps_per_second': 10.671, 'total_flos': 710459869102080.0, 'train_loss': 0.7855644952683222, 'epoch': 4.0})"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "part2_trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K2TswETsAoQj"
      },
      "source": [
        "**QUESTION:**\n",
        "\n",
        " 2.a What is the final validation loss that you were able to achieve for the part2 model after training for 4 epochs? (Copy and paste the decimal value for the final validation loss, to 5 significant digits, e.g. a number like 0.56781 or 0.87632. Put the answer in the answers file; it should match the value shown in your output in this notebook.)\n",
        "  - 0.68497"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GtDbBBE4w_gx"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Before moving on, save a checkpoint of the model you just trained in your Drive,\n",
        "So that you can pick up where you left off later if needed\n",
        "\"\"\"\n",
        "\n",
        "# Modify this path to the location in your Drive where you want to save the part2 model\n",
        "# part2_model_checkpoint_filepath = 'drive/MyDrive/ISchool/MIDS/266/model_checkpoints/part2_model'\n",
        "part2_model_checkpoint_filepath = 'model_checkpoints/part2_model'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "ouQMAZoww_t1"
      },
      "outputs": [],
      "source": [
        "# Run this line only after you've fine-tuned the part2_model\n",
        "part2_model.save_pretrained(part2_model_checkpoint_filepath, from_pt=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "NGN1544jw_yZ"
      },
      "outputs": [],
      "source": [
        "# Run this line only if you need to reload the model you fine-tuned earlier\n",
        "part2_model = T5ForConditionalGeneration.from_pretrained(part2_model_checkpoint_filepath)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gyv34a6Igjbf"
      },
      "source": [
        "### 2.3 Fine-Tuned Model Evaluation\n",
        "\n",
        "Now use the calculate_eval_metrics() function defined above to translate the test set and calculate evaluation metrics. Also print out a sample of the translated outputs. For now, use the same decoder .generate() kwargs that you chose for part1.\n",
        "\n",
        "Then answer the questions below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0C00JmnS_EEr",
        "outputId": "56e03700-c047-4472-f3eb-5729c6e8a9dc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BLEU:  {'bleu': 0.3499995259354115, 'precisions': [0.6313748881598569, 0.41110295915871853, 0.2961431268542659, 0.218063872255489], 'brevity_penalty': 0.972717288631285, 'length_ratio': 0.973082783138649, 'translation_length': 13412, 'reference_length': 13783}\n",
            "BLEURT:  -0.011276488\n"
          ]
        }
      ],
      "source": [
        "# Print out eval metrics for the part2_model on the test set\n",
        "\n",
        "part2_test_translations = calculate_eval_metrics(\n",
        "    test_pairs,\n",
        "    part2_model,\n",
        "    part2_tokenizer,\n",
        "    part2_batch_size,\n",
        "    task_prefix,\n",
        "    **part1_generate_kwargs\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n47QLl_KshfG",
        "outputId": "fe52e825-91a4-4193-ee3a-029055feceea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original:     There the grown serpent lies; the worm that's fled Hath nature that in time will venom breed, No teeth for the present.\n",
            "Reference:    There the grown serpent lies; the worm that has fled Has a nature that in time will breed venom, But he has no fangs now.\n",
            "Translation:  There the grown serpent lies; the worm that's fled Hath nature that in time will venom breed, No teeth for the present.\n",
            "\n",
            "Original:     Signior, it is the Moor.\n",
            "Reference:    Signior, it is the Moor.\n",
            "Translation:  Signior, it is the Moor.\n",
            "\n",
            "Original:     She died, my lord, but whiles her slander lived.\n",
            "Reference:    She was only dead, my lord, as long as her slander lived.\n",
            "Translation:  She died, my lord, but while her slander lived.\n",
            "\n",
            "Original:     Thou worms' meat in respect of a good piece of flesh, indeed.\n",
            "Reference:    You are about as much of a thinker as worm’s meat is a nice steak.\n",
            "Translation:  You worms' meat in respect of a good piece of flesh, indeed.\n",
            "\n",
            "Original:     A very fine one.\n",
            "Reference:    He’s a stylish man.\n",
            "Translation:  A very fine one.\n",
            "\n",
            "Original:     Retire!\n",
            "Reference:    Retreat!\n",
            "Translation:  Retire!\n",
            "\n",
            "Original:     Therefore my son i'th’ ooze is bedded, and I'll seek him deeper than e'er plummet sounded, And with him there lie mudded.\n",
            "Reference:    Therefore my son is embedded in the ooze; and I'll seek him deeper than a piece of lead ever measured How deep the water is, and I will lie there with him in the mud.\n",
            "Translation:  Therefore my son’s ooze is bedded, and I’ll seek him deeper than e'er plummet sounded, And with him there lie mudded.\n",
            "\n",
            "Original:     I can tell your Majesty, the duke is a prave man.\n",
            "Reference:    I can tell your Majesty, the duke is a brave man.\n",
            "Translation:  I can tell your Majesty, the duke is a prave man.\n",
            "\n",
            "Original:     Besides, it should appear, that if he had The present money to discharge the Jew, He would not take it.\n",
            "Reference:    Besides, it seems that, even if he had The money right now to pay the Jew, He wouldn’t take it.\n",
            "Translation:  Besides, it should appear that if he had The present money to discharge the Jew, He would not take it.\n",
            "\n",
            "Original:     Look where he goes even now out at the portal!\n",
            "Reference:    Look, where he goes, even now out the door!\n",
            "Translation:  Look where he goes even now at the portal!\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Print out a sample of the translated outputs to look at as well\n",
        "\n",
        "for i in range(10):\n",
        "    sample_i = random.choice(range(len(part2_test_translations)))\n",
        "    print('Original:    ', test_pairs[sample_i][0])\n",
        "    print('Reference:   ', test_pairs[sample_i][1])\n",
        "    print('Translation: ', part2_test_translations[sample_i])\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_NZ8u1NaAFcf"
      },
      "source": [
        "**QUESTION:**\n",
        "\n",
        " 2.b What is the overall BLEU score that you achieved on the test set for the part2 model? (Copy and paste the decimal value for the overall BLEU score, to 5 significant digits, e.g. a number like 0.03671 or 0.09763. Put the answer in the answers file; it should match the value shown in your output in this notebook.)\n",
        "  - 0.35000\n",
        "\n",
        "**QUESTION:**\n",
        "\n",
        " 2.c What is the mean BLEURT score that you achieved on the test set for the part2 model? (Copy and paste the decimal value for the mean BLEURT score, to 5 significant digits, e.g. a number like -1.12345 or -0.54321. Put the answer in the answers file; it should match the value shown in your output in this notebook.)\n",
        " - -0.011276\n",
        "\n",
        "\n",
        "**QUESTION:**\n",
        "\n",
        " 2.d What do you notice about the part2 model's output? It should be much better than the part1 model's output. But the translations still probably don't perfectly match the reference human translations. What does the part2 model seem to still be doing poorly? (Chose one of the following options that you agree with most, and put it in the answers file.)\n",
        "\n",
        " - A. The generated translations are gibberish.\n",
        "\n",
        " - B. The generated translations are written in a far more casual style than the reference human translations.\n",
        "\n",
        " - C. The generated translations mean something completely different from the input text and reference translations.\n",
        "\n",
        " - D. The generated translations are too similar to the input text, and haven't been rephrased as much as the reference human translations.\n",
        "\n",
        "  -> D"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PfJCnESMdAR5"
      },
      "source": [
        "### 2.4 Style Classifier\n",
        "\n",
        "Now that the model is able to output more coherent translations, we can start to get more picky about different aspects of the output. We should also make sure that our evaluation metrics are capturing everything we want to be able to assess and improve in the model's output.\n",
        "\n",
        "One thing we're not capturing yet is if the output has the right **style**. This task is sort of a translation task, but since it's between two different forms of English, we can also think of it as a style transfer task.\n",
        "\n",
        "BLEU might help a little with that, but when the model chooses different words from the human reference, it could do so in ways that are still good modern English or that are still too much like Shakespeare. BLEURT won't tell us anything about the style, as long as the meaning is still similar to the reference.\n",
        "\n",
        "How can we tell whether the output has the right style? We could train a separate classification model to predict whether text is Shakespearean or modern English. We have the data to do it! We just need to repurpose our data for a classification problem.\n",
        "\n",
        "Use the code below to train a BERT classifier to predict whether a sentence is Shakespearean or modern English. We're providing this code for you, because it's not the main task and not based on a similar example from class. We want you to use it as one of your evaluation metrics, to help you iterate on your models for the main task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xDQEZITtdAe6"
      },
      "outputs": [],
      "source": [
        "def make_style_classifier_data(text_pairs):\n",
        "    \"\"\"\n",
        "    Build a binary **style** dataset from parallel pairs for a classifier.\n",
        "\n",
        "    Args:\n",
        "        text_pairs: Iterable of (shakespeare_text, modern_text) tuples.\n",
        "\n",
        "    Returns:\n",
        "        datasets.Dataset\n",
        "            Shuffled dataset with two columns:\n",
        "              - \"text\":  strings (all Shakespeare examples followed by all Modern)\n",
        "              - \"label\": ints    (0 = Shakespeare, 1 = Modern)\n",
        "\n",
        "    Notes:\n",
        "        - Class labeling: source (old/Shakespeare) → 0, target (modern) → 1.\n",
        "        - Balance: for each pair you add one example to each class, so the split\n",
        "          is naturally balanced (given balanced input pairs).\n",
        "        - Shuffle: randomize order to avoid batches that are all one class first.\n",
        "          (For reproducibility, you can pass a seed: `.shuffle(seed=42)`.)\n",
        "    \"\"\"\n",
        "    # Concatenate texts from both domains into a single list\n",
        "    style_texts  = [pair[0] for pair in text_pairs] + [pair[1] for pair in text_pairs]\n",
        "\n",
        "    # Parallel labels: 0's for every Shakespeare sample, 1's for every Modern sample\n",
        "    style_labels = [0 for _ in text_pairs]          + [1 for _ in text_pairs]\n",
        "\n",
        "    # Construct a Hugging Face Dataset with explicit columns\n",
        "    style_dataset = Dataset.from_dict({\"text\": style_texts, \"label\": style_labels})\n",
        "\n",
        "    # Shuffle to interleave classes and break up ordering artifacts\n",
        "    return style_dataset.shuffle()\n",
        "\n",
        "# Build train/validation datasets for the style classifier\n",
        "style_train_ds = make_style_classifier_data(train_pairs)\n",
        "style_valid_ds = make_style_classifier_data(val_pairs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232,
          "referenced_widgets": [
            "f7040b9ee08843b9bc9a94500f650884",
            "71ba3efbb4f54fb1b89d7b7f046a6c48",
            "044f25f025e041e0abe1bce790c68ba9",
            "50a462462a684f32b29f72d092221f4f",
            "142a6a0a796846559aa8690b4c0e6fff",
            "e75d164d54fe4c96a3d2affe02d455dc",
            "54a26691afb44188803755d3719e8cfc",
            "453eda2625ed49afb198a18017643a15",
            "e927a2fc936649c79a26f3d324e70c91",
            "915f3d44e77c45398290be81e4f0c45d",
            "b086c35be0924d19a0dcc1cd7866f741",
            "55b88691a2bf4908b9124a1fba57a29f",
            "57b60333fcbf49608693878b1ed1aa70",
            "fd64d5000761476f991af9f07726c717",
            "2fe283f018d942e8bb50d1be109dfba5",
            "4fec74de31654058b2723e02eaf1ecc8",
            "124d78f2d7874532b0b00db4b996cc21",
            "8fa13e2fbae74d549e8d940ef1fc3377",
            "973d343469ae4fe1b18b064256c5c3dc",
            "bec20161b091400f96deff2df01482f8",
            "ebc08f4d99d14c70b4067aa85936b0a6",
            "03116c0a8ba44dd5abbc20767794c979",
            "0595fb7c0daf498c8e16658b9c0d3375",
            "454761b444e44b23b80bf13b24ad4583",
            "eb1851c469ff4eceab33c883307b7e85",
            "b3f62be8985e4bc888b70a94329344d0",
            "4c527cc642f84e0aa65d168f9ad4d590",
            "1fcbe2fedd664a12a46a9d5d4d51c6d4",
            "d0fb822da20c477fb5a21302d36a41ec",
            "ac9ce5d2406b4fb385c5321b56af05f9",
            "faad4baca20e4801bad13930dded20d2",
            "5b651696a67c4c6ea7d463880e2be872",
            "7514ab2975a64f208dc402b62c44086d",
            "98d95b97e5de45e9965e0905a956696e",
            "f6aa7d53586d425392c677dbd3c9dc09",
            "9726a9d88c00490e8c3557f696a77911",
            "5c94afecd9654f3bba3803d883411d86",
            "81c4f4647eb643e683418bcbc663d8e6",
            "8acab388e0144fcba54d808140541acf",
            "5330f7fa7bc04905ba3b73d3e1cf912e",
            "cd2ba1a92f43418c867d95a4b0697831",
            "9605ac80ccda455e98e1e4935a6c5d46",
            "bc162d36963046a8a818155a19e3bc0d",
            "23f35bf1ea964e77a975fd78614a83d6",
            "e3d77cde698244e6bb1b1423f0042018",
            "9f8f417a3d954a1592ba2fe0d01bcaca",
            "5d896e6f7763459fa6f9218b061d7734",
            "0f2fd4ca46e246beaaed4c3ade0e9159",
            "3be30224778346c0a5e697991cf78c5d",
            "53950f8405e0407faf1f9883e3bed3ce",
            "fe4b309929664023bc3fa3ac71e002f6",
            "81780971984b425c9aff95eb0e0873d4",
            "78d103a342974894a715aacb79af3075",
            "43011207c85f4e63b00093ae879e275b",
            "046ffd7e261c44ecadd70031d7e26178"
          ]
        },
        "id": "xpKXPqTZpj_a",
        "outputId": "0c9ca0c5-1960-4f38-945e-7867de6f720b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "bert_checkpoint_name = 'bert-base-cased'\n",
        "bert_tokenizer = BertTokenizer.from_pretrained(bert_checkpoint_name)\n",
        "bert_style_classifier_model = BertForSequenceClassification.from_pretrained(bert_checkpoint_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81,
          "referenced_widgets": [
            "51a50a755a8945d19768e0fc8f55440d",
            "e2fc47b53ac94159adb941696d9b94cf",
            "fe285bd0afec480bae695d0210c26607",
            "a4934337cf3046ec9db36fe860760899",
            "fab8e9e34e594c61b95b81e65ed67526",
            "d0c0480f28b0452a9686e679182d2e4e",
            "5974eea5bac44322b11becbb1b9a95ec",
            "cda072011d8c48cab504cf3411875dbd",
            "822b914104e4429a8bfb74cb54214824",
            "fb82c5db4dce43a78107109733efbcfc",
            "587ac31b82284245840d870ed5f81fd9",
            "281280c76783487f9ff59182bde4c16f",
            "9ad23674fd3a427ea447cb3c673ab825",
            "0914c022aeaf4bfe92b3b58a8e922c98",
            "5b0da65077cd4f9ab272eb91855eb279",
            "2a677249ac2d4790b8f54db3186e7894",
            "8a66c1214e44400eba4ab6fd1fb265ee",
            "8c717472d4b9405e879fc223c8d03c95",
            "6c648fd176b74d758de0d3933a7be0b5",
            "3c5d5f7971374d668fb3226ca49182dc",
            "22876480caf64837841817224c348ec1",
            "1be076231b144c6f88096e41fd56b68a"
          ]
        },
        "id": "HtCdHkOoplDr",
        "outputId": "28add6ef-13ef-40aa-b8eb-678e186d788a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Map: 100%|██████████| 33596/33596 [00:09<00:00, 3638.16 examples/s]\n",
            "Map: 100%|██████████| 2290/2290 [00:00<00:00, 4514.48 examples/s]\n"
          ]
        }
      ],
      "source": [
        "def preprocess_style_text(data):\n",
        "    \"\"\"\n",
        "    Tokenize a batch of raw texts for the style classifier (e.g., BERT).\n",
        "\n",
        "    Args:\n",
        "        data: A batch dict from HF Datasets with key \"text\" (list[str]).\n",
        "\n",
        "    Returns:\n",
        "        Dict of batched tensors suitable for modeling:\n",
        "          - input_ids: (batch, MAX_SEQUENCE_LENGTH)\n",
        "          - attention_mask: (batch, MAX_SEQUENCE_LENGTH)\n",
        "          - token_type_ids: (batch, MAX_SEQUENCE_LENGTH)  # BERT next-sentence segments (all zeros for single-sentence)\n",
        "    Notes:\n",
        "        - MAX_SEQUENCE_LENGTH caps the sequence; longer texts are truncated.\n",
        "        - Using `padding='max_length'` yields fixed-size batches (good for static shapes).\n",
        "        - `return_tensors=\"pt\"` returns PyTorch tensors directly to the dataset mapping.\n",
        "          (You can also return lists/np arrays and later call `set_format(\"torch\")`.)\n",
        "        - Some models (RoBERTa, DistilBERT) ignore `token_type_ids`; harmless to include.\n",
        "    \"\"\"\n",
        "    return bert_tokenizer.batch_encode_plus(\n",
        "        data['text'],\n",
        "        max_length=MAX_SEQUENCE_LENGTH,   # hard cap; consider 128–256 for sentence-level style, we hardcode 40 here\n",
        "        padding='max_length',             # pad all sequences to the same length\n",
        "        truncation=True,                  # cut off texts longer than the cap\n",
        "        return_attention_mask=True,       # mask distinguishes real tokens vs padding\n",
        "        return_token_type_ids=True,       # BERT uses this; others may ignore\n",
        "        return_tensors=\"pt\"               # return PyTorch tensors\n",
        "    )\n",
        "\n",
        "# Apply batched tokenization to train/val splits\n",
        "style_train_ds_preprocessed = style_train_ds.map(preprocess_style_text, batched=True)\n",
        "style_valid_ds_preprocessed = style_valid_ds.map(preprocess_style_text, batched=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "OpatMrZRdAm_"
      },
      "outputs": [],
      "source": [
        "style_classifier_batch_size = 32\n",
        "style_classifier_num_epochs = 2\n",
        "\n",
        "# TrainingArguments for the BERT style classifier.\n",
        "# - output_dir: where checkpoints/logs are written\n",
        "# - per_device_*_batch_size: batch size per GPU/CPU device (effective batch = this × #devices)\n",
        "# - num_train_epochs: total passes over the training data\n",
        "# - eval_strategy: run evaluation at the end of each epoch\n",
        "# - save_strategy: save a checkpoint at the end of each epoch\n",
        "# - report_to: disable external loggers (e.g., \"wandb\", \"tensorboard\", etc.)\n",
        "style_training_args = TrainingArguments(\n",
        "    output_dir=\"bert_shakespeare_style_classifier\",\n",
        "    per_device_train_batch_size=style_classifier_batch_size,\n",
        "    per_device_eval_batch_size=style_classifier_batch_size,\n",
        "    num_train_epochs=style_classifier_num_epochs,\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    report_to='none'\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "1be619ec5e424a1392e0432ab9e0819a",
            "fa70bb9e65d34146ae12876c23ce25bd",
            "831cb49780c64a7f9f5add6eda50899c",
            "6b782a9cf2ce4b4c9f686c0fc919ec12",
            "7e57f64929624e8ca0c90d4f3f657781",
            "67455f4182264bf6925719c34a2e3001",
            "32ab0e1d92564c5eb0e97933362950df",
            "7b4f53ebae0f46dd849399d1c72ce7e0",
            "da584f577a224a729468ae37a0e2ac98",
            "fe2dc20d2bae44858f4a40725068ffb3",
            "9a0ae15b950a41e394c5d41b6a8f57a9"
          ]
        },
        "id": "pCblAKhodAqq",
        "outputId": "0015ced0-8ea8-4119-cd44-8f71cb6ef533"
      },
      "outputs": [],
      "source": [
        "# Accuracy will be our primary evaluation metric for the style classifier.\n",
        "metric = evaluate.load(\"accuracy\")\n",
        "\n",
        "def compute_metrics(p):\n",
        "    \"\"\"\n",
        "    Convert model outputs to class predictions and compute accuracy.\n",
        "\n",
        "    Args:\n",
        "        p: Tuple or EvalPrediction from HF Trainer.\n",
        "           - p.predictions: logits array of shape (batch, num_labels)\n",
        "           - p.label_ids (or labels): true label ids of shape (batch,)\n",
        "\n",
        "    Returns:\n",
        "        dict: {\"accuracy\": <float>} suitable for Trainer logging/saving.\n",
        "    \"\"\"\n",
        "    predictions, labels = p  # Trainer passes (predictions, label_ids)\n",
        "    # Turn logits into hard class ids by argmax over the label dimension.\n",
        "    predictions = np.argmax(predictions, axis=1)\n",
        "    # Compute accuracy against the provided references.\n",
        "    return metric.compute(predictions=predictions, references=labels)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "p5wxavbmdAuP"
      },
      "outputs": [],
      "source": [
        "# Hugging Face Trainer wiring for the BERT style classifier.\n",
        "# - model:          the sequence classification model to fine-tune (2 labels: Shakespeare vs Modern)\n",
        "# - args:           TrainingArguments controlling batch sizes, epochs, eval/save cadence, etc.\n",
        "# - train_dataset:  tokenized training split (must include input_ids, attention_mask, [token_type_ids], label)\n",
        "# - eval_dataset:   tokenized validation split with the same columns as train\n",
        "# - compute_metrics:function that takes (predictions, labels) and returns a dict (e.g., {\"accuracy\": ...})\n",
        "style_trainer = Trainer(\n",
        "    model=bert_style_classifier_model,\n",
        "    args=style_training_args,\n",
        "    train_dataset=style_train_ds_preprocessed,\n",
        "    eval_dataset=style_valid_ds_preprocessed,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "id": "1Zvf_fJ9dAyD",
        "outputId": "996fb57a-b9d7-46f6-b753-1cea40de0412"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2100' max='2100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2100/2100 03:36, Epoch 2/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.407900</td>\n",
              "      <td>0.365615</td>\n",
              "      <td>0.821397</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.289500</td>\n",
              "      <td>0.372840</td>\n",
              "      <td>0.834498</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=2100, training_loss=0.36029799415951685, metrics={'train_runtime': 216.7505, 'train_samples_per_second': 309.997, 'train_steps_per_second': 9.689, 'total_flos': 1381168596230400.0, 'train_loss': 0.36029799415951685, 'epoch': 2.0})"
            ]
          },
          "execution_count": 56,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "style_trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "t8huZxEExZnJ"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Before moving on, save a checkpoint of the model you just trained in your Drive,\n",
        "So that you can pick up where you left off later if needed\n",
        "\"\"\"\n",
        "\n",
        "# Modify this path to the location in your Drive where you want to save the style classifier\n",
        "# style_classifier_checkpoint_filepath = 'drive/MyDrive/ISchool/MIDS/266/model_checkpoints/style_classifier'\n",
        "style_classifier_checkpoint_filepath = 'model_checkpoints/style_classifier'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "48X7B6HnxaBA"
      },
      "outputs": [],
      "source": [
        "# Run this line only after you've trained the style classifier model\n",
        "bert_style_classifier_model.save_pretrained(style_classifier_checkpoint_filepath, from_pt=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "NWOcS_chxaDB"
      },
      "outputs": [],
      "source": [
        "# Run this line only if you need to reload the style classifier you trained earlier\n",
        "bert_style_classifier_model = BertForSequenceClassification.from_pretrained(style_classifier_checkpoint_filepath)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WzRAemBTo3Zl"
      },
      "source": [
        "Now let's use the style classifier to classify the output from the Shakespeare translation model, using the test set from our main task. The function reports the average predicted probability of the positive class, which is the modern English style (and which is our goal for our main task model).\n",
        "\n",
        "We should also classify the original Shakespearean text and the human translations from the test set, to compare the scores as references.\n",
        "\n",
        "Run the next two cells of code, then answer the following questions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "QNYSjDnPLsIy"
      },
      "outputs": [],
      "source": [
        "def get_modern_style_score(text):\n",
        "    \"\"\"\n",
        "    Compute the average probability that a batch of texts are in **Modern English** style\n",
        "    using the fine-tuned BERT style classifier.\n",
        "\n",
        "    Args:\n",
        "        text: List[str] (or any iterable of strings). Each string is a sample to score.\n",
        "\n",
        "    Returns:\n",
        "        float: Mean P(modern) across the batch, where class index 1 = Modern.\n",
        "               (If you pass a single string in a length-1 list, this is that sample’s P(modern).)\n",
        "\n",
        "    Notes:\n",
        "        - Tokenization:\n",
        "          * Caps each sequence at MAX_SEQUENCE_LENGTH tokens and pads to that length.\n",
        "          * Returns PyTorch tensors sized (batch, MAX_SEQUENCE_LENGTH).\n",
        "        - Device usage:\n",
        "          * Moves inputs and the model to CUDA for scoring, then brings logits back to CPU.\n",
        "          * This function relocates the model every call; for performance, move the model\n",
        "            to GPU once outside this function and just feed tensors here.\n",
        "        - Probabilities:\n",
        "          * Applies softmax over logits to obtain P(class). Index 1 corresponds to Modern.\n",
        "          * Returns the **mean** probability across all provided texts.\n",
        "        - token_type_ids:\n",
        "          * Included for BERT; models like RoBERTa ignore this field (harmless to provide).\n",
        "    \"\"\"\n",
        "    text_inputs = bert_tokenizer.batch_encode_plus(\n",
        "        text,\n",
        "        max_length=MAX_SEQUENCE_LENGTH,     # truncate long texts to this cap\n",
        "        padding='max_length',               # pad shorter texts to fixed length\n",
        "        truncation=True,\n",
        "        return_attention_mask=True,\n",
        "        return_token_type_ids=True,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "    with torch.no_grad():\n",
        "        logits = bert_style_classifier_model.cuda()(\n",
        "            text_inputs['input_ids'].cuda(),\n",
        "            attention_mask=text_inputs['attention_mask'].cuda()\n",
        "        ).logits\n",
        "\n",
        "    # Convert logits -> probabilities; column 1 is P(Modern)\n",
        "    probs = softmax(logits.cpu().numpy(), axis=1)\n",
        "    return np.mean(probs[:, 1])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Translate Shakespeare to Modern English: Where is kind Hastings?'"
            ]
          },
          "execution_count": 61,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aylkIJnHo3qE",
        "outputId": "5d28625b-254a-472b-b538-4a452503a100"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Modern style score for generated translations:   0.4517341\n",
            "Modern style score for reference translations:   0.82950926\n",
            "Modern style score for original Shakespeare:     0.33652818\n"
          ]
        }
      ],
      "source": [
        "# Prepare three comparable text sets for style scoring:\n",
        "#  - Generated translations from your MT model\n",
        "#  - Human (reference) modern translations\n",
        "#  - Original Shakespeare lines (optionally with the task prefix; see note below)\n",
        "\n",
        "test_original_texts = [task_prefix + pair[0] for pair in test_pairs]\n",
        "test_label_texts    = [pair[1] for pair in test_pairs]\n",
        "\n",
        "# Compute the mean probability that each set is Modern style (class 1).\n",
        "# get_modern_style_score(...) returns the **mean** P(Modern) across the batch.\n",
        "translations_score = get_modern_style_score(part2_test_translations)\n",
        "reference_score    = get_modern_style_score(test_label_texts)\n",
        "shakespeare_score  = get_modern_style_score(test_original_texts) # We have prepended the task prefix \"Translate Shakespeare to Modern English:\" here. This might inflate the modern style score a bit, but it's acceptable for comparison.\n",
        "\n",
        "print(\"Modern style score for generated translations:  \", translations_score)\n",
        "print(\"Modern style score for reference translations:  \", reference_score)\n",
        "print(\"Modern style score for original Shakespeare:    \", shakespeare_score) # Still works not bad\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AIwVzWX0MKyc"
      },
      "source": [
        "**QUESTION:**\n",
        "\n",
        " 2.e What is the modern style classifier score that you got for the part2 model's generated translations? (Copy and paste the decimal value from the get_modern_style_score function above, to 5 significant digits, e.g. a number like 0.36712 or 0.97632. Put the answer in the answers file; it should match the value shown in your output in this notebook.)\n",
        "  - 0.45173\n",
        "\n",
        "**QUESTION:**\n",
        "\n",
        " 2.f What is the modern style classifier score that you got for the human reference translations? (Copy and paste the decimal value from the get_modern_style_score function above, to 5 significant digits, e.g. a number like 0.36712 or 0.97632. Put the answer in the answers file; it should match the value shown in your output in this notebook.)\n",
        "  - 0.82951\n",
        "\n",
        "**QUESTION:**\n",
        "\n",
        " 2.g What is the modern style classifier score that you got for the original Shakespeare text? (Copy and paste the decimal value from the get_modern_style_score function above, to 5 significant digits, e.g. a number like 0.36712 or 0.97632. Put the answer in the answers file; it should match the value shown in your output in this notebook.)\n",
        " - 0.33653\n",
        "\n",
        "**QUESTION:**\n",
        "\n",
        " 2.h What do you notice about differences between these scores, and what does that tell you about what the part2 model is doing? (Chose one of the following options that you agree with most, and put it in the answers file.)\n",
        "\n",
        " - A. The part2 model is generating output that is way more modern, casual, and younger generation speak than the human translations.\n",
        "\n",
        " - B. The part2 model is generating output that looks about as modern as the human translations, even if it doesn't always mean the same thing.\n",
        "\n",
        " - C. The part2 model is generating output that is partly modernized, more modern than the original Shakespeare, but still not as modern as the human references.\n",
        "\n",
        " - D. The part2 model is generating output that is still pretty much the same style as the original Shakespeare text.\n",
        "\n",
        " -> C"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bm81djk1OGKM"
      },
      "source": [
        "### 2.5 Revisit Decoder .Generate() Options\n",
        "\n",
        "Now that we have one more evaluation metrics, let's go back to the decoder .generate() arguments we used before. Are there any arguments you want to change, to try to do better on this latest evaluation metric?\n",
        "\n",
        "Try different options for the part2_generate_kwargs below, and run the two cells afterward with each set of choices to see how the evaluation metrics change.\n",
        "\n",
        "Then answer the questions below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 127,
      "metadata": {
        "id": "rmAE-3c-OGU2"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Fill in the decoder .generate() arguments that you want to use for the part2 model, like num_beams or top_p, etc.\n",
        "\"\"\"\n",
        "part2_generate_kwargs = {\n",
        "    \"do_sample\": True,\n",
        "    \"top_p\": 0.95,\n",
        "    \"temperature\": 1.05,\n",
        "    \"max_new_tokens\": 48,\n",
        "    \"min_new_tokens\": 10,\n",
        "    \"no_repeat_ngram_size\": 2,\n",
        "    \"repetition_penalty\": 1.15,\n",
        "}\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 128,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lLfzUo2nQDot",
        "outputId": "939ac1e8-659e-405b-d0ea-179555cfe249"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BLEU:  {'bleu': 0.19298524299603242, 'precisions': [0.42736639492753625, 0.23161208305587505, 0.14693638610641344, 0.09536861339517887], 'brevity_penalty': 1.0, 'length_ratio': 1.2815787564390917, 'translation_length': 17664, 'reference_length': 13783}\n",
            "BLEURT:  -0.51604944\n"
          ]
        }
      ],
      "source": [
        "# Print out eval metrics for the part2_model on the test set, with the new kwargs\n",
        "\n",
        "part2_test_translations = calculate_eval_metrics(\n",
        "    test_pairs,\n",
        "    part2_model,\n",
        "    part2_tokenizer,\n",
        "    part2_batch_size,\n",
        "    task_prefix,\n",
        "    **part2_generate_kwargs\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "voOOuiKzQDtY",
        "outputId": "5597bb48-dc2a-473f-cd85-b04d22210293"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Modern style score for generated translations:   0.6875998\n"
          ]
        }
      ],
      "source": [
        "# Calculate modern style scores for the part2 translations after using the new kwargs\n",
        "\n",
        "translations_score = get_modern_style_score(part2_test_translations)\n",
        "\n",
        "print(\"Modern style score for generated translations:  \", translations_score)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 130,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xOWu7thSHm7L",
        "outputId": "d0d80482-2ed5-4036-e915-4e69e41861f5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original:     Methinks the ground is even.\n",
            "Reference:    The ground feels flat to me.\n",
            "Translation:  Methinks the ground is actually even.’\n",
            "\n",
            "Original:     Yond island carrions, desperate of their bones, Ill-favoredly become the morning field.\n",
            "Reference:    Those island-bred skeletons, terrified for their bones, are an offensive sight on the morning field.\n",
            "Translation:  Ill-favored island carrions, desperate from their bones, become the morning field.\n",
            "\n",
            "Original:     If thou canst nod, speak too.\n",
            "Reference:    If you can nod, speak too.\n",
            "Translation:  If you can’t nod, speak too.\n",
            "\n",
            "Original:     But you, O you, So perfect and so peerless, are created Of every creature's best!\n",
            "Reference:    So perfect and so peerless, are created Out of every creature's best virtues.\n",
            "Translation:  That means your eyes are so perfect and that they are always rounded, that you are created From every creature's best!\n",
            "\n",
            "Original:     Most royal majesty, I crave no more than hath your highness offered.\n",
            "Reference:    Your highness, I want nothing more than what you’ve already offered.\n",
            "Translation:  Most royals, I crave nothing more than your highness.\n",
            "\n",
            "Original:     Dream on, dream on, of bloody deeds and death.\n",
            "Reference:    I am a villain.\n",
            "Translation:  Dream on bloody acts and death. That is a story of this.\n",
            "\n",
            "Original:     They met so near with their lips that their breaths embraced together.\n",
            "Reference:    They came so close with their lips that their breaths hugged each other.\n",
            "Translation:  They were so close with their lips that their breaths embraced together.\n",
            "\n",
            "Original:     If thou canst nod, speak too.\n",
            "Reference:    If you can nod, speak too.\n",
            "Translation:  If you can’t nod, speak too.\n",
            "\n",
            "Original:     That sir which serves and seeks for gain,  And follows but for form,  Will pack when it begins to rain  And leave thee in the storm.\n",
            "Reference:    The gentleman who serves you only for profit And is only superficially loyal to you Will take off when it starts to rain And leave you alone in the storm.\n",
            "Translation:  That sir that serves and seeks for gain, And follows with exception; Will pack when it begins to rain, and leave you in the storm.\n",
            "\n",
            "Original:     Being unprepared, Our will became the servant to defect, Which else should free have wrought.\n",
            "Reference:    Being unprepared, Our wishes became the servants to what we lacked, Which has worked out very well.\n",
            "Translation:  Being unprepared, We got the servant to defect, That else should free have made.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Print out a sample of the translated outputs with the revised .generate() parameters\n",
        "\n",
        "for i in range(10):\n",
        "    sample_i = random.choice(range(len(part2_test_translations)))\n",
        "    print('Original:    ', test_pairs[sample_i][0])\n",
        "    print('Reference:   ', test_pairs[sample_i][1])\n",
        "    print('Translation: ', part2_test_translations[sample_i])\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sBlCkKB-QUmy"
      },
      "source": [
        "**QUESTION:**\n",
        "\n",
        " 2.i Which decoder strategy seemed to increase the modern style score the most? (Choose one of the following options and put it in the answers file.)\n",
        "\n",
        " - A. Using a stricter option to always choose the highest predicted possibility output (e.g. beam search, or small k or p when using sampling).\n",
        "\n",
        " - B. Using a looser sampling method to allow the model to choose more varied output (e.g. top-k or top-p rather than beam search, especially with higher k or p and/or higher temperature).\n",
        "\n",
        " -> B\n",
        "\n",
        "**QUESTION:**\n",
        "\n",
        " 2.j What happens to the other evaluation metrics when you try to increase the modern style score by varying the decoder strategy discussed in 2.i? (Choose one of the following options and put it in the answers file.)\n",
        "\n",
        " - A. BLEU and BLEURT both seem to be positively correlated with the modern style score, when changing the decoder strategy.\n",
        "\n",
        " - B. BLEU and BLEURT both seem to be negatively correlated with the modern style score, when changing the decoder strategy.\n",
        "\n",
        " - C. BLEU seems to move with the modern style score, but BLEURT seems to go the other direction.\n",
        "\n",
        " - D. BLEURT seems to move with the modern style score, but BLEU seems to go the other direction.\n",
        "\n",
        " -> B\n",
        "\n",
        "**QUESTION:**\n",
        "\n",
        " 2.k Why do you think the relationship in question 2.j is happening? (Choose one of the following options and put it in the answers file.)\n",
        "\n",
        " - A. A stricter decoder strategy makes the model more likely to output the best translation, which is good for BLEU, BLEURT, and modern style objectives.\n",
        "\n",
        " - B. A looser decoder strategy gives the model more freedom to find a good modern style translation, which should also end up saying more of the same things in the same way as the human translation.\n",
        "\n",
        " - C. A stricter decoder strategy makes the model more likely to output a translation that has correct exact words and style, increasing BLEU and modern style scores, but might not mean the same thing as the human translation.\n",
        "\n",
        " - D. A looser decoder strategy gives the model more freedom to choose more modern style words, which the pre-trained model is more familiar with, but that freedom can make the model less likely to end up with the exact same words or meaning as the human translation.\n",
        "\n",
        " - E. A stricter decoder strategy makes the model more likely to choose more of the exact same words as used in the dataset, but not necessarily in the same order, so the meaning and style don't end up being as close to the human translation.\n",
        "\n",
        "  -> D"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sl5_wE3CJ02W"
      },
      "source": [
        "## 3. Adding Supplementary Paraphrase Dataset\n",
        "\n",
        "Can we do anything else to make the model capable of rephrasing the input text more into a different style (i.e. modernizing it more fully away from the Shakespeare), but still keep the same meaning?\n",
        "\n",
        "One related task that could help is a paraphrasing task. The [GLUE Microsoft Research Paraphrase Corpus (MRPC)](https://huggingface.co/datasets/nyu-mll/glue) dataset has pairs of sentences with labels indicating whether the two sentences are equivalent (i.e. they mean the same thing) or not.\n",
        "\n",
        "We could use that data as an additional supporting task for our T5 model, to see if it helps our model get better at accurately rephrasing the input text into a differently worded output.\n",
        "\n",
        "### 3.1 Load and preprocess the supplemental dataset\n",
        "\n",
        "Load the dataset from Huggingface and look at the contents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380,
          "referenced_widgets": [
            "55ba108f924d400fa59f07d6d03e68b3",
            "2648d6564f9d44c4addea6e98705eb8c",
            "144ab30481404d9db0e5c3a4aaa1910d",
            "a2f29a6ecff14894978315b464245bf2",
            "07503b1a1d194793bf552e7e6e6e3926",
            "4670da2ebba74515ac57597493ede653",
            "95135968886f4f9e8ad2b98abc6bfadb",
            "1057897f1c5440d68f06fd8cf2fc9cbc",
            "d06d374bdcfe434abbe86b5b4094f141",
            "76b83a93885745a49fd9d97107ff177f",
            "6cc1c33900f84cd8a315e601256723c6",
            "dfd9ddd0d3d444f0ad86142c60916a6c",
            "54dd7430afe54da09d1002f99fb86c60",
            "13e9ce38297845abb3dbf2f62bc0b951",
            "12f5cd02df1c4b76b063542350f4ecd3",
            "ed409465fdad4c82855a3bcfcc13c52c",
            "c4bc48bcc2d34926871b5b80f25ca0e1",
            "8b80dcca614a45c0a8b83d85313ad5d2",
            "0426e9bb7c954011b0a6ff901a24e317",
            "96f701c889f14b4d88ab8c2dd552810e",
            "865068c13904443ba3cb2111bf940717",
            "7ed17274eb2644b1b0d5eed235d2077a",
            "e7580493dbab416194d63ee4c53d1b44",
            "59ff1ca2dcd047dd95b5ed2343be2cf1",
            "d8135edfdb974af69bb65c1c973a6f7b",
            "6d18383b92114ad8aa1e2d09ae3affe5",
            "dd814769177d4f009b306a76d1b9981f",
            "972ac71589a74af78d01ffda669f88be",
            "617648b3db4440d0b8412ca6bff4c616",
            "fe53cc80986d43199a188a717c088b4e",
            "6478c7e8b9db41359591d55f73285457",
            "64b96108a81d482aae1c5552e21bc61f",
            "2ce25b6dd63845a29055a5cba2d25b80",
            "031313d2d49e43c6be54ff60fbc6c31e",
            "18248d355ef5441883a6654361a687a5",
            "91a5c0d2385242508b19a9de4071748d",
            "139a0fbca2ac45ee928975db4fd467cd",
            "a33e1579a35e4763b4a1e720f992a34f",
            "be100decbb2e441eae036222d3b651fe",
            "384c79dcfae5404d9827e8bc7835841e",
            "8756f20a61604a5fad39103f9a55803d",
            "201eba1bba514fdd8e748c51c9096b06",
            "77f3b94ec3ae48e0aa088b6ea80039fd",
            "00a9f6306af0409293ff51ea5024dd84",
            "78c55e04ad17471db8000c31595f9669",
            "984130880b314d73bbe9450f0f6e12f8",
            "29661b87be2f468c9673c8deccaec805",
            "5d4b1f86de1a462597ec76dc4e3d481d",
            "130d78dba10e4e77a55d531a549c7f93",
            "c8fa6c99eb50461d8d00f5c5299a920e",
            "732f1d95c44c47bd98f770f8bd296b01",
            "cf95bb5d162246329ba435f0b2aa3007",
            "e0cc03ef17cf41dab7db494da922c381",
            "b06b351d37204f388403d2e7fd5db308",
            "cf05f61cd7014cfa9b44a557533ee4bd",
            "ca5275fd71dc484a9b0ef51668df9048",
            "c22dfc61ee6946bdbb1c8a542fae6100",
            "1b3bb8ebb7c046689463293c486572af",
            "ffdb26ac41614352aa6f4ddae640be8c",
            "c6e371fc5090404f851399e2d67f9310",
            "e9165e17ebb041359d9c0e449a80e719",
            "3a0f722361cb4fe79d2299f1d2d21d04",
            "fbffad3492784f30969ef7d8b6656444",
            "d861b7d8591b4abe84a940e07d0595a7",
            "df17ea3803dc4a7191c49ed6e5554eba",
            "649401e161504f41a167b3a60807cb0a",
            "559126d4aadb4c5b947e83240d1cfb57",
            "18f792a9f8e1414990d512af4d041eef",
            "38deeabd111e43a4810d4bfdc38a9fea",
            "f224dbd941a845bca4e0c5e2b5dfb105",
            "f443240acac94f41bd578d60eab7bd3b",
            "a6b1c46ff86647a09f68973c30e4b5c2",
            "29a0165f18b04f5f981baa8fb93a6155",
            "913536d21985497dac6995ef0011c8c8",
            "eeb9a6044f56439c9e0b32bc2c1b9319",
            "472b75c007db427a91936a6239ed3794",
            "a44b18255e6f4a95a1a6abe832569afb"
          ]
        },
        "id": "sqdtBmtpJ1Jf",
        "outputId": "08bbc534-eb52-45cf-a2fe-c9b1f305db85"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`trust_remote_code` is not supported anymore.\n",
            "Please check that the Hugging Face dataset 'SetFit/mrpc' isn't based on a loading script and remove `trust_remote_code`.\n",
            "If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.\n",
            "Repo card metadata block was not found. Setting CardData to empty.\n",
            "Generating train split: 100%|██████████| 3668/3668 [00:00<00:00, 42567.27 examples/s]\n",
            "Generating validation split: 100%|██████████| 408/408 [00:00<00:00, 95655.45 examples/s]\n",
            "Generating test split: 100%|██████████| 1725/1725 [00:00<00:00, 296973.87 examples/s]\n"
          ]
        }
      ],
      "source": [
        "mrpc_data = load_dataset('SetFit/mrpc', trust_remote_code=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RZ2QDvSdJ1Mc",
        "outputId": "8f9d5d67-cbbc-49ff-dea4-b851e043a5e6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['text1', 'text2', 'label', 'idx', 'label_text'],\n",
              "        num_rows: 3668\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['text1', 'text2', 'label', 'idx', 'label_text'],\n",
              "        num_rows: 408\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['text1', 'text2', 'label', 'idx', 'label_text'],\n",
              "        num_rows: 1725\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 108,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "mrpc_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7YfmyHCsJ1PW",
        "outputId": "a43d61f4-ada4-4741-ac85-6ec38b281063"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Value('int64')"
            ]
          },
          "execution_count": 110,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "mrpc_data['train'].features['label']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Value('string')"
            ]
          },
          "execution_count": 112,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "mrpc_data['train'].features['text2']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bjuJt3jdJ9PT"
      },
      "source": [
        "Let's use just the pairs that are labeled as equivalent (correct paraphrases), and split the sentences to use the first as the model's input and second as the model's output. Then we can use that to train our T5 model to better generate rephrased statements in modern English with the same meaning as the input but in different words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81,
          "referenced_widgets": [
            "367dbb0267574d11a0f971011fc4d083",
            "fcc8b2cb324b4cf5a381420aba73d6da",
            "82f4b25bcf0f4f619135859d2ee0aacf",
            "6bbdcd72c7f64ab6a10bce3abb85a7e5",
            "11b6610629a54223a9042b68a127f475",
            "595fd60f0b6c463993dbf2aeefc7ec62",
            "35eb23a35fea465e84457faeb361c154",
            "706b00e34dc844c88e5b0befc9226004",
            "0cdfa2fb76d2404984660e6c44688dc5",
            "959ae872e60547708fc9c5374a5e10c5",
            "fa4e16e3699f4a378d05d9b61b674ca4",
            "a040bb2cf8b946109a835d8874d35341",
            "43c4dfdfa07040bb978f562fd11e7b97",
            "517d9bfbad0345e2ae66d117eb2fcc8d",
            "497fb4653b4f4cb4ad033a1823175ea6",
            "b7bfedd04a6b46ef81a453844777f677",
            "8abd175e4b97478d86522569c86cc27d",
            "d49c038414ab482ebc3d0cd76e98d545",
            "1efe042e33ac4bf1ad25a1d84e3f328d",
            "3a5abd80e0fc496dbcf4adcd79c1e0db",
            "13992a908c53471594a51504e65e882f",
            "deb3d6c95a6242d698d35a829a75a455"
          ]
        },
        "id": "L1F2fPeYJ1SE",
        "outputId": "17e36085-70e4-4aa1-a1d4-c89269030610"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Filter: 100%|██████████| 3668/3668 [00:00<00:00, 65783.72 examples/s]\n",
            "Filter: 100%|██████████| 408/408 [00:00<00:00, 55323.81 examples/s]\n"
          ]
        }
      ],
      "source": [
        "# Keep only MRPC pairs labeled as *equivalent* (label == 1 → true paraphrases).\n",
        "# We'll later map `text1` → source and `text2` → target for seq2seq training.\n",
        "\n",
        "mrpc_equiv_train = mrpc_data[\"train\"].filter(lambda example: example[\"label\"] == 1)\n",
        "mrpc_equiv_valid = mrpc_data[\"validation\"].filter(lambda example: example[\"label\"] == 1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uVBfoSwKN4L_"
      },
      "source": [
        "Fill in the code below to encode sentence1 e.g. \"text1\" as the model's input and sentence2 e.g. \"text2\" as the model's output.\n",
        "\n",
        "We will also add a different prefix for this supporting task, so make sure to add the prefix to the inputs in the function below. (You can use the preprocess_translation_batch function above as an example.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {
        "id": "BtmZkZkudA80"
      },
      "outputs": [],
      "source": [
        "def preprocess_mrpc_for_paraphrase_generation(mrpc_ds, tokenizer, prefix):\n",
        "    \"\"\"\n",
        "    Prepare MRPC paraphrase pairs for seq2seq generation:\n",
        "      - Inputs:   text1 (with a task prefix prepended)\n",
        "      - Outputs:  text2 (target paraphrase)\n",
        "    Returns tensors for Trainer: {'input_ids', 'labels'}.\n",
        "    \"\"\"\n",
        "    # Optionally prepend a task instruction to each source sentence\n",
        "    if prefix:\n",
        "        mrpc_ds[\"text1\"] = [prefix + t for t in mrpc_ds[\"text1\"]]\n",
        "\n",
        "    # Encode inputs (encoder side)\n",
        "    input_encoded = tokenizer.batch_encode_plus(\n",
        "        mrpc_ds[\"text1\"],\n",
        "        max_length=MAX_SEQUENCE_LENGTH,\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "    # Encode targets (decoder side labels)\n",
        "    output_encoded = tokenizer.batch_encode_plus(\n",
        "        mrpc_ds[\"text2\"],\n",
        "        max_length=MAX_SEQUENCE_LENGTH,\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "    return {\n",
        "        \"input_ids\": input_encoded[\"input_ids\"],\n",
        "        \"labels\": output_encoded[\"input_ids\"],\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_UksD44fPkn6"
      },
      "source": [
        "Now map the preprocessing function to the MRPC train and validation datasets. Use the part2_tokenizer to preprocess the data, since we're using the same T5 pre-trained model checkpoint as in part 2. For the preprocessing function's \"prefix\" argument, use the paraphrase_prefix provided below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81,
          "referenced_widgets": [
            "05755d9433f745149766396b59ee1d58",
            "aecf23acfec949f2a8cadda0bcf9a57f",
            "5ed1c0c05a1c4c75a279bf43b2c7e7ac",
            "932b2978bd48409e9a666456e30460f9",
            "aa5c4a244ad6492f98f487fde8577902",
            "5489a78c49074fbba3fdef4008481ff5",
            "98c3ec7bc91d4936b1b930a55e49f5ff",
            "5a6c093042ac4ffa9197dabce36786cc",
            "0ee11ba2e6f24c35b3945ffb80ad4e69",
            "1c5c9f7ccd0a46ba88ea095364afa2ff",
            "a88b021a846f4f899cba22b4a538786f",
            "4a8de07f8a874b279868ad583e4910cb",
            "8eeeb016f98542bd9759dd038ee1e570",
            "20983f6fdd324a92845d09ff1852699e",
            "4b655d8db2e64622aba2ea1c8c6e25cf",
            "f7c17306f4b8479ab0e3820ee2ff4ddf",
            "a3f954625298402e9bf3c37e70062746",
            "440ba63bfad147778a8f4e409a53cbd7",
            "37a86108a3b64bc5af2312a7799b7ef7",
            "c3c469f5dd99444d9d55a82c3f05b034",
            "f2072851c1a84a0c983bc55000bc8ba9",
            "6dce62c63072421f89dd2364d48c1df9"
          ]
        },
        "id": "CgB2B-P6dBAV",
        "outputId": "2aed2a0d-3fae-403e-c5f9-faec952d4d6c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Map: 100%|██████████| 2474/2474 [00:00<00:00, 6061.51 examples/s]\n",
            "Map: 100%|██████████| 279/279 [00:00<00:00, 4707.51 examples/s]\n"
          ]
        }
      ],
      "source": [
        "paraphrase_prefix = 'Paraphrase in modern English: '\n",
        "\n",
        "### YOUR CODE HERE\n",
        "\n",
        "# Map MRPC (label==1) splits into seq2seq features using the pretrained tokenizer\n",
        "mrpc_paraphrase_train = mrpc_equiv_train.map(\n",
        "    preprocess_mrpc_for_paraphrase_generation,\n",
        "    batched=True,\n",
        "    fn_kwargs={\"tokenizer\": part2_tokenizer, \"prefix\": paraphrase_prefix}\n",
        ")\n",
        "\n",
        "mrpc_paraphrase_valid = mrpc_equiv_valid.map(\n",
        "    preprocess_mrpc_for_paraphrase_generation,\n",
        "    batched=True,\n",
        "    fn_kwargs={\"tokenizer\": part2_tokenizer, \"prefix\": paraphrase_prefix}\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "### END YOUR CODE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ba4cLyiyQMMJ"
      },
      "source": [
        "### 3.2 Train T5 on Paraphrasing Task\n",
        "\n",
        "Load a fresh copy of the pre-trained T5 model (using the same pre-trained model checkpoint as part2), so that we can train it first on the paraphrase task, and last on the main task data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 131,
      "metadata": {
        "id": "_Tgg2lBWQWDV"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Load a new copy of the same pre-trained model (we'll use the same in tokenizer as part2)\n",
        "\"\"\"\n",
        "\n",
        "t5_pretrained_checkpoint_name = 'google-t5/t5-small'\n",
        "\n",
        "### YOUR CODE HERE\n",
        "\n",
        "part3_model = T5ForConditionalGeneration.from_pretrained(t5_pretrained_checkpoint_name)\n",
        "\n",
        "# T5 convention: decoder starts with the pad token (usually set already, but make it explicit if you like)\n",
        "part3_model.config.decoder_start_token_id = part2_tokenizer.pad_token_id\n",
        "\n",
        "\n",
        "### END YOUR CODE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xs4Uy482Q5V-"
      },
      "source": [
        "Now create the training args and trainer for the paraphrase task, and train the model. Use the `create_seq2seq_training_args` and `create_seq2seq_trainer` functions like before.\n",
        "\n",
        "You'll be using the part3_model you just loaded, and the MRPC data you preprocessed. Use the batch_size and num_epochs provided for the paraphrase task below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 132,
      "metadata": {
        "id": "FraCXsP6Rcr-"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Create the training args and trainer for the paraphrase task.\n",
        "\"\"\"\n",
        "\n",
        "paraphrase_batch_size = 32\n",
        "paraphrase_num_epochs = 4\n",
        "\n",
        "### YOUR CODE HERE\n",
        "\n",
        "# Build training arguments (same fixed fields as before)\n",
        "paraphrase_training_args = create_seq2seq_training_args(\n",
        "    paraphrase_batch_size,\n",
        "    paraphrase_num_epochs\n",
        ")\n",
        "\n",
        "# Wire up model, args, and MRPC paraphrase datasets\n",
        "paraphrase_trainer = create_seq2seq_trainer(\n",
        "    part3_model,\n",
        "    paraphrase_training_args,\n",
        "    mrpc_paraphrase_train,\n",
        "    mrpc_paraphrase_valid\n",
        ")\n",
        "\n",
        "\n",
        "### END YOUR CODE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 133,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 236
        },
        "id": "l_Adc3AnL_1X",
        "outputId": "744d683a-1344-40b7-e30d-6830bee1535d"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='312' max='312' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [312/312 00:28, Epoch 4/4]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.359217</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.255481</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.228767</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.223685</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=312, training_loss=1.6392105298164563, metrics={'train_runtime': 28.8769, 'train_samples_per_second': 342.696, 'train_steps_per_second': 10.804, 'total_flos': 104636130263040.0, 'train_loss': 1.6392105298164563, 'epoch': 4.0})"
            ]
          },
          "execution_count": 133,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "paraphrase_trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GkIvMxIeU0jz"
      },
      "source": [
        "### 3.3 Fine-Tune Paraphrase-Trained Model on Main Task\n",
        "\n",
        "Now create the training args and trainer for the main task. Use the `create_seq2seq_training_args` and `create_seq2seq_trainer` functions one more time.\n",
        "\n",
        "You'll be using the same model that you just trained on the paraphrase task (part3_model). Use the batch size and num epochs provided below.\n",
        "\n",
        "For training data, use the same data as part2: `train_ds_part2` and `val_ds_part2`. We're using the same pre-trained model checkpoint, i.e. the same tokenizer, and the same task prefix, so the data has already been preprocessed correctly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 134,
      "metadata": {
        "id": "MScmxiy9ZaIO"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Create the training args and trainer for the main task using the part3_model.\n",
        "\"\"\"\n",
        "\n",
        "part3_batch_size = 32\n",
        "part3_num_epochs = 4\n",
        "\n",
        "### YOUR CODE HERE\n",
        "\n",
        "# Training args for fine-tuning on the Shakespeare→Modern task (same helper)\n",
        "part3_training_args = create_seq2seq_training_args(\n",
        "    part3_batch_size,\n",
        "    part3_num_epochs\n",
        ")\n",
        "\n",
        "# Trainer that continues training the *same* model (part3_model) on the main task data\n",
        "part3_trainer = create_seq2seq_trainer(\n",
        "    part3_model,\n",
        "    part3_training_args,\n",
        "    train_ds_part2,\n",
        "    val_ds_part2\n",
        ")\n",
        "\n",
        "### END YOUR CODE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 135,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 236
        },
        "id": "FV1aOzpXcI1v",
        "outputId": "c40d230d-89c7-4230-c777-167cfba6925a"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2100' max='2100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2100/2100 03:13, Epoch 4/4]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.801600</td>\n",
              "      <td>0.707475</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.737800</td>\n",
              "      <td>0.691277</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.723900</td>\n",
              "      <td>0.685622</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.710800</td>\n",
              "      <td>0.683007</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=2100, training_loss=0.742597910563151, metrics={'train_runtime': 193.3912, 'train_samples_per_second': 347.441, 'train_steps_per_second': 10.859, 'total_flos': 710459869102080.0, 'train_loss': 0.742597910563151, 'epoch': 4.0})"
            ]
          },
          "execution_count": 135,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "part3_trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 136,
      "metadata": {
        "id": "di4zMzWtx95k"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Before moving on, save a checkpoint of the model you just trained in your Drive,\n",
        "So that you can pick up where you left off later if needed\n",
        "\"\"\"\n",
        "\n",
        "# Modify this path to the location in your Drive where you want to save the part3 model\n",
        "# part3_model_checkpoint_filepath = 'drive/MyDrive/ISchool/MIDS/266/model_checkpoints/part3_model'\n",
        "part3_model_checkpoint_filepath = 'model_checkpoints/part3_model'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 137,
      "metadata": {
        "id": "hTi-rOy9x-DS"
      },
      "outputs": [],
      "source": [
        "# Run this line only after you've trained the part3 model on both tasks\n",
        "part3_model.save_pretrained(part3_model_checkpoint_filepath, from_pt=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 138,
      "metadata": {
        "id": "VgzXnWk1x-F0"
      },
      "outputs": [],
      "source": [
        "# Run this line only if you need to reload the part3 model you trained earlier\n",
        "part3_model = T5ForConditionalGeneration.from_pretrained(part3_model_checkpoint_filepath)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RJqR5W1hZyUs"
      },
      "source": [
        "### 3.4 Paraphrase-Trained Model Evaluation\n",
        "\n",
        "Use the functions defined above to translate the test set and calculate the same set of evaluation metrics as used on the part2 model.\n",
        "\n",
        "Use the same decoder .generate() arguments as part2 (`part2_generate_kwargs`), so that we can compare the part2 and part3 models as closely as possible.\n",
        "\n",
        "Run the next three cells, then answer the questions below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 139,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1pXcdJGAL_58",
        "outputId": "cefedced-eaf2-418d-d0a3-4d731c1af544"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BLEU:  {'bleu': 0.18760278053208665, 'precisions': [0.42463608420808446, 0.22679836004472606, 0.14151006486992576, 0.0908893395133256], 'brevity_penalty': 1.0, 'length_ratio': 1.2510338823187985, 'translation_length': 17243, 'reference_length': 13783}\n",
            "BLEURT:  -0.51736\n"
          ]
        }
      ],
      "source": [
        "# Print out eval metrics for the part3_model on the test set, with the new kwargs\n",
        "\n",
        "part3_test_translations = calculate_eval_metrics(\n",
        "    test_pairs,\n",
        "    part3_model,\n",
        "    part2_tokenizer,\n",
        "    part3_batch_size,\n",
        "    task_prefix,\n",
        "    **part2_generate_kwargs\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 140,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dAw04aBVMxBY",
        "outputId": "ce07fd3d-2adb-4a2c-9003-cb6b281f998f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Modern style score for generated translations:   0.6705979\n"
          ]
        }
      ],
      "source": [
        "# Calculate modern style scores for the part3 translations after using the new kwargs\n",
        "\n",
        "translations_score = get_modern_style_score(part3_test_translations)\n",
        "\n",
        "print(\"Modern style score for generated translations:  \", translations_score)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 141,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iwetEG0oMyNh",
        "outputId": "b27aff2d-7e04-45ac-b85d-00832ac10ee7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original:     There the grown serpent lies; the worm that's fled Hath nature that in time will venom breed, No teeth for the present.\n",
            "Reference:    There the grown serpent lies; the worm that has fled Has a nature that in time will breed venom, But he has no fangs now.\n",
            "Translation:  When the grown serpent lie, the worm that has escaped the Hath nature that in time will venom breed, No teeth for the present.\n",
            "\n",
            "Original:     Signior, it is the Moor.\n",
            "Reference:    Signior, it is the Moor.\n",
            "Translation:  Signior, it is the Moor.\n",
            "\n",
            "Original:     She died, my lord, but whiles her slander lived.\n",
            "Reference:    She was only dead, my lord, as long as her slander lived.\n",
            "Translation:  She died, my lord, but as she was living, she lived.\n",
            "\n",
            "Original:     Thou worms' meat in respect of a good piece of flesh, indeed.\n",
            "Reference:    You are about as much of a thinker as worm’s meat is a nice steak.\n",
            "Translation:  You're pretty nice, a good piece of flesh.\n",
            "\n",
            "Original:     A very fine one.\n",
            "Reference:    He’s a stylish man.\n",
            "Translation:  A very fine one.) And I can tell you why, when you write a German translation of Shakespeare to French, I think Shakespeare is incredibly good in directing you to his world.\n",
            "\n",
            "Original:     Retire!\n",
            "Reference:    Retreat!\n",
            "Translation:  Come on! I mean, let’s wait!\n",
            "\n",
            "Original:     Therefore my son i'th’ ooze is bedded, and I'll seek him deeper than e'er plummet sounded, And with him there lie mudded.\n",
            "Reference:    Therefore my son is embedded in the ooze; and I'll seek him deeper than a piece of lead ever measured How deep the water is, and I will lie there with him in the mud.\n",
            "Translation:  So your son’s ooze is now bedded and I'll seek him deeper than e'er plummet sounded, and with him lie mudded.\n",
            "\n",
            "Original:     I can tell your Majesty, the duke is a prave man.\n",
            "Reference:    I can tell your Majesty, the duke is a brave man.\n",
            "Translation:  I can tell your Majesty, the duke is a prave man.\n",
            "\n",
            "Original:     Besides, it should appear, that if he had The present money to discharge the Jew, He would not take it.\n",
            "Reference:    Besides, it seems that, even if he had The money right now to pay the Jew, He wouldn’t take it.\n",
            "Translation:  Besides, it should seem that, if he had The current money to discharge the Jew, He wouldn’t take it.\n",
            "\n",
            "Original:     Look where he goes even now out at the portal!\n",
            "Reference:    Look, where he goes, even now out the door!\n",
            "Translation:  Look what he goes till now at the portal!\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Print out a sample of the translated outputs to look at as well\n",
        "\n",
        "for i in range(10):\n",
        "    sample_i = random.choice(range(len(part3_test_translations)))\n",
        "    print('Original:    ', test_pairs[sample_i][0])\n",
        "    print('Reference:   ', test_pairs[sample_i][1])\n",
        "    print('Translation: ', part3_test_translations[sample_i])\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qb8p115NaORe"
      },
      "source": [
        "**QUESTION:**\n",
        "\n",
        " 3.a What is the overall BLEU score that you achieved on the test set for the part3 model? (Copy and paste the decimal value for the overall BLEU score, to 5 significant digits, e.g. a number like 0.03671 or 0.09763. Put the answer in the answers file; it should match the value shown in your output in this notebook.)  \n",
        "  - 0.18760\n",
        "\n",
        "**QUESTION:**\n",
        "\n",
        " 3.b What is the mean BLEURT score that you achieved on the test set for the part3 model? (Copy and paste the decimal value for the mean BLEURT score, to 5 significant digits, e.g. a number like -1.12345 or -0.54321. Put the answer in the answers file; it should match the value shown in your output in this notebook.)\n",
        "  - -0.51736\n",
        "\n",
        "**QUESTION:**\n",
        "\n",
        " 3.c What is the modern style classifier score that you got for the part3 model's generated translations? (Copy and paste the decimal value from the get_modern_style_score function above, to 5 significant digits, e.g. a number like 0.36712 or 0.97632. Put the answer in the answers file; it should match the value shown in your output in this notebook.)\n",
        "  - 0.67060\n",
        "\n",
        "**QUESTION:**\n",
        "\n",
        " 3.d How do the part3 model's evaluation scores and output compare to the part2 model? Write a short answer about what you observe in the markdown cell below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7AulNHRSbIc5"
      },
      "source": [
        "*** YOUR ANSWER TO QUESTION 3.d HERE IN THIS TEXT CELL***\n",
        "\n",
        "Compared to the part 2 model, the part 3 model (which was pre-trained on the MRPC paraphrase task) performs slightly worse across all three metrics.\n",
        "\n",
        "BLEU Score: Part 3 (0.18760) is slightly lower than Part 2 (0.19299).\n",
        "\n",
        "BLEURT Score: Part 3 (-0.51736) is marginally lower than Part 2 (-0.51605), indicating a very small decrease in semantic similarity to the reference.\n",
        "\n",
        "Modern Style Score: Part 3 (0.67060) is also slightly lower than Part 2 (0.68760), suggesting its output is classified as slightly less \"modern.\"\n",
        "\n",
        "Observing the sample outputs, the additional pre-training on paraphrasing seems to have made the model more prone to changing the core meaning of the original text, rather than just the style. For example, it translated \"Retire!\" as \"Come on! I mean, let’s wait!\", which is a significant semantic shift. It appears that while the paraphrasing pre-training encouraged rephrasing, it didn't perfectly align with the specific goal of Shakespeare-to-Modern style transfer, leading to a slight degradation in performance on the target task.\n",
        "\n",
        "*** END YOUR ANSWER ***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "2025-fall-main (3.10.2)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.2"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "state": {},
        "version_major": 2,
        "version_minor": 0
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
